0
00:00:00,000 --> 00:00:06,000
So, how can the memory system arrange for the right data to be in the right place at the right time?

1
00:00:06,000 --> 00:00:11,000
Our goal is to have the frequently-used data in some fast SRAM.

2
00:00:11,000 --> 00:00:17,000
That means the memory system will have to be able to predict which memory locations will be accessed.

3
00:00:17,000 --> 00:00:24,000
And to keep the overhead of moving data into and out of SRAM manageable, we'd like to amortize the cost of the move over many accesses.

4
00:00:24,000 --> 00:00:30,000
In other words we want any block of data we move into SRAM to be accessed many times.

5
00:00:30,000 --> 00:00:38,000
When not in SRAM, data would live in the larger, slower DRAM that serves as main memory.

6
00:00:38,000 --> 00:00:46,000
If the system is working as planned, DRAM accesses would happen infrequently, e.g., only when it's time to bring another block of data into SRAM.

7
00:00:46,000 --> 00:00:56,000
If we look at how programs access memory, it turns out we *can* make accurate predictions about which memory locations will be accessed.

8
00:00:56,000 --> 00:01:11,000
The guiding principle is "locality of reference" which tells us that if there's an access to address X at time t, it's very probable that the program will access a nearby location in the near future.

9
00:01:11,000 --> 00:01:18,000
To understand why programs exhibit locality of reference, let's look at how a running program accesses memory.

10
00:01:18,000 --> 00:01:21,000
Instruction fetches are quite predictable.

11
00:01:21,000 --> 00:01:30,000
Execution usually proceeds sequentially since most of the time the next instruction is fetched from the location after that of the current instruction.

12
00:01:30,000 --> 00:01:37,000
Code that loops will repeatedly fetch the same sequence of instructions, as shown here on the left of the time line.

13
00:01:37,000 --> 00:01:47,000
There will of course be branches and subroutine calls that interrupt sequential execution, but then we're back to fetching instructions from consecutive locations.

14
00:01:47,000 --> 00:02:01,000
Some programming constructs, e.g., method dispatch in object-oriented languages, can produce scattered references to very short code sequences (as shown on the right of the time line) but order is quickly restored.

15
00:02:01,000 --> 00:02:04,000
This agrees with our intuition about program execution.

16
00:02:04,000 --> 00:02:12,000
For example, once we execute the first instruction of a procedure, we'll almost certainly execute the remaining instructions in the procedure.

17
00:02:12,000 --> 00:02:24,000
So if we arranged for all the code of a procedure to moved to SRAM when the procedure's first instruction was fetched, we'd expect that many subsequent instruction fetches could be satisfied by the SRAM.

18
00:02:24,000 --> 00:02:35,000
And although fetching the first word of a block from DRAM has relatively long latency, the DRAM's fast column accesses will quickly stream the remaining words from sequential addresses.

19
00:02:35,000 --> 00:02:42,000
This will amortize the cost of the initial access over the whole sequence of transfers.

20
00:02:42,000 --> 00:02:49,000
The story is similar for accesses by a procedure to its arguments and local variables in the current stack frame.

21
00:02:49,000 --> 00:02:57,000
Again there will be many accesses to a small region of memory during the span of time we're executing the procedure's code.

22
00:02:57,000 --> 00:03:04,000
Data accesses generated by LD and ST instructions also exhibit locality.

23
00:03:04,000 --> 00:03:08,000
The program may be accessing the components of an object or struct.

24
00:03:08,000 --> 00:03:11,000
Or it may be stepping through the elements of an array.

25
00:03:11,000 --> 00:03:19,000
Sometimes information is moved from one array or data object to another, as shown by the data accesses on the right of the timeline.

26
00:03:19,000 --> 00:03:28,000
Using simulations we can estimate the number of different locations that will be accessed over a particular span of time.

27
00:03:28,000 --> 00:03:33,000
What we discover when we do this is the notion of a "working set" of locations that are accessed repeatedly.

28
00:03:33,000 --> 00:03:42,000
If we plot the size of the working set as a function of the size of the time interval, we see that the size of the working set levels off.

29
00:03:42,000 --> 00:03:54,000
In other words once the time interval reaches a certain size the number of locations accessed is approximately the same independent of when in time the interval occurs.

30
00:03:54,000 --> 00:04:09,000
As we see in our plot to the left, the actual addresses accessed will change, but the number of *different* addresses during the time interval will, on the average, remain relatively constant and, surprisingly, not all that large!

31
00:04:09,000 --> 00:04:19,000
This means that if we can arrange for our SRAM to be large enough to hold the working set of the program, most accesses will be able to be satisfied by the SRAM.

32
00:04:19,000 --> 00:04:30,000
We'll occasionally have to move new data into the SRAM and old data back to DRAM, but the DRAM access will occur less frequently than SRAM accesses.

33
00:04:30,000 --> 00:04:45,000
We'll work out the mathematics in a slide or two, but you can see that thanks to locality of reference we're on track to build a memory out of a combination of SRAM and DRAM that performs like an SRAM but has the capacity of the DRAM.

34
00:04:45,000 --> 00:04:50,000
The SRAM component of our hierarchical memory system is called a "cache".

35
00:04:50,000 --> 00:04:55,000
It provides low-latency access to recently-accessed blocks of data.

36
00:04:55,000 --> 00:05:02,000
If the requested data is in the cache, we have a "cache hit" and the data is supplied by the SRAM.

37
00:05:02,000 --> 00:05:12,000
If the requested data is not in the cache, we have a "cache miss" and a block of data containing the requested location will have to be moved from DRAM into the cache.

38
00:05:12,000 --> 00:05:20,000
The locality principle tells us that we should expect cache hits to occur much more frequently than cache misses.

39
00:05:20,000 --> 00:05:25,000
Modern computer systems often use multiple levels of SRAM caches.

40
00:05:25,000 --> 00:05:35,000
The levels closest to the CPU are smaller but very fast, while the levels further away from the CPU are larger and hence slower.

41
00:05:35,000 --> 00:05:44,000
A miss at one level of the cache generates an access to the next level, and so on until a DRAM access is needed to satisfy the initial request.

42
00:05:44,000 --> 00:05:50,000
Caching is used in many applications to speed up accesses to frequently-accessed data.

43
00:05:50,000 --> 00:06:04,000
For example, your browser maintains a cache of frequently-accessed web pages and uses its local copy of the web page if it determines the data is still valid, avoiding the delay of transferring the data over the Internet.

44
00:06:04,000 --> 00:06:08,000
Here's an example memory hierarchy that might be found on a modern computer.

45
00:06:08,000 --> 00:06:17,000
There are three levels on-chip SRAM caches, followed by DRAM main memory and a flash-memory cache for the hard disk drive.

46
00:06:17,000 --> 00:06:26,000
The compiler is responsible for deciding which data values are kept in the CPU registers and which values require the use of LDs and STs.

47
00:06:26,000 --> 00:06:32,000
The 3-level cache and accesses to DRAM are managed by circuity in the memory system.

48
00:06:32,000 --> 00:06:44,000
After that the access times are long enough (many hundreds of instruction times) that the job of managing the movement of data between the lower levels of the hierarchy is turned over to software.

49
00:06:44,000 --> 00:06:48,000
Today we're discussing how the on-chip caches work.

50
00:06:48,000 --> 00:06:56,000
In Part 3 of the course, we'll discuss how the software manages main memory and non-volatile storage devices.

51
00:06:56,000 --> 00:07:07,000
Whether managed by hardware or software, each layer of the memory system is designed to provide lower-latency access to frequently-accessed locations in the next, slower layer.

52
00:07:07,000 --> 00:07:12,000
But, as we'll see, the implementation strategies will be quite different in the slower layers of the hierarchy.

