{"C03S01B05-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c3/c3s1/5?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c3s1v5", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:06", "is_worked_example": false, "text": "Now let's turn our attention to control hazards, illustrated by the code fragment shown here."}, {"start": "00:00:06", "is_lecture": true, "end": "00:00:09", "is_worked_example": false, "text": "Which instruction should be executed after the BNE?"}, {"start": "00:00:09", "is_lecture": true, "end": "00:00:13", "is_worked_example": false, "text": "If the value in R3 is non-zero, ADDC should be executed."}, {"start": "00:00:13", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "If the value in R3 is zero, the next instruction should be SUB."}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "If the current instruction is an explicit transfer of control (i.e., JMPs or branches), the choice of the next instruction depends on the execution of the current instruction."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:34", "is_worked_example": false, "text": "What are the implications of this dependency on our execution pipeline?"}, {"start": "00:00:34", "is_lecture": true, "end": "00:00:40", "is_worked_example": false, "text": "How does the unpipelined implementation determine the next instruction?"}, {"start": "00:00:40", "is_lecture": true, "end": "00:00:47", "is_worked_example": false, "text": "For branches (BEQ or BNE), the value to be loaded into the program counter depends on"}, {"start": "00:00:47", "is_lecture": true, "end": "00:00:52", "is_worked_example": false, "text": "(1) the opcode, i.e., whether the instruction is a BEQ or a BNE,"}, {"start": "00:00:52", "is_lecture": true, "end": "00:00:58", "is_worked_example": false, "text": "(2) the current value of the program counter since that's used in the offset calculation, and"}, {"start": "00:00:58", "is_lecture": true, "end": "00:01:06", "is_worked_example": false, "text": "(3) the value stored in the register specified by the RA field of the instruction since that's the value tested by the branch."}, {"start": "00:01:06", "is_lecture": true, "end": "00:01:15", "is_worked_example": false, "text": "For JMP instructions, the next value of the program counter depends once again on the opcode field and the value of the RA register."}, {"start": "00:01:15", "is_lecture": true, "end": "00:01:24", "is_worked_example": false, "text": "For all other instructions, the next PC value depends only the opcode of the instruction and the value PC+4."}, {"start": "00:01:24", "is_lecture": true, "end": "00:01:28", "is_worked_example": false, "text": "Exceptions also change the program counter."}, {"start": "00:01:28", "is_lecture": true, "end": "00:01:30", "is_worked_example": false, "text": "We'll deal with them later in the lecture."}, {"start": "00:01:30", "is_lecture": true, "end": "00:01:43", "is_worked_example": false, "text": "The control hazard is triggered by JMP and branches since their execution depends on the value in the RA register, i.e., they need to read from the register file, which happens in the RF pipeline stage."}, {"start": "00:01:43", "is_lecture": true, "end": "00:01:51", "is_worked_example": false, "text": "Our bypass mechanisms ensure that we'll use the correct value for the RA register even if it's not yet written into the register file."}, {"start": "00:01:51", "is_lecture": true, "end": "00:02:04", "is_worked_example": false, "text": "What we're concerned about here is that the address of the instruction following the JMP or branch will be loaded into program counter at the end of the cycle when the JMP or branch is in the RF stage."}, {"start": "00:02:04", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "But what should the IF stage be doing while all this is going on in RF stage?"}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:22", "is_worked_example": false, "text": "The answer is that in the case of JMPs and taken branches, we don't know what the IF stage should be doing until those instructions are able to access the value of the RA register in the RF stage."}, {"start": "00:02:22", "is_lecture": true, "end": "00:02:29", "is_worked_example": false, "text": "One solution is to stall the IF stage until the RF stage can compute the necessary result."}, {"start": "00:02:29", "is_lecture": true, "end": "00:02:33", "is_worked_example": false, "text": "This was the first of our general strategies for dealing with hazards."}, {"start": "00:02:33", "is_lecture": true, "end": "00:02:35", "is_worked_example": false, "text": "How would this work?"}, {"start": "00:02:35", "is_lecture": true, "end": "00:02:43", "is_worked_example": false, "text": "If the opcode in the RF stage is JMP, BEQ, or BNE, stall the IF stage for one cycle."}, {"start": "00:02:43", "is_lecture": true, "end": "00:02:56", "is_worked_example": false, "text": "In the example code shown here, assume that the value in R3 is non-zero when the BNE is executed, i.e., that the instruction following BNE should be the ADDC at the top of the loop."}, {"start": "00:02:56", "is_lecture": true, "end": "00:03:05", "is_worked_example": false, "text": "The pipeline diagram shows the effect we're trying to achieve: a NOP is inserted into the pipeline in cycles 4 and 8."}, {"start": "00:03:05", "is_lecture": true, "end": "00:03:12", "is_worked_example": false, "text": "Then execution resumes in the next cycle after the RF stage determines what instruction comes next."}, {"start": "00:03:12", "is_lecture": true, "end": "00:03:26", "is_worked_example": false, "text": "Note, by the way, that we're relying on our bypass logic to deliver the correct value for R3 from the MEM stage since the ADDC instruction that wrote into R3 is still in the pipeline, i.e., we have a data hazard to deal with too!"}, {"start": "00:03:26", "is_lecture": true, "end": "00:03:37", "is_worked_example": false, "text": "Looking at, say, the WB stage in the pipeline diagram, we see it takes 4 cycles to execute one iteration of our 3-instruction loop."}, {"start": "00:03:37", "is_lecture": true, "end": "00:03:43", "is_worked_example": false, "text": "So the effective CPI is 4/3, an increase of 33%."}, {"start": "00:03:43", "is_lecture": true, "end": "00:03:51", "is_worked_example": false, "text": "Using stall to deal with control hazards has had an impact on the instruction throughput of our execution pipeline."}, {"start": "00:03:51", "is_lecture": true, "end": "00:03:57", "is_worked_example": false, "text": "We've already seen the logic needed to introduce NOPs into the pipeline."}, {"start": "00:03:57", "is_lecture": true, "end": "00:04:05", "is_worked_example": false, "text": "In this case, we add a mux to the instruction path in the IF stage, controlled by the IRSrc_IF signal."}, {"start": "00:04:05", "is_lecture": true, "end": "00:04:11", "is_worked_example": false, "text": "We use the superscript on the control signals to indicate which pipeline stage holds the logic they control."}, {"start": "00:04:11", "is_lecture": true, "end": "00:04:24", "is_worked_example": false, "text": "If the opcode in the RF stage is JMP, BEQ, or BNE we set IRSrc_IF to 1, which causes a NOP to replace the instruction that was being read from main memory."}, {"start": "00:04:24", "is_lecture": true, "end": "00:04:35", "is_worked_example": false, "text": "And, of course, we'll be setting the PCSEL control signals to select the correct next PC value, so the IF stage will fetch the desired follow-on instruction in the next cycle."}, {"start": "00:04:35", "is_lecture": true, "end": "00:04:41", "is_worked_example": false, "text": "If we replace an instruction with NOP, we say we \"annulled\" the instruction."}, {"start": "00:04:41", "is_lecture": true, "end": "00:04:50", "is_worked_example": false, "text": "The branch instructions in the Beta ISA make their branch decision in the RF stage since they only need the value in register RA."}, {"start": "00:04:50", "is_lecture": true, "end": "00:04:55", "is_worked_example": false, "text": "But suppose the ISA had a branch where the branch decision was made in ALU stage."}, {"start": "00:04:55", "is_lecture": true, "end": "00:05:07", "is_worked_example": false, "text": "When the branch decision is made in the ALU stage, we need to introduce two NOPs into the pipeline, replacing the now unwanted instructions in the RF and IF stages."}, {"start": "00:05:07", "is_lecture": true, "end": "00:05:11", "is_worked_example": false, "text": "This would increase the effective CPI even further."}, {"start": "00:05:11", "is_lecture": true, "end": "00:05:18", "is_worked_example": false, "text": "But the tradeoff is that the more complex branches may reduce the number of instructions in the program."}, {"start": "00:05:18", "is_lecture": true, "end": "00:05:25", "is_worked_example": false, "text": "If we annul instructions in all the earlier pipeline stages, this is called \"flushing the pipeline\"."}, {"start": "00:05:25", "is_lecture": true, "end": "00:05:36", "is_worked_example": false, "text": "Since flushing the pipeline has a big impact on the effective CPI, we do it when it's the only way to ensure the correct behavior of the execution pipeline."}, {"start": "00:05:36", "is_lecture": true, "end": "00:05:42", "is_worked_example": false, "text": "We can be smarter about when we choose to flush the pipeline when executing branches."}, {"start": "00:05:42", "is_lecture": true, "end": "00:05:59", "is_worked_example": false, "text": "If the branch is not taken, it turns out that the pipeline has been doing the right thing by fetching the instruction following the branch."}, {"start": "00:05:59", "is_lecture": true, "end": "00:05:58", "is_worked_example": false, "text": "Starting execution of an instruction even when we're unsure whether we really want it executed is called \"speculation\"."}, {"start": "00:05:58", "is_lecture": true, "end": "00:06:08", "is_worked_example": false, "text": "Speculative execution is okay if we're able to annul the instruction before it has an effect on the CPU state, e.g., by writing into the register file or main memory."}, {"start": "00:06:08", "is_lecture": true, "end": "00:06:21", "is_worked_example": false, "text": "Since these state changes (called \"side effects\") happen in the later pipeline stages, an instruction can progress through the IF, RF, and ALU stages before we have to make a final decision about whether it should be annulled."}, {"start": "00:06:21", "is_lecture": true, "end": "00:06:25", "is_worked_example": false, "text": "How does speculation help with control hazards?"}, {"start": "00:06:25", "is_lecture": true, "end": "00:06:33", "is_worked_example": false, "text": "Guessing that the next value of the program counter is PC+4 is correct for all but JMPs and taken branches."}, {"start": "00:06:33", "is_lecture": true, "end": "00:06:41", "is_worked_example": false, "text": "Here's our example again, but this time let's assume that the BNE is not taken, i.e., that the value in R3 is zero."}, {"start": "00:06:41", "is_lecture": true, "end": "00:06:46", "is_worked_example": false, "text": "The SUB instruction enters the pipeline at the start of cycle 4."}, {"start": "00:06:46", "is_lecture": true, "end": "00:06:50", "is_worked_example": false, "text": "At the end of cycle 4, we know whether or not to annul the SUB."}, {"start": "00:06:50", "is_lecture": true, "end": "00:06:57", "is_worked_example": false, "text": "If the branch is not taken, we want to execute the SUB instruction, so we just let it continue down the pipeline."}, {"start": "00:06:57", "is_lecture": true, "end": "00:07:05", "is_worked_example": false, "text": "In other words, instead of always annulling the instruction following branch, we only annul it if the branch was taken."}, {"start": "00:07:05", "is_lecture": true, "end": "00:07:12", "is_worked_example": false, "text": "If the branch is not taken, the pipeline has speculated correctly and no instructions need to be annulled."}, {"start": "00:07:12", "is_lecture": true, "end": "00:07:20", "is_worked_example": false, "text": "However if the BNE is taken, the SUB is annulled at the end of cycle 4 and a NOP is executed in cycle 5."}, {"start": "00:07:20", "is_lecture": true, "end": "00:07:24", "is_worked_example": false, "text": "So we only introduce a bubble in the pipeline when there's a taken branch."}, {"start": "00:07:24", "is_lecture": true, "end": "00:07:30", "is_worked_example": false, "text": "Fewer bubbles will decrease the impact of annulment on the effective CPI."}, {"start": "00:07:30", "is_lecture": true, "end": "00:07:40", "is_worked_example": false, "text": "We'll be using the same data path circuitry as before, we'll just be a bit more clever about when the value of the IRSrc_IF control signal is set to 1."}, {"start": "00:07:40", "is_lecture": true, "end": "00:07:47", "is_worked_example": false, "text": "Instead of setting it to 1 for all branches, we only set it to 1 when the branch is taken."}, {"start": "00:07:47", "is_lecture": true, "end": "00:07:55", "is_worked_example": false, "text": "Our naive strategy of always speculating that the next instruction comes from PC+4 is wrong for JMPs and taken branches."}, {"start": "00:07:55", "is_lecture": true, "end": "00:08:05", "is_worked_example": false, "text": "Looking at simulated execution traces, we'll see that this error in speculation leads to about 10% higher effective CPI."}, {"start": "00:08:05", "is_lecture": true, "end": "00:08:06", "is_worked_example": false, "text": "Can we do better?"}, {"start": "00:08:06", "is_lecture": true, "end": "00:08:11", "is_worked_example": false, "text": "This is an important question for CPUs with deep pipelines."}, {"start": "00:08:11", "is_lecture": true, "end": "00:08:20", "is_worked_example": false, "text": "For example, Intel's Nehalem processor from 2009 resolves the more complex x86 branch instructions quite late in the pipeline."}, {"start": "00:08:20", "is_lecture": true, "end": "00:08:32", "is_worked_example": false, "text": "Since Nehalem is capable of executing multiple instructions each cycle, flushing the pipeline in Nehalem actually annuls the execution of many instructions, resulting in a considerable hit on the CPI."}, {"start": "00:08:32", "is_lecture": true, "end": "00:08:41", "is_worked_example": false, "text": "Like many modern processor implementations, Nehalem has a much more sophisticated speculation mechanism."}, {"start": "00:08:41", "is_lecture": true, "end": "00:08:49", "is_worked_example": false, "text": "Rather than always guessing the next instruction is at PC+4, it only does that for non-branch instructions."}, {"start": "00:08:49", "is_lecture": true, "end": "00:08:59", "is_worked_example": false, "text": "For branches, it predicts the behavior of each individual branch based on what the branch did last time it was executed and some knowledge of how the branch is being used."}, {"start": "00:08:59", "is_lecture": true, "end": "00:09:09", "is_worked_example": false, "text": "For example, backward branches at the end of loops, which are taken for all but the final iteration of the loop, can be identified by their negative branch offset values."}, {"start": "00:09:09", "is_lecture": true, "end": "00:09:20", "is_worked_example": false, "text": "Nehalem can even determine if there's correlation between branch instructions, using the results of an another, earlier branch to speculate on the branch decision of the current branch."}, {"start": "00:09:20", "is_lecture": true, "end": "00:09:32", "is_worked_example": false, "text": "With these sophisticated strategies, Nehalem's speculation is correct 95% to 99% of the time, greatly reducing the impact of branches on the effective CPI."}, {"start": "00:09:32", "is_lecture": true, "end": "00:09:38", "is_worked_example": false, "text": "There's also the lazy option of changing the ISA to deal with control hazards."}, {"start": "00:09:38", "is_lecture": true, "end": "00:09:45", "is_worked_example": false, "text": "For example, we could change the ISA to specify that the instruction following a jump or branch is always executed."}, {"start": "00:09:45", "is_lecture": true, "end": "00:09:49", "is_worked_example": false, "text": "In other words the transfer of control happens *after* the next instruction."}, {"start": "00:09:49", "is_lecture": true, "end": "00:09:55", "is_worked_example": false, "text": "This change ensures that the guess of PC+4 as the address of the next instruction is always correct!"}, {"start": "00:09:55", "is_lecture": true, "end": "00:10:08", "is_worked_example": false, "text": "In the example shown here, assuming we changed the ISA, we can reorganize the execution order of the loop to place the MUL instruction after the BNE instruction, in the so-called \"branch delay slot\"."}, {"start": "00:10:08", "is_lecture": true, "end": "00:10:16", "is_worked_example": false, "text": "Since the instruction in the branch delay slot is always executed, the MUL instruction will be executed during each iteration of the loop."}, {"start": "00:10:16", "is_lecture": true, "end": "00:10:20", "is_worked_example": false, "text": "The resulting execution is shown in this pipeline diagram."}, {"start": "00:10:20", "is_lecture": true, "end": "00:10:29", "is_worked_example": false, "text": "Assuming we can find an appropriate instruction to place in the delay slot, the branch will have zero impact on the effective CPI."}, {"start": "00:10:29", "is_lecture": true, "end": "00:10:32", "is_worked_example": false, "text": "Are branch delay slots a good idea?"}, {"start": "00:10:32", "is_lecture": true, "end": "00:10:38", "is_worked_example": false, "text": "Seems like they reduce the negative impact that branches might have on instruction throughput."}, {"start": "00:10:38", "is_lecture": true, "end": "00:10:45", "is_worked_example": false, "text": "The downside is that only half the time can we find instructions to move to the branch delay slot."}, {"start": "00:10:45", "is_lecture": true, "end": "00:10:51", "is_worked_example": false, "text": "The other half of the time we have to fill it with an explicit NOP instruction, increasing the size of the code."}, {"start": "00:10:51", "is_lecture": true, "end": "00:10:59", "is_worked_example": false, "text": "And if we make the branch decision later in the pipeline, there are more branch delay slots, which would be even harder to fill."}, {"start": "00:10:59", "is_lecture": true, "end": "00:11:06", "is_worked_example": false, "text": "In practice, it turns out that branch prediction works better than delay slots in reducing the impact of branches."}, {"start": "00:11:06", "is_lecture": true, "end": "00:11:14", "is_worked_example": false, "text": "So, once again we see that it's problematic to alter the ISA to improve the throughput of pipelined execution."}, {"start": "00:11:14", "is_lecture": true, "end": "00:11:25", "is_worked_example": false, "text": "ISAs outlive implementations, so it's best not to change the execution semantics to deal with performance issues created by a particular implementation."}]}, "C08S02B01-WE.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c8/c8s2/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c8s2v1", "items": [{"start": "00:00:00", "is_lecture": false, "end": "00:00:07", "is_worked_example": true, "text": "For this problem, assume that we have a computer system that has three devices D1, D2, and D3."}, {"start": "00:00:07", "is_lecture": false, "end": "00:00:11", "is_worked_example": true, "text": "Each of these devices can cause interrupts in our system."}, {"start": "00:00:11", "is_lecture": false, "end": "00:00:16", "is_worked_example": true, "text": "This table summarizes the interrupt characteristics of our three devices."}, {"start": "00:00:16", "is_lecture": false, "end": "00:00:23", "is_worked_example": true, "text": "For each device, we are given its service time which is the amount of time it takes to service an interrupt for that particular device."}, {"start": "00:00:23", "is_lecture": false, "end": "00:00:30", "is_worked_example": true, "text": "We are given an interrupt frequency which tells us how frequently the interrupts for that device arrive."}, {"start": "00:00:30", "is_lecture": false, "end": "00:00:35", "is_worked_example": true, "text": "You can assume that the first interrupt of each device can arrive at any time."}, {"start": "00:00:35", "is_lecture": false, "end": "00:00:44", "is_worked_example": true, "text": "The deadline is the longest amount of time that is allowed between the interrupt request and the completion of the interrupt handler."}, {"start": "00:00:44", "is_lecture": false, "end": "00:00:50", "is_worked_example": true, "text": "Assume we have a program P that takes 100 seconds to execute when interrupts are disabled."}, {"start": "00:00:50", "is_lecture": false, "end": "00:00:57", "is_worked_example": true, "text": "We would like to figure out how long it would take to execute this program when interrupts are enabled."}, {"start": "00:00:57", "is_lecture": false, "end": "00:01:06", "is_worked_example": true, "text": "To answer this question, we need to determine the amount of cpu time that is dedicated to the handling of each of the three devices."}, {"start": "00:01:06", "is_lecture": false, "end": "00:01:19", "is_worked_example": true, "text": "D1 has a service time of 400us and it runs every 800us so it is using 400/800 or 50% of the cpu time."}, {"start": "00:01:19", "is_lecture": false, "end": "00:01:34", "is_worked_example": true, "text": "D2 has a service time of 250us and it runs every 1000us so it is using 250/1000 or 25% of the cpu time."}, {"start": "00:01:34", "is_lecture": false, "end": "00:01:42", "is_worked_example": true, "text": "D3 uses 100/800 or 12.5% of the cpu time."}, {"start": "00:01:42", "is_lecture": false, "end": "00:01:47", "is_worked_example": true, "text": "This means that the user programs have the remaining cpu time available to them."}, {"start": "00:01:47", "is_lecture": false, "end": "00:01:53", "is_worked_example": true, "text": "The remaining cpu time is 12.5% or 1/8 of the cpu time."}, {"start": "00:01:53", "is_lecture": false, "end": "00:02:05", "is_worked_example": true, "text": "If the user program can only run for one eighth of the time, that means that a program that takes 100 seconds without interrupts will take 800 seconds to run with interrupts enabled."}, {"start": "00:02:05", "is_lecture": false, "end": "00:02:13", "is_worked_example": true, "text": "We want to consider whether there is a weak priority ordering that could satisfy all of the constraints for this system?"}, {"start": "00:02:13", "is_lecture": false, "end": "00:02:27", "is_worked_example": true, "text": "Recall that with a weak priority ordering, there is no preemption, so if an interrupt handler has begun running it runs to completion even if another interrupt of higher priority arrives before its completion."}, {"start": "00:02:27", "is_lecture": false, "end": "00:02:36", "is_worked_example": true, "text": "Upon completion, all interrupts that have arrived, regardless of their order of arrival, are processed in priority order."}, {"start": "00:02:36", "is_lecture": false, "end": "00:02:44", "is_worked_example": true, "text": "If there is a weak priority ordering that satisfies our system, then we should determine the priority ordering."}, {"start": "00:02:44", "is_lecture": false, "end": "00:02:52", "is_worked_example": true, "text": "If there is no such ordering, then we should identify the devices for which a weak priority ordering cannot guarantee meeting all the constraints."}, {"start": "00:02:52", "is_lecture": false, "end": "00:03:21", "is_worked_example": true, "text": "Returning to our device characteristics table and comparing our deadlines to the device service times, we see that in a weak priority system if the D1 handler which has a service time of 400us happens to be running when a D2 or D3 interrupt arrives, then the D2 or D3 devices could miss their deadlines because the service time of D1 plus their own service time is greater than their deadline."}, {"start": "00:03:21", "is_lecture": false, "end": "00:03:44", "is_worked_example": true, "text": "In other words, if D2 or D3 have to wait up to 400us before beginning to be serviced then their completion time won't be until 650us for D2 which is greater than its deadline of 300us, and 500us for D3 which is greater than its deadline of 400us."}, {"start": "00:03:44", "is_lecture": false, "end": "00:03:53", "is_worked_example": true, "text": "Thus, there is no weak priority system ordering which is guaranteed to satisfy all of our system constraints."}, {"start": "00:03:53", "is_lecture": false, "end": "00:03:59", "is_worked_example": true, "text": "Now, lets reconsider the same question assuming a strong priority ordering."}, {"start": "00:03:59", "is_lecture": false, "end": "00:04:09", "is_worked_example": true, "text": "Recall that with a strong priority ordering, the handler for a device with a higher priority will pre-empt a running handler of a lower priority device."}, {"start": "00:04:09", "is_lecture": false, "end": "00:04:28", "is_worked_example": true, "text": "In other words if the priority of A is greater than B and an A interrupt arrives midway through the handling of a B interrupt, then the B interrupt handler will get interrupted, the A handler will be run, and upon completion of the A handler, the B handler will be resumed."}, {"start": "00:04:28", "is_lecture": false, "end": "00:04:35", "is_worked_example": true, "text": "If there is a strong priority ordering that satisfies our system, then we should specify what it is."}, {"start": "00:04:35", "is_lecture": false, "end": "00:04:45", "is_worked_example": true, "text": "If there is no such ordering, then we should identify the devices for which even a strong priority ordering cannot guarantee meeting all the constraints."}, {"start": "00:04:45", "is_lecture": false, "end": "00:05:01", "is_worked_example": true, "text": "Since we now allow preemption of lower priority device handlers in order to satisfy the requirements of a higher priority handler, we are no longer faced with the issue that devices D2 and D3 can't meet their deadlines if D1 happens to be running first."}, {"start": "00:05:01", "is_lecture": false, "end": "00:05:19", "is_worked_example": true, "text": "In addition, since at the beginning of our problem we determined that there is enough time to service all of our interrupts given their service times and interrupt frequencies, that means that there must exist a strong priority ordering that can satisfy all the constraints of our system."}, {"start": "00:05:19", "is_lecture": false, "end": "00:05:30", "is_worked_example": true, "text": "You can use the scheme that a device with a shorter deadline should have a higher priority than one with a longer deadline to arrive at a valid strong priority ordering."}, {"start": "00:05:30", "is_lecture": false, "end": "00:05:39", "is_worked_example": true, "text": "A valid strong priority ordering is D2 has the highest priority, then D3 and then D1."}, {"start": "00:05:39", "is_lecture": false, "end": "00:05:46", "is_worked_example": true, "text": "Another way of expressing this is D2 > D3 > D1."}, {"start": "00:05:46", "is_lecture": false, "end": "00:05:55", "is_worked_example": true, "text": "Note that for this example, this priority ordering is the only valid ordering that will satisfy all the constraints of our strong priority system."}, {"start": "00:05:55", "is_lecture": false, "end": "00:06:04", "is_worked_example": true, "text": "To convince ourselves of this, let's take a closer look at other priority possibilities and determine what would happen in those situations."}, {"start": "00:06:04", "is_lecture": false, "end": "00:06:13", "is_worked_example": true, "text": "If D1 had a higher priority than either D2 or D3, then the deadlines for D2 and D3 would not be guaranteed to be satisfied."}, {"start": "00:06:13", "is_lecture": false, "end": "00:06:17", "is_worked_example": true, "text": "This means that D1 must have the lowest priority."}, {"start": "00:06:17", "is_lecture": false, "end": "00:06:34", "is_worked_example": true, "text": "Now between D2 and D3, if D3 had a higher priority than D2, then if D3 was being serviced when a D2 interrupt arrived, the D2 interrupt may not complete until 350us which is beyond its deadline."}, {"start": "00:06:34", "is_lecture": false, "end": "00:06:42", "is_worked_example": true, "text": "So this shows us that D2 must have the highest priority, then D3 and finally D1."}]}, "C06S01B02-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c6/c6s1/2?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c6s1v2", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:05", "is_worked_example": false, "text": "Let's create a new abstraction called a \"process\" to capture the notion of a running program."}, {"start": "00:00:05", "is_lecture": true, "end": "00:00:15", "is_worked_example": false, "text": "A process encompasses all the resources that would be used when running a program including those of the CPU, the MMU, input and output devices, etc."}, {"start": "00:00:15", "is_lecture": true, "end": "00:00:20", "is_worked_example": false, "text": "Each process has a \"state\" that captures everything we know about its execution."}, {"start": "00:00:20", "is_lecture": true, "end": "00:00:22", "is_worked_example": false, "text": "The process state includes"}, {"start": "00:00:22", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "* the hardware state of the CPU, i.e., the values in the registers and program counter."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:37", "is_worked_example": false, "text": "* the contents of the process' virtual address space, including code, data values, the stack, and data objects dynamically allocated from the heap."}, {"start": "00:00:37", "is_lecture": true, "end": "00:00:45", "is_worked_example": false, "text": "Under the management of the MMU, this portion of the state can be resident in main memory or can reside in secondary storage."}, {"start": "00:00:45", "is_lecture": true, "end": "00:00:53", "is_worked_example": false, "text": "* the hardware state of the MMU, which, as we saw earlier, depends on the context-number and page-directory registers."}, {"start": "00:00:53", "is_lecture": true, "end": "00:00:57", "is_worked_example": false, "text": "Also included are the pages allocated for the hierarchical page map."}, {"start": "00:00:57", "is_lecture": true, "end": "00:01:06", "is_worked_example": false, "text": "* additional information about the process' input and output activities, such as where it has reached in reading or writing files in the file system,"}, {"start": "00:01:06", "is_lecture": true, "end": "00:01:10", "is_worked_example": false, "text": "the status and buffers associated with open network connections,"}, {"start": "00:01:10", "is_lecture": true, "end": "00:01:16", "is_worked_example": false, "text": "pending events from the user interface (e.g., keyboard characters and mouse clicks), and so on."}, {"start": "00:01:16", "is_lecture": true, "end": "00:01:25", "is_worked_example": false, "text": "As we'll see, there is a special, privileged process, called the operating system (OS), running in its own kernel-mode context."}, {"start": "00:01:25", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "The OS manages all the bookkeeping for each process, arranging for the process run periodically."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:44", "is_worked_example": false, "text": "The OS will provide various services to the processes, such as accessing data in files, establishing network connections, managing the window system and user interface, and so on."}, {"start": "00:01:44", "is_lecture": true, "end": "00:01:53", "is_worked_example": false, "text": "To switch from running one user-mode process to another, the OS will need to capture and save the *entire* state of the current user-mode process."}, {"start": "00:01:53", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "Some of it already lives in main memory, so we're all set there."}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:01", "is_worked_example": false, "text": "Some of it will be found in various kernel data structures."}, {"start": "00:02:01", "is_lecture": true, "end": "00:02:08", "is_worked_example": false, "text": "And some of it we'll need to be able to save and restore from the various hardware resources in the CPU and MMU."}, {"start": "00:02:08", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "In order to successfully implement processes, the OS must be able to make it seem as if each process was running on its own \"virtual machine\" that works independently of other virtual machines for other processes."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:30", "is_worked_example": false, "text": "Our goal is to efficiently share one physical machine between all the virtual machines."}, {"start": "00:02:30", "is_lecture": true, "end": "00:02:33", "is_worked_example": false, "text": "Here's a sketch of the organization we're proposing."}, {"start": "00:02:33", "is_lecture": true, "end": "00:02:38", "is_worked_example": false, "text": "The resources provided by a physical machine are shown at the bottom of the slide."}, {"start": "00:02:38", "is_lecture": true, "end": "00:02:43", "is_worked_example": false, "text": "The CPU and main memory form the computation engine at heart of the system."}, {"start": "00:02:43", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "Connected to the CPU are various peripherals, a collective noun coined from the English word \"periphery\" that indicates the resources surrounding the CPU."}, {"start": "00:02:53", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "A timer generates periodic CPU interrupts that can be used to trigger periodic actions."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:05", "is_worked_example": false, "text": "Secondary storage provides high-capacity non-volatile memories for the system."}, {"start": "00:03:05", "is_lecture": true, "end": "00:03:08", "is_worked_example": false, "text": "Connections to the outside world are important too."}, {"start": "00:03:08", "is_lecture": true, "end": "00:03:12", "is_worked_example": false, "text": "Many computers include USB connections for removable devices."}, {"start": "00:03:12", "is_lecture": true, "end": "00:03:16", "is_worked_example": false, "text": "And most provide wired or wireless network connections."}, {"start": "00:03:16", "is_lecture": true, "end": "00:03:23", "is_worked_example": false, "text": "And finally there are usually video monitors, keyboards and mice that serve as the user interface."}, {"start": "00:03:23", "is_lecture": true, "end": "00:03:28", "is_worked_example": false, "text": "Cameras and microphones are becoming increasing important as the next generation of user interface."}, {"start": "00:03:28", "is_lecture": true, "end": "00:03:35", "is_worked_example": false, "text": "The physical machine is managed by the OS running in the privileged kernel context."}, {"start": "00:03:35", "is_lecture": true, "end": "00:03:43", "is_worked_example": false, "text": "The OS handles the low-level interfaces to the peripherals, initializes and manages the MMU contexts, and so on."}, {"start": "00:03:43", "is_lecture": true, "end": "00:03:48", "is_worked_example": false, "text": "It's the OS that creates the virtual machine seen by each process."}, {"start": "00:03:48", "is_lecture": true, "end": "00:03:54", "is_worked_example": false, "text": "User-mode programs run directly on the physical processor, but their execution can be interrupted by the timer,"}, {"start": "00:03:54", "is_lecture": true, "end": "00:04:01", "is_worked_example": false, "text": "giving the OS the opportunity to save away the current process state and move to running the next process."}, {"start": "00:04:01", "is_lecture": true, "end": "00:04:11", "is_worked_example": false, "text": "Via the MMU, the OS provides each process with an independent virtual address space that's isolated from the actions of other processes."}, {"start": "00:04:11", "is_lecture": true, "end": "00:04:19", "is_worked_example": false, "text": "The virtual peripherals provided by the OS isolate the process from all the details of sharing resources with other processes."}, {"start": "00:04:19", "is_lecture": true, "end": "00:04:28", "is_worked_example": false, "text": "The notion of a window allows the process to access a rectangular array of pixels without having to worry if some pixels in the window are hidden by other windows."}, {"start": "00:04:28", "is_lecture": true, "end": "00:04:35", "is_worked_example": false, "text": "Or worrying about how to ensure the mouse cursor always appears on top of whatever is being displayed, and so on."}, {"start": "00:04:35", "is_lecture": true, "end": "00:04:46", "is_worked_example": false, "text": "Instead of accessing I/O devices directly, each process has access to a stream of I/O events that are generated when a character is typed, the mouse is clicked, etc."}, {"start": "00:04:46", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "For example, the OS deals with how to determine which typed characters belong to which process."}, {"start": "00:04:53", "is_lecture": true, "end": "00:05:03", "is_worked_example": false, "text": "In most window systems, the user clicks on a window to indicate that the process that owns the window now has the keyboard focus and should receive any subsequent typed characters."}, {"start": "00:05:03", "is_lecture": true, "end": "00:05:08", "is_worked_example": false, "text": "And the position of the mouse when clicked might determine which process receives the click."}, {"start": "00:05:08", "is_lecture": true, "end": "00:05:17", "is_worked_example": false, "text": "All of which is to say that the details of sharing have been abstracted out of the simple interface provided by the virtual peripherals."}, {"start": "00:05:17", "is_lecture": true, "end": "00:05:21", "is_worked_example": false, "text": "The same is true of accessing files on disk."}, {"start": "00:05:21", "is_lecture": true, "end": "00:05:30", "is_worked_example": false, "text": "The OS provides the useful abstraction of having each file appear as a contiguous, growable array of bytes that supports read and write operations."}, {"start": "00:05:30", "is_lecture": true, "end": "00:05:42", "is_worked_example": false, "text": "The OS knows how the file is mapped to a pool of sectors on the disk and deals with bad sectors, reducing fragmentation, and improving throughput by doing read look-aheads and write behinds."}, {"start": "00:05:42", "is_lecture": true, "end": "00:05:50", "is_worked_example": false, "text": "For networks, the OS provides access to an in-order stream of bytes to some remote socket."}, {"start": "00:05:50", "is_lecture": true, "end": "00:05:59", "is_worked_example": false, "text": "It implements the appropriate network protocols for packetizing the stream, addressing the packets, and dealing with dropped, damaged, or out-of-order packets."}, {"start": "00:05:59", "is_lecture": true, "end": "00:06:12", "is_worked_example": false, "text": "To configure and control these virtual services, the process communicates with the OS using supervisor calls (SVCs), a type of controlled-access procedure call that invokes code in the OS kernel."}, {"start": "00:06:12", "is_lecture": true, "end": "00:06:18", "is_worked_example": false, "text": "The details of the design and implementation of each virtual service are beyond the scope of this course."}, {"start": "00:06:18", "is_lecture": true, "end": "00:06:24", "is_worked_example": false, "text": "If you're interested, a course on operating systems will explore each of these topics in detail."}, {"start": "00:06:24", "is_lecture": true, "end": "00:06:34", "is_worked_example": false, "text": "The OS provides an independent virtual machine for each process, periodically switching from running one process to running the next process."}, {"start": "00:06:34", "is_lecture": true, "end": "00:06:39", "is_worked_example": false, "text": "Let's follow along as we switch from running process #0 to running process #1."}, {"start": "00:06:39", "is_lecture": true, "end": "00:06:44", "is_worked_example": false, "text": "Initially, the CPU is executing user-mode code in process #0."}, {"start": "00:06:44", "is_lecture": true, "end": "00:06:52", "is_worked_example": false, "text": "That execution is interrupted, either by an explicit yield by the program, or, more likely, by a timer interrupt."}, {"start": "00:06:52", "is_lecture": true, "end": "00:07:01", "is_worked_example": false, "text": "Either ends up transferring control to OS code running in kernel mode, while saving the current PC+4 value in the XP register."}, {"start": "00:07:01", "is_lecture": true, "end": "00:07:05", "is_worked_example": false, "text": "We'll talk about the interrupt mechanism in more detail in just a moment."}, {"start": "00:07:05", "is_lecture": true, "end": "00:07:10", "is_worked_example": false, "text": "The OS saves the state of process #0 in the appropriate table in kernel storage."}, {"start": "00:07:10", "is_lecture": true, "end": "00:07:14", "is_worked_example": false, "text": "Then it reloads the state from the kernel table for process #1."}, {"start": "00:07:14", "is_lecture": true, "end": "00:07:20", "is_worked_example": false, "text": "Note that the process #1 state was saved when process #1 was interrupted at some earlier point."}, {"start": "00:07:20", "is_lecture": true, "end": "00:07:26", "is_worked_example": false, "text": "The OS then uses a JMP() to resume user-mode execution using the newly restored process #1 state."}, {"start": "00:07:26", "is_lecture": true, "end": "00:07:32", "is_worked_example": false, "text": "Execution resumes in process #1 just where it was when interrupted earlier."}, {"start": "00:07:32", "is_lecture": true, "end": "00:07:37", "is_worked_example": false, "text": "And now we're running the user-mode program in process #1."}, {"start": "00:07:37", "is_lecture": true, "end": "00:07:42", "is_worked_example": false, "text": "We've interrupted one process and resumed execution of another."}, {"start": "00:07:42", "is_lecture": true, "end": "00:07:50", "is_worked_example": false, "text": "We'll keep doing this in a round-robin fashion, giving each process a chance to run, before starting another round of execution."}, {"start": "00:07:50", "is_lecture": true, "end": "00:07:54", "is_worked_example": false, "text": "The black arrows give a sense for how time proceeds."}, {"start": "00:07:54", "is_lecture": true, "end": "00:08:00", "is_worked_example": false, "text": "For each process, virtual time unfolds as a sequence of executed instructions."}, {"start": "00:08:00", "is_lecture": true, "end": "00:08:07", "is_worked_example": false, "text": "Unless it looks at a real-time clock, a process is unaware that occasionally its execution is suspended for a while."}, {"start": "00:08:07", "is_lecture": true, "end": "00:08:12", "is_worked_example": false, "text": "The suspension and resumption are completely transparent to a running process."}, {"start": "00:08:12", "is_lecture": true, "end": "00:08:26", "is_worked_example": false, "text": "Of course, from the outside we can see that in real time, the execution path moves from process to process, visiting the OS during switches, producing the dove-tailed execution path we see here."}, {"start": "00:08:26", "is_lecture": true, "end": "00:08:35", "is_worked_example": false, "text": "Time-multiplexing of the CPU is called \"timesharing\" and we'll examine the implementation in more detail in the following segment."}]}, "C11S01B01-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c11/c11s1/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c11s1v1", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:06", "is_worked_example": false, "text": "Computer systems bring together many technologies and harness them to provide fast execution of programs."}, {"start": "00:00:06", "is_lecture": true, "end": "00:00:11", "is_worked_example": false, "text": "Some of these technologies are relatively new, others have been with us for decades."}, {"start": "00:00:11", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "Each of the system components comes with a detailed specification of their functionality and interface."}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "The expectation is that system designers can engineer the system based on the component specifications without having to know the details of the implementations of each component."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:40", "is_worked_example": false, "text": "This is good since many of the underlying technologies change, often in ways that allow the components to become smaller, faster, cheaper, more energy efficient, and so on."}, {"start": "00:00:40", "is_lecture": true, "end": "00:00:48", "is_worked_example": false, "text": "Assuming the new components still implement same interfaces, they can be integrated into the system with very little effort."}, {"start": "00:00:48", "is_lecture": true, "end": "00:00:55", "is_worked_example": false, "text": "The moral of this story is that the important part of the system architecture is the interfaces."}, {"start": "00:00:55", "is_lecture": true, "end": "00:01:02", "is_worked_example": false, "text": "Our goal is to design interface specifications that can survive many generations of technological change."}, {"start": "00:01:02", "is_lecture": true, "end": "00:01:12", "is_worked_example": false, "text": "One approach to long-term survival is to base the specification on a useful abstraction that hides most, if not all, of the low-level implementation details."}, {"start": "00:01:12", "is_lecture": true, "end": "00:01:17", "is_worked_example": false, "text": "Operating systems provide many interfaces that have remained stable for many years."}, {"start": "00:01:17", "is_lecture": true, "end": "00:01:28", "is_worked_example": false, "text": "For example, network interfaces that reliably deliver streams of bytes to named hosts, hiding the details of packets, sockets, error detection and recovery, etc."}, {"start": "00:01:28", "is_lecture": true, "end": "00:01:37", "is_worked_example": false, "text": "Or windowing and graphics systems that render complex images, shielding the application from details about the underlying graphics engine."}, {"start": "00:01:37", "is_lecture": true, "end": "00:01:44", "is_worked_example": false, "text": "Or journaled file systems that behind-the-scenes defend against corruption in the secondary storage arrays."}, {"start": "00:01:44", "is_lecture": true, "end": "00:01:55", "is_worked_example": false, "text": "Basically, we're long since past the point where we can afford to start from scratch each time the integrated circuit gurus are able to double the number of transistors on a chip,"}, {"start": "00:01:55", "is_lecture": true, "end": "00:02:01", "is_worked_example": false, "text": "or the communication wizards figure out how to go from 1GHz networks to 10GHz networks,"}, {"start": "00:02:01", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "or the memory mavens are able to increase the size of main memory by a factor of 4."}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:17", "is_worked_example": false, "text": "The interfaces that insulate us from technological change are critical to ensure that improved technology isn't a constant source of disruption."}, {"start": "00:02:17", "is_lecture": true, "end": "00:02:26", "is_worked_example": false, "text": "There are some famous examples of where an apparently convenient choice of interface has had embarrassing long-term consequences."}, {"start": "00:02:26", "is_lecture": true, "end": "00:02:36", "is_worked_example": false, "text": "For example, back in the days of stand-alone computing, different ISAs made different choices on how to store multi-byte numeric values in main memory."}, {"start": "00:02:36", "is_lecture": true, "end": "00:02:50", "is_worked_example": false, "text": "IBM architectures store the most-significant byte in the lowest address (so called \"big endian\"), while Intel's x86 architectures store the least-significant byte first (so called \"little endian\")."}, {"start": "00:02:50", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "But this leads to all sorts of complications in a networked world where numeric data is often transferred from system to system."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:06", "is_worked_example": false, "text": "This is a prime example of a locally-optimal choice having an unfortunate global impact."}, {"start": "00:03:06", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "As the phrase goes: \"a moment of convenience, a lifetime of regret.\""}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:22", "is_worked_example": false, "text": "Another example is the system-level communication strategy chosen for the first IBM PC, the original personal computer based Intel CPU chips."}, {"start": "00:03:22", "is_lecture": true, "end": "00:03:33", "is_worked_example": false, "text": "IBM built their expansion bus for adding I/O peripherals, memory cards, etc., by simply using the interface signals provided by then-current x86 CPU."}, {"start": "00:03:33", "is_lecture": true, "end": "00:03:44", "is_worked_example": false, "text": "So the width of the data bus, the number of address pins, the data-transfer protocol, etc. where are exactly as designed for interfacing to that particular CPU."}, {"start": "00:03:44", "is_lecture": true, "end": "00:03:50", "is_worked_example": false, "text": "A logical choice since it got the job done while keeping costs as low a possible."}, {"start": "00:03:50", "is_lecture": true, "end": "00:04:03", "is_worked_example": false, "text": "But that choice quickly proved unfortunate as newer, higher-performance CPUs were introduced, capable of addressing more memory or providing 32-bit instead of 16-bit external data paths."}, {"start": "00:04:03", "is_lecture": true, "end": "00:04:12", "is_worked_example": false, "text": "So system architects were forced into offering customers the Hobson's choice of crippling system throughput for the sake of backward compatibility,"}, {"start": "00:04:12", "is_lecture": true, "end": "00:04:19", "is_worked_example": false, "text": "or discarding the networking card they bought last year since it was now incompatible with this year's system."}, {"start": "00:04:19", "is_lecture": true, "end": "00:04:22", "is_worked_example": false, "text": "But there are success stories too."}, {"start": "00:04:22", "is_lecture": true, "end": "00:04:35", "is_worked_example": false, "text": "The System/360 interfaces chosen by IBM in the early 1960s carried over to the System/370 in the 70's and 80's and to the Enterprise System Architecture/390 of the 90's."}, {"start": "00:04:35", "is_lecture": true, "end": "00:04:46", "is_worked_example": false, "text": "Customers had the expectation that software written for the earlier machines would continue to work on the newer systems and IBM was able to fulfill that expectation."}, {"start": "00:04:46", "is_lecture": true, "end": "00:04:59", "is_worked_example": false, "text": "Maybe the most notable long-term interface success is the design the TCP and IP network protocols in the early 70's, which formed the basis for most packet-based network communication."}, {"start": "00:04:59", "is_lecture": true, "end": "00:05:09", "is_worked_example": false, "text": "A recent refresh expanded the network addresses from 32 to 128 bits, but that was largely invisible to applications using the network."}, {"start": "00:05:09", "is_lecture": true, "end": "00:05:19", "is_worked_example": false, "text": "It was a remarkably prescient set of engineering choices that stood the test of time for over four decades of exponential growth in network connectivity."}, {"start": "00:05:19", "is_lecture": true, "end": "00:05:27", "is_worked_example": false, "text": "Today's lecture topic is figuring out the appropriate interface choices for interconnecting system components."}, {"start": "00:05:27", "is_lecture": true, "end": "00:05:37", "is_worked_example": false, "text": "In the earliest systems these connections were very ad hoc in the sense that the protocols and physical implementation were chosen independently for each connection that had to be made."}, {"start": "00:05:37", "is_lecture": true, "end": "00:05:49", "is_worked_example": false, "text": "The cable connecting the CPU box to the memory box (yes, in those days, they lived in separate 19\" racks!) was different than the cable connecting the CPU to the disk."}, {"start": "00:05:49", "is_lecture": true, "end": "00:05:55", "is_worked_example": false, "text": "Improving circuit technologies allowed system components to shrink from cabinet-size to board-size"}, {"start": "00:05:55", "is_lecture": true, "end": "00:06:04", "is_worked_example": false, "text": "and system engineers designed a modular packaging scheme that allowed users to mix-and-match board types that plugged into a communication backplane."}, {"start": "00:06:04", "is_lecture": true, "end": "00:06:14", "is_worked_example": false, "text": "The protocols and signals on the backplane reflected the different choices made by each vendor -- IBM boards didn't plug into Digital Equipment backplanes, and vice versa."}, {"start": "00:06:14", "is_lecture": true, "end": "00:06:27", "is_worked_example": false, "text": "This evolved into some standardized communication backplanes that allowed users to do their own system integration, choosing different vendors for their CPU, memory, networking, etc."}, {"start": "00:06:27", "is_lecture": true, "end": "00:06:31", "is_worked_example": false, "text": "Healthy competition quickly brought prices down and drove innovation."}, {"start": "00:06:31", "is_lecture": true, "end": "00:06:44", "is_worked_example": false, "text": "However this promising development was overtaken by rapidly improving performance, which required communication bandwidths that simply could not be supported across a multi-board backplane."}, {"start": "00:06:44", "is_lecture": true, "end": "00:06:55", "is_worked_example": false, "text": "These demands for higher performance and the ability to integrate many different communication channels into a single chip, lead to a proliferation of different channels."}, {"start": "00:06:55", "is_lecture": true, "end": "00:07:05", "is_worked_example": false, "text": "In many ways, the system architecture was reminiscent of the original systems -- ad-hoc purpose-built communication channels specialized to a specific task."}, {"start": "00:07:05", "is_lecture": true, "end": "00:07:15", "is_worked_example": false, "text": "As we'll see, engineering considerations have led to the widespread adoption of general-purpose unidirectional point-to-point communication channels."}, {"start": "00:07:15", "is_lecture": true, "end": "00:07:21", "is_worked_example": false, "text": "There are still several types of channels depending on the required performance, the distance travelled, etc.,"}, {"start": "00:07:21", "is_lecture": true, "end": "00:07:28", "is_worked_example": false, "text": "but asynchronous point-to-point channels have mostly replaced the synchronous multi-signal channels of earlier systems."}, {"start": "00:07:28", "is_lecture": true, "end": "00:07:41", "is_worked_example": false, "text": "Most system-level communications involve signaling over wires, so next we'll look into some the engineering issues we've had to deal with as communication speeds have increased from kHz to GHz."}]}, "C05S02B01-WE.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c5/c5s2/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c5s2v1", "items": [{"start": "00:00:00", "is_lecture": false, "end": "00:00:06", "is_worked_example": true, "text": "Virtual memory allows programs to behave as if they have a larger memory than they actually do."}, {"start": "00:00:06", "is_lecture": false, "end": "00:00:14", "is_worked_example": true, "text": "The way this works is by using virtual addresses, which refer to addresses on disk, in our programs."}, {"start": "00:00:14", "is_lecture": false, "end": "00:00:23", "is_worked_example": true, "text": "The virtual addresses are translated into physical addresses using the page map which is a lookup table that has one entry per virtual page."}, {"start": "00:00:23", "is_lecture": false, "end": "00:00:31", "is_worked_example": true, "text": "The page map knows whether the virtual page is in physical memory and if so it immediately returns the physical page number."}, {"start": "00:00:31", "is_lecture": false, "end": "00:00:42", "is_worked_example": true, "text": "If the page is not in physical memory, then this causes a fault which means that the virtual page must be brought in from disk to physical memory before it can be accessed."}, {"start": "00:00:42", "is_lecture": false, "end": "00:00:52", "is_worked_example": true, "text": "To do this the least recently used (LRU) page in the physical memory is removed to make room for the address that is currently being requested."}, {"start": "00:00:52", "is_lecture": false, "end": "00:00:57", "is_worked_example": true, "text": "The page map is also updated with the new mapping of virtual to physical pages."}, {"start": "00:00:57", "is_lecture": false, "end": "00:01:04", "is_worked_example": true, "text": "Since bringing data to and from disk is an expensive operation, data is moved in chunks."}, {"start": "00:01:04", "is_lecture": false, "end": "00:01:10", "is_worked_example": true, "text": "This makes sense because of the concept of locality which we studied as part of our Caches unit."}, {"start": "00:01:10", "is_lecture": false, "end": "00:01:21", "is_worked_example": true, "text": "The idea is that instructions, or data, that are close to the current address are likely to be accessed as well, so it makes sense to fetch more than one word of data at a time."}, {"start": "00:01:21", "is_lecture": false, "end": "00:01:32", "is_worked_example": true, "text": "This is especially true if the cost of fetching the first word is significantly higher than the cost of fetching adjacent memory locations as is the case with accesses to disk."}, {"start": "00:01:32", "is_lecture": false, "end": "00:01:37", "is_worked_example": true, "text": "So data is moved back and forth from disk in pages."}, {"start": "00:01:37", "is_lecture": false, "end": "00:01:41", "is_worked_example": true, "text": "The size of a page is the same in both virtual and physical memory."}, {"start": "00:01:41", "is_lecture": false, "end": "00:01:45", "is_worked_example": true, "text": "Lets look at an example of how virtual memory is used."}, {"start": "00:01:45", "is_lecture": false, "end": "00:01:59", "is_worked_example": true, "text": "While it is usually the case that the virtual address space is larger than the physical address space, this is not a requirement and in this problem the virtual address space happens to be smaller than the physical address space."}, {"start": "00:01:59", "is_lecture": false, "end": "00:02:06", "is_worked_example": true, "text": "Specifically, virtual addresses are 16 bits long so they can address 2^16 bytes."}, {"start": "00:02:06", "is_lecture": false, "end": "00:02:14", "is_worked_example": true, "text": "Physical addresses are 20 bits long so that means that our physical memory is of size 2^20 bytes."}, {"start": "00:02:14", "is_lecture": false, "end": "00:02:20", "is_worked_example": true, "text": "Our page size is 2^8 bytes or 256 bytes per page."}, {"start": "00:02:20", "is_lecture": false, "end": "00:02:30", "is_worked_example": true, "text": "This means that the 16 bit virtual address consists of 8 bits of page offset and another 8 bits for the virtual page number (or VPN)."}, {"start": "00:02:30", "is_lecture": false, "end": "00:02:39", "is_worked_example": true, "text": "The 20 bit physical address consists of the same 8 bit page offset and another 12 bits for the physical page number (or PPN)."}, {"start": "00:02:39", "is_lecture": false, "end": "00:02:46", "is_worked_example": true, "text": "The first question we want to consider is what is the size of the page map in this example?"}, {"start": "00:02:46", "is_lecture": false, "end": "00:02:53", "is_worked_example": true, "text": "Recall that a page map has 1 entry per virtual page in order to map  each virtual page to a physical page."}, {"start": "00:02:53", "is_lecture": false, "end": "00:03:02", "is_worked_example": true, "text": "This means that the number of entries in the page map is 2^8 where 8 is the number of bits in the VPN."}, {"start": "00:03:02", "is_lecture": false, "end": "00:03:11", "is_worked_example": true, "text": "The size of each page map entry is 14 bits, 12 for the PPN, 1 for the dirty bit and 1 for the resident bit."}, {"start": "00:03:11", "is_lecture": false, "end": "00:03:22", "is_worked_example": true, "text": "Suppose that you are told that the page size is doubled in size so that there are now 2^9 bytes per page, but the size of your physical and virtual addresses remain the same."}, {"start": "00:03:22", "is_lecture": false, "end": "00:03:28", "is_worked_example": true, "text": "We would like to determine what effect this change would have on some of the page map attributes."}, {"start": "00:03:28", "is_lecture": false, "end": "00:03:33", "is_worked_example": true, "text": "The first question is how does the size of each page map entry in bits change?"}, {"start": "00:03:33", "is_lecture": false, "end": "00:03:47", "is_worked_example": true, "text": "Since the size of a physical address continues to be 20 bits long, then the change in page offset size from 8 to 9 bits implies that the size of the PPN decreased by 1 bit from 12 to 11."}, {"start": "00:03:47", "is_lecture": false, "end": "00:03:53", "is_worked_example": true, "text": "This implies that the size of each page map entry also decreases by 1 bit."}, {"start": "00:03:53", "is_lecture": false, "end": "00:03:58", "is_worked_example": true, "text": "How are the number of entries in the page map affected by the change in page size?"}, {"start": "00:03:58", "is_lecture": false, "end": "00:04:09", "is_worked_example": true, "text": "Since the number of entries in a page map is equal to the number of  virtual pages, that means that if the size of each page doubled, then  we have half as many virtual pages."}, {"start": "00:04:09", "is_lecture": false, "end": "00:04:15", "is_worked_example": true, "text": "This is shown in the size of the VPN which has decreased from 8 to 7  bits."}, {"start": "00:04:15", "is_lecture": false, "end": "00:04:24", "is_worked_example": true, "text": "This also means that the number of entries in the page map have halved  in size from 2^8 entries down to 2^7 entries."}, {"start": "00:04:24", "is_lecture": false, "end": "00:04:31", "is_worked_example": true, "text": "How about the number of accesses of the page map that are required to translate a single virtual address?"}, {"start": "00:04:31", "is_lecture": false, "end": "00:04:37", "is_worked_example": true, "text": "This parameter does not change as a result of the pages doubling in size."}, {"start": "00:04:37", "is_lecture": false, "end": "00:04:43", "is_worked_example": true, "text": "Suppose we return to our original page size of 256 bytes per page."}, {"start": "00:04:43", "is_lecture": false, "end": "00:04:49", "is_worked_example": true, "text": "We now execute these two lines of code, a load followed by a store operation."}, {"start": "00:04:49", "is_lecture": false, "end": "00:05:04", "is_worked_example": true, "text": "The comment after each instruction shows us the value of the PC when each of the instructions is executed, so it is telling us that the load instruction is at address 0x1FC and the store instruction is at address 0x200."}, {"start": "00:05:04", "is_lecture": false, "end": "00:05:13", "is_worked_example": true, "text": "To execute these two lines of code, we must first fetch each instruction and then perform the data access required by that instruction."}, {"start": "00:05:13", "is_lecture": false, "end": "00:05:21", "is_worked_example": true, "text": "Since our pages are 2^8 bytes long, that means that the bottom 8 bits of our address correspond to the page offset."}, {"start": "00:05:21", "is_lecture": false, "end": "00:05:28", "is_worked_example": true, "text": "Notice that our instruction addresses are specified in hex so 8 bits correspond to the bottom 2 hex characters."}, {"start": "00:05:28", "is_lecture": false, "end": "00:05:39", "is_worked_example": true, "text": "This means that when accessing the LD instruction, the VPN = 1 (which is what remains of our virtual address after removing the bottom 8 bits.)"}, {"start": "00:05:39", "is_lecture": false, "end": "00:05:44", "is_worked_example": true, "text": "The data accessed by the LD instruction comes from VPN 3."}, {"start": "00:05:44", "is_lecture": false, "end": "00:05:53", "is_worked_example": true, "text": "Next we fetch the store instruction from VPN 2, and finally we store an updated value to VPN 6."}, {"start": "00:05:53", "is_lecture": false, "end": "00:06:02", "is_worked_example": true, "text": "Given the page map shown here, we would like to determine the unique physical addresses that are accessed by this code segment."}, {"start": "00:06:02", "is_lecture": false, "end": "00:06:07", "is_worked_example": true, "text": "Recall that the four virtual addresses that will be accessed are:"}, {"start": "00:06:07", "is_lecture": false, "end": "00:06:11", "is_worked_example": true, "text": "0x1FC which is in VPN 1"}, {"start": "00:06:11", "is_lecture": false, "end": "00:06:16", "is_worked_example": true, "text": "0x34C which is in VPN 3"}, {"start": "00:06:16", "is_lecture": false, "end": "00:06:20", "is_worked_example": true, "text": "0x200 which is in VPN 2"}, {"start": "00:06:20", "is_lecture": false, "end": "00:06:25", "is_worked_example": true, "text": "and 0x604 which is in VPN 6."}, {"start": "00:06:25", "is_lecture": false, "end": "00:06:41", "is_worked_example": true, "text": "Assume that all the code and data required to handle page faults is located at physical page 0, your goal is to determine the 5 different physical pages that will get accessed and the order in which they will get accessed by this code segment."}, {"start": "00:06:41", "is_lecture": false, "end": "00:06:45", "is_worked_example": true, "text": "We begin by looking up VPN 1 in our page map."}, {"start": "00:06:45", "is_lecture": false, "end": "00:06:49", "is_worked_example": true, "text": "We see that its resident bit is set to 1."}, {"start": "00:06:49", "is_lecture": false, "end": "00:06:56", "is_worked_example": true, "text": "This means that the virtual page is in physical memory and its PPN is 0x007."}, {"start": "00:06:56", "is_lecture": false, "end": "00:07:07", "is_worked_example": true, "text": "Thus the first physical page that we access is page 0x7, and the first physical address is determined by concatenating the PPN to the page offset."}, {"start": "00:07:07", "is_lecture": false, "end": "00:07:12", "is_worked_example": true, "text": "This results in a physical address of 0x7FC."}, {"start": "00:07:12", "is_lecture": false, "end": "00:07:20", "is_worked_example": true, "text": "Next, we want to load the data at virtual address 0x34C which is in VPN 3."}, {"start": "00:07:20", "is_lecture": false, "end": "00:07:26", "is_worked_example": true, "text": "Looking up VPN 3 in our page map, we find out that its not resident in physical memory."}, {"start": "00:07:26", "is_lecture": false, "end": "00:07:33", "is_worked_example": true, "text": "This means that we need to make room for it by removing the least recently used page from physical memory."}, {"start": "00:07:33", "is_lecture": false, "end": "00:07:40", "is_worked_example": true, "text": "The least recently used page is VPN 2 which maps to PPN 0x602."}, {"start": "00:07:40", "is_lecture": false, "end": "00:07:53", "is_worked_example": true, "text": "Since the dirty bit of our LRU page is 0, that means that we have not done any writes to this page while it was in physical memory so the version in physical memory and on disk are identical."}, {"start": "00:07:53", "is_lecture": false, "end": "00:08:07", "is_worked_example": true, "text": "So to free up physical page 0x602, all we need to do is change the resident bit of VPN 2 to 0 and now we can bring VPN 3 into physical page 0x602."}, {"start": "00:08:07", "is_lecture": false, "end": "00:08:15", "is_worked_example": true, "text": "Recall that the code for handling the page fault is in physical page 0 so the second physical page that we access is page 0."}, {"start": "00:08:15", "is_lecture": false, "end": "00:08:29", "is_worked_example": true, "text": "The updated page map, after handling the page fault, looks like this, where the resident bit for VPN 2 has been set to 0, and PPN 0x602 is now used for VPN 3."}, {"start": "00:08:29", "is_lecture": false, "end": "00:08:36", "is_worked_example": true, "text": "Since this is a LD operation, we are not modifying the page so the dirty bit is set to 0."}, {"start": "00:08:36", "is_lecture": false, "end": "00:08:50", "is_worked_example": true, "text": "The physical address for virtual address 0x34C is now 0x6024C which is now in VPN 0x602."}, {"start": "00:08:50", "is_lecture": false, "end": "00:08:57", "is_worked_example": true, "text": "Next we need to fetch the store instruction from virtual address 0x200 which is in VPN 2."}, {"start": "00:08:57", "is_lecture": false, "end": "00:09:02", "is_worked_example": true, "text": "Since we just removed VPN 2 from physical memory we get another page fault."}, {"start": "00:09:02", "is_lecture": false, "end": "00:09:10", "is_worked_example": true, "text": "This time we will remove the next LRU page from physical memory in order to make room for VPN 2 once again."}, {"start": "00:09:10", "is_lecture": false, "end": "00:09:20", "is_worked_example": true, "text": "In this case, the dirty bit is set to 1 which means that we have written to PPN 0x097 after it was fetched from disk."}, {"start": "00:09:20", "is_lecture": false, "end": "00:09:34", "is_worked_example": true, "text": "This means that the page fault handler will need to first write physical page 0x097 back to virtual page 5 before we can use physical page 0x097 for VPN 2."}, {"start": "00:09:34", "is_lecture": false, "end": "00:09:40", "is_worked_example": true, "text": "After handling the page fault, our updated page map looks like this."}, {"start": "00:09:40", "is_lecture": false, "end": "00:09:49", "is_worked_example": true, "text": "VPN 5 is no longer resident, and instead VPN 2 is resident in physical page 0x097."}, {"start": "00:09:49", "is_lecture": false, "end": "00:09:56", "is_worked_example": true, "text": "In addition, we set the dirty bit to 0 because we have not made any changes to this virtual page."}, {"start": "00:09:56", "is_lecture": false, "end": "00:10:08", "is_worked_example": true, "text": "We now know that virtual address 0x200 maps to physical address 0x09700 after the handling of the page fault."}, {"start": "00:10:08", "is_lecture": false, "end": "00:10:16", "is_worked_example": true, "text": "Finally, we need to perform the store to virtual address 0x604 which is in VPN 6."}, {"start": "00:10:16", "is_lecture": false, "end": "00:10:25", "is_worked_example": true, "text": "Since VPN 6 is resident in physical memory, we can access it at physical page 0x790 as shown in the page map."}, {"start": "00:10:25", "is_lecture": false, "end": "00:10:35", "is_worked_example": true, "text": "This means that virtual address 0x604 maps to physical address 0x79004."}, {"start": "00:10:35", "is_lecture": false, "end": "00:10:46", "is_worked_example": true, "text": "Note that because the dirty bit of VPN 6 was already a 1, we don't need to make any further modifications to the page map as a result of executing the store operation."}, {"start": "00:10:46", "is_lecture": false, "end": "00:10:52", "is_worked_example": true, "text": "If the dirty bit had been a 0, then we would have set it to 1."}, {"start": "00:10:52", "is_lecture": false, "end": "00:11:09", "is_worked_example": true, "text": "So the five physical pages that were accessed by this program are: page 0x7, page 0 for the page faults, page 0x602, page 0x097, and page 0x790."}]}, "C06S01B06-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c6/c6s1/6?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c6s1v6", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:07", "is_worked_example": false, "text": "User-mode programs need to communicate with the OS to request service or get access to useful OS data like the time of day."}, {"start": "00:00:07", "is_lecture": true, "end": "00:00:15", "is_worked_example": false, "text": "But if they're running in a different MMU context than the OS, they don't have direct access to OS code and data."}, {"start": "00:00:15", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "And that might be bad idea in any case:"}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "the OS is usually responsible for implementing security and access policies and other users of the system would be upset if any random user program could circumvent those protections."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "What's needed is the ability for user-mode programs to call OS code at specific entry points, using registers or the user-mode virtual memory to send or receive information."}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:50", "is_worked_example": false, "text": "We'd use these \"supervisor calls\" to access a well-documented and secure OS application programming interface (API)."}, {"start": "00:00:50", "is_lecture": true, "end": "00:00:58", "is_worked_example": false, "text": "An example of such an interface is POSIX (https://en.wikipedia.org/wiki/POSIX), a standard interface implemented by many Unix-like operating systems."}, {"start": "00:00:58", "is_lecture": true, "end": "00:01:09", "is_worked_example": false, "text": "As it turns out, we have a way of transferring control from a user-mode program to a specific OS handler -- just execute an illegal instruction!"}, {"start": "00:01:09", "is_lecture": true, "end": "00:01:16", "is_worked_example": false, "text": "We'll adopt the convention of using illegal instructions with an opcode field of 1 to serve as supervisor calls."}, {"start": "00:01:16", "is_lecture": true, "end": "00:01:25", "is_worked_example": false, "text": "The low order bits of these SVC instructions will contain an index indicating which SVC service we're trying to access."}, {"start": "00:01:25", "is_lecture": true, "end": "00:01:27", "is_worked_example": false, "text": "Let's see how this would work."}, {"start": "00:01:27", "is_lecture": true, "end": "00:01:31", "is_worked_example": false, "text": "Here's our user-mode/kernel-mode diagram again."}, {"start": "00:01:31", "is_lecture": true, "end": "00:01:42", "is_worked_example": false, "text": "Note that the user-mode programs contain supervisor calls with different indices, which when executed are intended to serve as requests for different OS services."}, {"start": "00:01:42", "is_lecture": true, "end": "00:01:56", "is_worked_example": false, "text": "When an SVC instruction is executed, the hardware detects the opcode field of 1 as an illegal instruction and triggers an exception that runs the OS IllOp handler, as we saw in the previous segment."}, {"start": "00:01:56", "is_lecture": true, "end": "00:02:05", "is_worked_example": false, "text": "The handler saves the process state in the temporary storage area, then dispatches to the appropriate handler based on the opcode field."}, {"start": "00:02:05", "is_lecture": true, "end": "00:02:17", "is_worked_example": false, "text": "This handler can access the user's registers in the temporary storage area, or using the appropriate OS subroutines can access the contents of any user-mode virtual address."}, {"start": "00:02:17", "is_lecture": true, "end": "00:02:28", "is_worked_example": false, "text": "If information is to be returned to the user, the return values can be stored in the temporary storage area, overwriting, say, the saved contents of the user's R0 register."}, {"start": "00:02:28", "is_lecture": true, "end": "00:02:36", "is_worked_example": false, "text": "Then, when the handler completes, the potentially-updated saved register values are reloaded into the CPU registers"}, {"start": "00:02:36", "is_lecture": true, "end": "00:02:43", "is_worked_example": false, "text": "and execution of the user-mode program resumes at the instruction following the supervisor call."}, {"start": "00:02:43", "is_lecture": true, "end": "00:02:54", "is_worked_example": false, "text": "In the previous segment we saw how the illegal instruction handler uses a dispatch table to choose the appropriate sub-handler depending on the opcode field of the illegal instruction."}, {"start": "00:02:54", "is_lecture": true, "end": "00:03:02", "is_worked_example": false, "text": "In this slide we see the sub-handler for SVC instructions, i.e., those with an opcode field of 1."}, {"start": "00:03:02", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "This code uses the low-order bits of the instruction to access another dispatch table to select the appropriate code for each of the eight possible SVCs."}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:16", "is_worked_example": false, "text": "Our Tiny OS only has a meagre selection of simple services."}, {"start": "00:03:16", "is_lecture": true, "end": "00:03:27", "is_worked_example": false, "text": "A real OS would have SVCs for accessing files, dealing with network connections, managing virtual memory, spawning new processes, and so on."}, {"start": "00:03:27", "is_lecture": true, "end": "00:03:33", "is_worked_example": false, "text": "Here's the code for resuming execution of the user-mode process when the SVC handler is done:"}, {"start": "00:03:33", "is_lecture": true, "end": "00:03:42", "is_worked_example": false, "text": "simply restore the saved values for the registers and JMP to resume execution at the instruction following the SVC instruction."}, {"start": "00:03:42", "is_lecture": true, "end": "00:03:50", "is_worked_example": false, "text": "There are times when for some reason the SVC request cannot be completed and the request should be retried in the future."}, {"start": "00:03:50", "is_lecture": true, "end": "00:04:02", "is_worked_example": false, "text": "For example, the ReadCh SVC returns the next character typed by the user, but if no character has yet been typed, the OS cannot complete the request at this time."}, {"start": "00:04:02", "is_lecture": true, "end": "00:04:16", "is_worked_example": false, "text": "In this case, the SVC handler should branch to I_Wait, which arranges for the SVC instruction to be re-executed next time this process runs and then calls Scheduler() to run the next process."}, {"start": "00:04:16", "is_lecture": true, "end": "00:04:23", "is_worked_example": false, "text": "This gives all the other processes a chance to run before the SVC is tried again, hopefully this time successfully."}, {"start": "00:04:23", "is_lecture": true, "end": "00:04:30", "is_worked_example": false, "text": "You can see that this code also serves as the implementation for two different SVCs!"}, {"start": "00:04:30", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "A process can give up the remainder of its current execution time slice by calling the Yield() SVC."}, {"start": "00:03:47", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "This simply causes the OS to call Scheduler(), suspending execution of the current process until its next turn in the round-robin scheduling process."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "And to stop execution, a process can call the Halt() SVC."}, {"start": "00:04:53", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "Looking at the implementation, we can see that \"halt\" is a bit of misnomer."}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:09", "is_worked_example": false, "text": "What really happens is that the system arranges to re-execute the Halt() SVC each time the process is scheduled, which then causes the OS to schedule the next process for execution."}, {"start": "00:05:09", "is_lecture": true, "end": "00:05:16", "is_worked_example": false, "text": "The process appears to halt since the instruction following the Halt() SVC is never executed."}, {"start": "00:05:16", "is_lecture": true, "end": "00:05:19", "is_worked_example": false, "text": "Adding new SVC handlers is straightforward."}, {"start": "00:05:19", "is_lecture": true, "end": "00:05:25", "is_worked_example": false, "text": "First we need to define new SVC macros for use in user-mode programs."}, {"start": "00:05:25", "is_lecture": true, "end": "00:05:30", "is_worked_example": false, "text": "In this example, we're defining SVCs for getting and setting the time of day."}, {"start": "00:05:30", "is_lecture": true, "end": "00:05:42", "is_worked_example": false, "text": "Since these are the eighth and ninth SVCs, we need to make a small adjustment to the SVC dispatch code and then add the appropriate entries to the end of the dispatch table."}, {"start": "00:05:42", "is_lecture": true, "end": "00:05:46", "is_worked_example": false, "text": "The code for the new handlers is equally straightforward."}, {"start": "00:05:46", "is_lecture": true, "end": "00:05:54", "is_worked_example": false, "text": "The handler can access the value of the program's R0 by looking at the correct entry in the UserMState temporary holding area."}, {"start": "00:05:54", "is_lecture": true, "end": "00:05:59", "is_worked_example": false, "text": "It just takes a few instructions to implement the desired operations."}, {"start": "00:05:59", "is_lecture": true, "end": "00:06:06", "is_worked_example": false, "text": "The SVC mechanism provides controlled access to OS services and data."}, {"start": "00:06:06", "is_lecture": true, "end": "00:06:16", "is_worked_example": false, "text": "As we'll see in a few lectures, it'll be useful that SVC handlers can't be interrupted since they are running in supervisor mode where interrupts are disabled."}, {"start": "00:06:16", "is_lecture": true, "end": "00:06:29", "is_worked_example": false, "text": "So, for example, if we need to increment a value in main memory, using a LD/ADDC/ST sequence, but we want to ensure no other process execution intervenes between the LD and the ST,"}, {"start": "00:06:29", "is_lecture": true, "end": "00:06:36", "is_worked_example": false, "text": "we can encapsulate the required functionality as an SVC, which is guaranteed to be uninterruptible."}, {"start": "00:06:36", "is_lecture": true, "end": "00:06:43", "is_worked_example": false, "text": "We've made an excellent start at exploring the implementation of a simple time-shared operating system."}, {"start": "00:06:43", "is_lecture": true, "end": "00:06:51", "is_worked_example": false, "text": "We'll continue the exploration in the next lecture when we see how the OS deals with external input/output devices."}]}, "C12S01B04-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c12/c12s1/4?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c12s1v4", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:04", "is_worked_example": false, "text": "A conceptual schematic for a multicore processor is shown below."}, {"start": "00:00:04", "is_lecture": true, "end": "00:00:12", "is_worked_example": false, "text": "To reduce the average memory access time, each of the four cores has its own cache, which will satisfy most memory requests."}, {"start": "00:00:12", "is_lecture": true, "end": "00:00:17", "is_worked_example": false, "text": "If there's a cache miss, a request is sent to the shared main memory."}, {"start": "00:00:17", "is_lecture": true, "end": "00:00:27", "is_worked_example": false, "text": "With a modest number of cores and a good cache hit ratio, the number of memory requests that must access main memory during normal operation should be pretty small."}, {"start": "00:00:27", "is_lecture": true, "end": "00:00:33", "is_worked_example": false, "text": "To keep the number of memory accesses to a minimum, the caches implement a write-back strategy,"}, {"start": "00:00:33", "is_lecture": true, "end": "00:00:40", "is_worked_example": false, "text": "where ST instructions update the cache, but main memory is only updated when a dirty cache line is replaced."}, {"start": "00:00:40", "is_lecture": true, "end": "00:00:50", "is_worked_example": false, "text": "Our goal is that each core should share the contents of main memory, i.e., changes made by one core should visible to all the other cores."}, {"start": "00:00:50", "is_lecture": true, "end": "00:00:57", "is_worked_example": false, "text": "In the example shown here, core 0 is running Thread A and core 1 is running Thread B."}, {"start": "00:00:57", "is_lecture": true, "end": "00:01:04", "is_worked_example": false, "text": "Both threads reference two shared memory locations holding the values for the variables X and Y."}, {"start": "00:01:04", "is_lecture": true, "end": "00:01:09", "is_worked_example": false, "text": "The current values of X and Y are 1 and 2, respectively."}, {"start": "00:01:09", "is_lecture": true, "end": "00:01:14", "is_worked_example": false, "text": "Those values are held in main memory as well as being cached by each core."}, {"start": "00:01:14", "is_lecture": true, "end": "00:01:17", "is_worked_example": false, "text": "What happens when the threads are executed?"}, {"start": "00:01:17", "is_lecture": true, "end": "00:01:22", "is_worked_example": false, "text": "Each thread executes independently, updating its cache during stores to X and Y."}, {"start": "00:01:22", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "For any possible execution order, either concurrent or sequential, the result is the same: Thread A prints \"2\", Thread B prints \"1\"."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:37", "is_worked_example": false, "text": "Hardware engineers would point to the consistent outcomes and declare victory!"}, {"start": "00:01:37", "is_lecture": true, "end": "00:01:42", "is_worked_example": false, "text": "But closer examination of the final system state reveals some problems."}, {"start": "00:01:42", "is_lecture": true, "end": "00:01:47", "is_worked_example": false, "text": "After execution is complete, the two cores disagree on the values of X and Y."}, {"start": "00:01:47", "is_lecture": true, "end": "00:01:52", "is_worked_example": false, "text": "Threads running on core 0 will see X=3 and Y=2."}, {"start": "00:01:52", "is_lecture": true, "end": "00:01:56", "is_worked_example": false, "text": "Threads running on core 1 will see X=1 and Y=4."}, {"start": "00:01:56", "is_lecture": true, "end": "00:02:01", "is_worked_example": false, "text": "Because of the caches, the system isn't behaving as if there's a single shared memory."}, {"start": "00:02:01", "is_lecture": true, "end": "00:02:12", "is_worked_example": false, "text": "On the other hand, we can't eliminate the caches since that would cause the average memory access time to skyrocket, ruining any hoped-for performance improvement from using multiple cores."}, {"start": "00:02:12", "is_lecture": true, "end": "00:02:15", "is_worked_example": false, "text": "What outcome should we expect?"}, {"start": "00:02:15", "is_lecture": true, "end": "00:02:21", "is_worked_example": false, "text": "One plausible standard of correctness is the outcome when the threads are run a single timeshared core."}, {"start": "00:02:21", "is_lecture": true, "end": "00:02:31", "is_worked_example": false, "text": "The argument would be that a multicore implementation should produce the same outcome but more quickly, with parallel execution replacing timesharing."}, {"start": "00:02:31", "is_lecture": true, "end": "00:02:39", "is_worked_example": false, "text": "The table shows the possible results of the timesharing experiment, where the outcome depends on the order in which the statements are executed."}, {"start": "00:02:39", "is_lecture": true, "end": "00:02:52", "is_worked_example": false, "text": "Programmers will understand that there is more than one possible outcome and know that they would have to impose additional constraints on execution order, say, using semaphores, if they wanted a specific outcome."}, {"start": "00:02:52", "is_lecture": true, "end": "00:03:02", "is_worked_example": false, "text": "Notice that the multicore outcome of 2,1 doesn't appear anywhere on the list of possible outcomes from sequential timeshared execution."}, {"start": "00:03:02", "is_lecture": true, "end": "00:03:12", "is_worked_example": false, "text": "The notion that executing N threads in parallel should correspond to some interleaved execution of those threads on a single core is called \"sequential consistency\"."}, {"start": "00:03:12", "is_lecture": true, "end": "00:03:21", "is_worked_example": false, "text": "If multicore systems implement sequential consistency, then programmers can think of the systems as providing hardware-accelerated timesharing."}, {"start": "00:03:21", "is_lecture": true, "end": "00:03:24", "is_worked_example": false, "text": "So, our simple multicore system fails on two accounts."}, {"start": "00:03:24", "is_lecture": true, "end": "00:03:33", "is_worked_example": false, "text": "First, it doesn't correctly implement a shared memory since, as we've seen, it's possible for the two cores to disagree about the current value of a shared variable."}, {"start": "00:03:33", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "Second, as a consequence of the first problem, the system doesn't implement sequential consistency."}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:42", "is_worked_example": false, "text": "Clearly, we'll need to figure out a fix!"}, {"start": "00:03:42", "is_lecture": true, "end": "00:03:46", "is_worked_example": false, "text": "One possible fix is to give up on sequential consistency."}, {"start": "00:03:46", "is_lecture": true, "end": "00:03:56", "is_worked_example": false, "text": "An alternative memory semantics is \"weak consistency\", which only requires that the memory operations from each thread appear to be performed in the order issued by that thread."}, {"start": "00:03:56", "is_lecture": true, "end": "00:04:08", "is_worked_example": false, "text": "In other words, in a weakly consistent system, if a particular thread writes to X and then writes to Y, the possible outcomes from reads of X and Y by any thread would be one of"}, {"start": "00:04:08", "is_lecture": true, "end": "00:04:10", "is_worked_example": false, "text": "(unchanged X, unchanged Y),"}, {"start": "00:04:10", "is_lecture": true, "end": "00:04:13", "is_worked_example": false, "text": "or (changed X, unchanged Y),"}, {"start": "00:04:13", "is_lecture": true, "end": "00:04:16", "is_worked_example": false, "text": "or (changed X, changed Y)."}, {"start": "00:04:16", "is_lecture": true, "end": "00:04:21", "is_worked_example": false, "text": "But no thread would see changed Y but unchanged X."}, {"start": "00:04:21", "is_lecture": true, "end": "00:04:30", "is_worked_example": false, "text": "In a weakly consistent system, memory operations from other threads may overlap in arbitrary ways (not necessarily consistent with any sequential interleaving)."}, {"start": "00:04:30", "is_lecture": true, "end": "00:04:37", "is_worked_example": false, "text": "Note that our multicore cache system doesn't itself guarantee even weak consistency."}, {"start": "00:04:37", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "A thread that executes \"write X; write Y\" will update its local cache, but later cache replacements may cause the updated Y value to be written to main memory before the updated X value."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "To implement weak consistency, the thread should be modified to \"write X; communicate changes to all other processors; write Y\"."}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:05", "is_worked_example": false, "text": "In the next section, we'll discuss how to modify the caches to perform the required communication automatically."}, {"start": "00:05:05", "is_lecture": true, "end": "00:05:15", "is_worked_example": false, "text": "Out-of-order cores have an extra complication since there's no guarantee that successive ST instructions will complete in the order they appeared in the program."}, {"start": "00:05:15", "is_lecture": true, "end": "00:05:25", "is_worked_example": false, "text": "These architectures provide a BARRIER instruction that guarantees that memory operations before the BARRIER are completed before memory operation executed after the BARRIER."}, {"start": "00:05:25", "is_lecture": true, "end": "00:05:35", "is_worked_example": false, "text": "There are many types of memory consistency -- each commercially-available multicore system has its own particular guarantees about what happens when."}, {"start": "00:05:35", "is_lecture": true, "end": "00:05:44", "is_worked_example": false, "text": "So the prudent programmer needs to read the ISA manual carefully to ensure that her program will do what she wants."}, {"start": "00:05:44", "is_lecture": true, "end": "00:05:50", "is_worked_example": false, "text": "See the referenced PDF file for a very readable discussion about memory semantics in multicore systems."}]}, "C08S01B11-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c8/c8s1/11?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c8s1v11", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:03", "is_worked_example": false, "text": "Let's finish up by looking at two extended examples."}, {"start": "00:00:03", "is_lecture": true, "end": "00:00:11", "is_worked_example": false, "text": "The scenario for both examples is the control system for the International Space Station, which has to handle three recurring tasks:"}, {"start": "00:00:11", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "supply ship guidance (SSG), gyroscope control (G), and cabin pressure (CP)."}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "For each device, the table shows us the time between successive requests (the period), the service time for each request, and the service deadline for each request."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:33", "is_worked_example": false, "text": "We'll first analyze the system assuming that it's using a weak priority system."}, {"start": "00:00:33", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "First question: What is the maximum service time for the cabin pressure task that still allows all constraints to be met?"}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:54", "is_worked_example": false, "text": "Well, the SSG task has a maximum allowable latency of 20 ms, i.e., it's service routine must start execution within 20 ms if it is to meet its 25 ms deadline."}, {"start": "00:00:54", "is_lecture": true, "end": "00:01:01", "is_worked_example": false, "text": "The G task has a maximum allowable latency of 10 ms if it's to meet its deadline."}, {"start": "00:01:01", "is_lecture": true, "end": "00:01:09", "is_worked_example": false, "text": "So no other handler can take longer than 10 ms to run or the G task will miss its deadline."}, {"start": "00:01:09", "is_lecture": true, "end": "00:01:14", "is_worked_example": false, "text": "2. Give a weak priority ordering that meets the constraints."}, {"start": "00:01:14", "is_lecture": true, "end": "00:01:26", "is_worked_example": false, "text": "Using the earliest deadline strategy discussed earlier, the priority would be G with the highest priority, SSG with the middle priority, and CP with the lowest priority."}, {"start": "00:01:26", "is_lecture": true, "end": "00:01:31", "is_worked_example": false, "text": "3. What fraction of time will the processor spend idle?"}, {"start": "00:01:31", "is_lecture": true, "end": "00:01:38", "is_worked_example": false, "text": "We need to compute the fraction of CPU cycles needed to service the recurring requests for each task."}, {"start": "00:01:38", "is_lecture": true, "end": "00:01:45", "is_worked_example": false, "text": "SSG takes 5/30 = 16.67% of the CPU cycles."}, {"start": "00:01:45", "is_lecture": true, "end": "00:01:49", "is_worked_example": false, "text": "G takes 10/40 = 25% of the CPU cycles."}, {"start": "00:01:49", "is_lecture": true, "end": "00:01:54", "is_worked_example": false, "text": "And CP takes 10/100 = 10% of the CPU cycles."}, {"start": "00:01:54", "is_lecture": true, "end": "00:48:33", "is_worked_example": false, "text": "So servicing the task requests takes 51.67% of the cycles, leaving"}, {"start": "00:02:02", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "So the astronauts will be able to play Minecraft in their spare time :)"}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:13", "is_worked_example": false, "text": "4. What is the worst-case delay for each task until completion of its service routine?"}, {"start": "00:02:13", "is_lecture": true, "end": "00:02:25", "is_worked_example": false, "text": "Each task might have to wait for the longest-running lower-priority handler to complete plus the service times of any other higher-priority tasks plus, of course, its own service time."}, {"start": "00:02:25", "is_lecture": true, "end": "00:02:37", "is_worked_example": false, "text": "SSG has the lowest priority, so it might have to wait for CP and G to complete (a total of 20 ms), then add its own service time (5 ms)."}, {"start": "00:02:37", "is_lecture": true, "end": "00:02:42", "is_worked_example": false, "text": "So it's worst-case completion time is 25 ms after the request."}, {"start": "00:02:42", "is_lecture": true, "end": "00:02:52", "is_worked_example": false, "text": "G might to wait for CP to complete (10 ms), then add its own service time (10 ms) for a worst-case completion time of 20 ms."}, {"start": "00:02:52", "is_lecture": true, "end": "00:03:06", "is_worked_example": false, "text": "CP might have to wait for SSG to finish (5 ms), then wait for G to run (10 ms), then add its own service time (10 ms) for a worst-case completion time of 25 ms."}, {"start": "00:03:06", "is_lecture": true, "end": "00:03:12", "is_worked_example": false, "text": "Let's redo the problem, this timing assuming a strong priority system where, as before,"}, {"start": "00:03:12", "is_lecture": true, "end": "00:03:18", "is_worked_example": false, "text": "G has the highest priority, SSG the middle priority, and CP the lowest priority."}, {"start": "00:03:18", "is_lecture": true, "end": "00:03:24", "is_worked_example": false, "text": "What is the maximum service time for CP that still allows all constraints to be met?"}, {"start": "00:03:24", "is_lecture": true, "end": "00:03:35", "is_worked_example": false, "text": "This calculation is different in a strong priority system, since the service time of CP is no longer constrained by the maximum allowable latency of the higher-priority tasks --"}, {"start": "00:03:35", "is_lecture": true, "end": "00:03:38", "is_worked_example": false, "text": "they'll simply preempt CP when they need to run!"}, {"start": "00:03:38", "is_lecture": true, "end": "00:03:49", "is_worked_example": false, "text": "Instead we need to think about how much CPU time will be used by the SSG and G tasks in the 100 ms interval between the CP request and its deadline."}, {"start": "00:03:49", "is_lecture": true, "end": "00:04:02", "is_worked_example": false, "text": "In a 100 ms interval, there might be four SSG requests (at times 0, 30, 60, and 90) and three G requests (at times 0, 40, and 80)."}, {"start": "00:04:02", "is_lecture": true, "end": "00:04:07", "is_worked_example": false, "text": "Together these requests require a total of 50 ms to service."}, {"start": "00:04:07", "is_lecture": true, "end": "00:04:14", "is_worked_example": false, "text": "So the service time for CP can be up 50 ms and still meet the 100 ms deadline."}, {"start": "00:04:14", "is_lecture": true, "end": "00:04:19", "is_worked_example": false, "text": "2. What fraction of the time will the processor spend idle?"}, {"start": "00:04:19", "is_lecture": true, "end": "00:04:26", "is_worked_example": false, "text": "Assuming a 50 ms service time for CP, it now consumes 50% of the CPU."}, {"start": "00:04:26", "is_lecture": true, "end": "00:04:37", "is_worked_example": false, "text": "The other request loads are as before, so 91.67% of the CPU cycles will be spent servicing requests, leaving 8.33% of idle time."}, {"start": "00:04:37", "is_lecture": true, "end": "00:04:41", "is_worked_example": false, "text": "3. What is the worst-case completion time for each task?"}, {"start": "00:04:41", "is_lecture": true, "end": "00:04:52", "is_worked_example": false, "text": "The G task has the highest priority, so its service routine runs immediately after the request is received and its worst-case completion time is exactly its service time."}, {"start": "00:04:52", "is_lecture": true, "end": "00:05:01", "is_worked_example": false, "text": "In the 25 ms interval between an SSG request and its deadline, there might be at most one G request that will preempt execution."}, {"start": "00:05:01", "is_lecture": true, "end": "00:05:09", "is_worked_example": false, "text": "So the worst-case completion time is one G service time (10 ms) plus the SSG service time (5 ms)."}, {"start": "00:05:09", "is_lecture": true, "end": "00:05:20", "is_worked_example": false, "text": "Finally, from the calculation for problem 1, we chose the service time for the CP task so that it will complete just at its deadline of 100 ms,"}, {"start": "00:05:20", "is_lecture": true, "end": "00:05:24", "is_worked_example": false, "text": "taking into account the service time for multiple higher-priority requests."}, {"start": "00:05:24", "is_lecture": true, "end": "00:05:27", "is_worked_example": false, "text": "We covered a lot of ground in this lecture!"}, {"start": "00:05:27", "is_lecture": true, "end": "00:05:34", "is_worked_example": false, "text": "We saw that the computation needed for user-mode programs to interact with external devices was split into two parts."}, {"start": "00:05:34", "is_lecture": true, "end": "00:05:42", "is_worked_example": false, "text": "On the device-side, the OS handles device interrupts and performs the task of moving data between kernel buffers and the device."}, {"start": "00:05:42", "is_lecture": true, "end": "00:05:49", "is_worked_example": false, "text": "On the application side, user-mode programs access the information via SVC calls to the OS."}, {"start": "00:05:49", "is_lecture": true, "end": "00:05:58", "is_worked_example": false, "text": "We worried about how to handle SVC requests that needed to wait for an I/O event before the request could be satisfied."}, {"start": "00:05:58", "is_lecture": true, "end": "00:06:08", "is_worked_example": false, "text": "Ultimately we came up with a sleep/wakeup mechanism that suspends execution of the process until the some interrupt routine signals that the needed information has arrived,"}, {"start": "00:06:08", "is_lecture": true, "end": "00:06:11", "is_worked_example": false, "text": "causing the sleeping process to marked as active."}, {"start": "00:06:11", "is_lecture": true, "end": "00:06:18", "is_worked_example": false, "text": "Then the SVC is retried the next time the now active process is scheduled for execution."}, {"start": "00:06:18", "is_lecture": true, "end": "00:06:24", "is_worked_example": false, "text": "We discussed hard real-time constraints with their latencies, service times and deadlines."}, {"start": "00:06:24", "is_lecture": true, "end": "00:06:30", "is_worked_example": false, "text": "Then we explored the implementation of interrupt systems using both weak and strong priorities."}, {"start": "00:06:30", "is_lecture": true, "end": "00:06:37", "is_worked_example": false, "text": "Real-life computer systems usually implement strong priorities and support a modest number of priority levels,"}, {"start": "00:06:37", "is_lecture": true, "end": "00:06:43", "is_worked_example": false, "text": "using a weak priority system to deal with multiple devices assigned to the same strong priority level."}, {"start": "00:06:43", "is_lecture": true, "end": "00:06:51", "is_worked_example": false, "text": "This seems to work quite well in practice, allowing the systems to meet the variety of real-time constraints imposed by their I/O devices."}]}, "C03S02B01-WE.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c3/c3s2/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c3s2v1", "items": [{"start": "00:00:00", "is_lecture": false, "end": "00:00:13", "is_worked_example": true, "text": "For this problem, assume that you have a fully functioning 5-stage pipelined beta with full bypassing and annulment of branch delay slots as presented in lecture."}, {"start": "00:00:13", "is_lecture": false, "end": "00:00:17", "is_worked_example": true, "text": "This beta has been running the program shown here for a while."}, {"start": "00:00:17", "is_lecture": false, "end": "00:00:24", "is_worked_example": true, "text": "The actual functionality of this program is not so important for this problem, but lets just review it quickly."}, {"start": "00:00:24", "is_lecture": false, "end": "00:00:30", "is_worked_example": true, "text": "This program begins by initializing R1 to 0 before entering the loop."}, {"start": "00:00:30", "is_lecture": false, "end": "00:00:35", "is_worked_example": true, "text": "R1 represents the index of the array element currently being accessed."}, {"start": "00:00:35", "is_lecture": false, "end": "00:00:40", "is_worked_example": true, "text": "Within the loop, the value of that array element is loaded into R0."}, {"start": "00:00:40", "is_lecture": false, "end": "00:00:47", "is_worked_example": true, "text": "R1 is then incremented by 4 in order to point to the next element in the array."}, {"start": "00:00:47", "is_lecture": false, "end": "00:00:57", "is_worked_example": true, "text": "We then compare the array element that was just loaded into R0 with the updated index in R1 and if they are not equal, then we repeat the loop."}, {"start": "00:00:57", "is_lecture": false, "end": "00:01:08", "is_worked_example": true, "text": "If they are equal, then we store the current value of R1 into a memory location called index to remember which index value satisfied the compare instruction."}, {"start": "00:01:08", "is_lecture": false, "end": "00:01:13", "is_worked_example": true, "text": "We want to understand how this program would run on our beta."}, {"start": "00:01:13", "is_lecture": false, "end": "00:01:19", "is_worked_example": true, "text": "In order to do this, we will create a pipeline diagram showing the execution of this program."}, {"start": "00:01:19", "is_lecture": false, "end": "00:01:28", "is_worked_example": true, "text": "A pipeline diagram demonstrates which instruction is currently being executed in each of the 5 pipeline stages."}, {"start": "00:01:28", "is_lecture": false, "end": "00:01:33", "is_worked_example": true, "text": "Our rows indicate the pipeline stage that the instruction is in."}, {"start": "00:01:33", "is_lecture": false, "end": "00:01:36", "is_worked_example": true, "text": "There are five pipeline stages."}, {"start": "00:01:36", "is_lecture": false, "end": "00:01:42", "is_worked_example": true, "text": "The first is IF, or instruction fetch, which fetches the next instruction from memory."}, {"start": "00:01:42", "is_lecture": false, "end": "00:01:50", "is_worked_example": true, "text": "The second is RF, or register file stage which reads the source operands of the instruction."}, {"start": "00:01:50", "is_lecture": false, "end": "00:01:58", "is_worked_example": true, "text": "Next comes the ALU stage where all required arithmetic and logic unit operations are executed."}, {"start": "00:01:58", "is_lecture": false, "end": "00:02:10", "is_worked_example": true, "text": "The fourth stage is the MEM stage where we can begin accessing memory for a load or store operation because the address of the memory location was computed in the ALU stage."}, {"start": "00:02:10", "is_lecture": false, "end": "00:02:19", "is_worked_example": true, "text": "Finally, the last stage is WB, or the write back stage where the results are written back into the register file."}, {"start": "00:02:19", "is_lecture": false, "end": "00:02:25", "is_worked_example": true, "text": "The columns in a pipeline diagram represent the execution cycles."}, {"start": "00:02:25", "is_lecture": false, "end": "00:02:35", "is_worked_example": true, "text": "Our loop begins with a LD operation, so we see our LD instruction in the IF stage in cycle 1001."}, {"start": "00:02:35", "is_lecture": false, "end": "00:02:40", "is_worked_example": true, "text": "The LD operation then proceeds down the 5 stages of the pipelined beta."}, {"start": "00:02:40", "is_lecture": false, "end": "00:02:45", "is_worked_example": true, "text": "Next comes the ADDC instruction."}, {"start": "00:02:45", "is_lecture": false, "end": "00:02:58", "is_worked_example": true, "text": "Since there is no dependency between the LD and the ADDC instruction, the ADDC instruction begins in cycle 1002 and proceeds through all the 5 stages of the beta pipeline as well."}, {"start": "00:02:58", "is_lecture": false, "end": "00:03:03", "is_worked_example": true, "text": "Next comes the CMPEQ instruction."}, {"start": "00:03:03", "is_lecture": false, "end": "00:03:15", "is_worked_example": true, "text": "When we reach the CMPEQ instruction, we are met with our first data hazard caused by the fact that the LD is updating R0, and the CMPEQ wants to read this new value of R0."}, {"start": "00:03:15", "is_lecture": false, "end": "00:03:22", "is_worked_example": true, "text": "Recall, that a LD does not produce its value until the WB stage of the pipeline."}, {"start": "00:03:22", "is_lecture": false, "end": "00:03:32", "is_worked_example": true, "text": "This means that even with full bypassing logic, the CMPEQ instruction cannot read register R0 until the LD is in the WB stage."}, {"start": "00:03:32", "is_lecture": false, "end": "00:03:38", "is_worked_example": true, "text": "So we must initiate a stall of the pipeline in cycle 1004."}, {"start": "00:03:38", "is_lecture": false, "end": "00:03:55", "is_worked_example": true, "text": "The stall can be seen in our pipeline diagram in cycle 1005 where the CMPEQ has remained in the RF stage and we have inserted a NOP in place of the CMPEQ that was coming down the pipe one cycle earlier."}, {"start": "00:03:55", "is_lecture": false, "end": "00:03:59", "is_worked_example": true, "text": "The instruction that follows the CMPEQ is the BNE."}, {"start": "00:03:59", "is_lecture": false, "end": "00:04:13", "is_worked_example": true, "text": "Notice that it entered the IF stage in cycle 1004, but it too was stalled by the CMPEQ, so the BNE remains in the IF stage while the CMPEQ is stuck in the RF stage."}, {"start": "00:04:13", "is_lecture": false, "end": "00:04:35", "is_worked_example": true, "text": "In cycle 1005, the CMPEQ is able to complete the read of its operands by using the bypass path from the WB stage to read the updated value of R0, and by using the bypass path from the MEM stage to read the updated value of R1 produced by the ADDC instruction."}, {"start": "00:04:35", "is_lecture": false, "end": "00:04:47", "is_worked_example": true, "text": "In cycle 1006, the CMPEQ instruction moves on to the ALU stage and the BNE can move on to the RF stage."}, {"start": "00:04:47", "is_lecture": false, "end": "00:05:08", "is_worked_example": true, "text": "Since the CMPEQ is going to update the value of R2 which is the register that the BNE is trying to read, we need to make use of the bypass path from the ALU stage to the RF stage in order to provide the BNE with the result of the CMPEQ instruction in cycle 1006."}, {"start": "00:05:08", "is_lecture": false, "end": "00:05:12", "is_worked_example": true, "text": "The RF stage is also the stage when Z is generated."}, {"start": "00:05:12", "is_lecture": false, "end": "00:05:18", "is_worked_example": true, "text": "The Z signal tells the beta whether or not a register is equal to zero."}, {"start": "00:05:18", "is_lecture": false, "end": "00:05:27", "is_worked_example": true, "text": "This means that by the end of the RF stage in cycle 1006, the BNE will know whether it is repeating the loop or not."}, {"start": "00:05:27", "is_lecture": false, "end": "00:05:32", "is_worked_example": true, "text": "We now illustrate what happens to the pipeline diagram if the loop is repeated."}, {"start": "00:05:32", "is_lecture": false, "end": "00:05:46", "is_worked_example": true, "text": "In cycle 1006, the ST instruction enters the IF stage of the pipeline because until we resolve whether a branch is taken or not, we assume that we should continue fetching the next instruction."}, {"start": "00:05:46", "is_lecture": false, "end": "00:05:56", "is_worked_example": true, "text": "If the BNE determines that it should branch back to LOOP, then this ST instruction which was just fetched must be annulled by inserting a NOP in its place."}, {"start": "00:05:56", "is_lecture": false, "end": "00:06:07", "is_worked_example": true, "text": "The annulment is initiated in cycle 1006 and shows up as a NOP in the RF stage in cycle 1007."}, {"start": "00:06:07", "is_lecture": false, "end": "00:06:18", "is_worked_example": true, "text": "In cycle 1007, we also see that we now fetch the first instruction of the loop which is the LD instruction so that we can repeat the loop."}, {"start": "00:06:18", "is_lecture": false, "end": "00:06:34", "is_worked_example": true, "text": "Here is a complete pipeline diagram showing repeated execution of the loop in our sample code together with the bypass paths being used as well as the initiation of stalls and annulment of branch delay slots."}, {"start": "00:06:34", "is_lecture": false, "end": "00:06:40", "is_worked_example": true, "text": "We are now ready to answer a few questions about the execution of this loop on our beta."}, {"start": "00:06:40", "is_lecture": false, "end": "00:06:55", "is_worked_example": true, "text": "The first question we want to consider is which of the registers R0, R1, and/or R2 were read at least once directly from the register file rather than through a bypass path?"}, {"start": "00:06:55", "is_lecture": false, "end": "00:07:04", "is_worked_example": true, "text": "Looking back at our completed pipeline diagram, we see that the LD and ADDC instructions did not get their operands through bypass paths."}, {"start": "00:07:04", "is_lecture": false, "end": "00:07:13", "is_worked_example": true, "text": "Since both of those instructions read R1, that means that register R1 was read at least once directly from the register file."}, {"start": "00:07:13", "is_lecture": false, "end": "00:07:19", "is_worked_example": true, "text": "R0 which is only read by the CMPEQ always comes from a bypass path."}, {"start": "00:07:19", "is_lecture": false, "end": "00:07:26", "is_worked_example": true, "text": "Similarly, R2, which is only read by the BNE, always comes from a bypass path as well."}, {"start": "00:07:26", "is_lecture": false, "end": "00:07:33", "is_worked_example": true, "text": "Next, we want to identify the cycle in which stall was set to 1 in the pipelined beta hardware."}, {"start": "00:07:33", "is_lecture": false, "end": "00:07:38", "is_worked_example": true, "text": "This occurs in the cycle where the stall is initiated which was in cycle 1004."}, {"start": "00:07:38", "is_lecture": false, "end": "00:07:51", "is_worked_example": true, "text": "At the end of that cycle the instructions that are currently in the IF and RF stage are stalled by not allowing a load of a new value into the instruction registers of that pipeline stage."}, {"start": "00:07:51", "is_lecture": false, "end": "00:07:58", "is_worked_example": true, "text": "Next, we want to determine in which cycle was ANNUL_IF != 0?"}, {"start": "00:07:58", "is_lecture": false, "end": "00:08:07", "is_worked_example": true, "text": "Recall that the ANNUL_STAGE control signals specify when an annulment is initiated in that particular stage."}, {"start": "00:08:07", "is_lecture": false, "end": "00:08:15", "is_worked_example": true, "text": "In order to initiate an annulment, then the instruction that is currently in the IF stage is replaced with a NOP."}, {"start": "00:08:15", "is_lecture": false, "end": "00:08:21", "is_worked_example": true, "text": "This occurs in the IF stage when we need to annul a branch delay slot."}, {"start": "00:08:21", "is_lecture": false, "end": "00:08:25", "is_worked_example": true, "text": "In our example this occurs in cycle 1006."}, {"start": "00:08:25", "is_lecture": false, "end": "00:08:30", "is_worked_example": true, "text": "In which cycle was ANNUL_RF != 0?"}, {"start": "00:08:30", "is_lecture": false, "end": "00:08:36", "is_worked_example": true, "text": "This question is asking when an annulment was initiated in the RF stage."}, {"start": "00:08:36", "is_lecture": false, "end": "00:08:42", "is_worked_example": true, "text": "This occurred when the CMPEQ instruction needed to be stalled in the RF stage."}, {"start": "00:08:42", "is_lecture": false, "end": "00:08:54", "is_worked_example": true, "text": "In order to fill the pipeline bubbles, a NOP is inserted into the pipeline in place of the CMPEQ instruction that was in the RF stage in cycle 1004."}, {"start": "00:08:54", "is_lecture": false, "end": "00:09:01", "is_worked_example": true, "text": "The stall and thus the setting of ANNUL_RF != 0 occurs in cycle 1004."}, {"start": "00:09:01", "is_lecture": false, "end": "00:09:07", "is_worked_example": true, "text": "In which cycle was ANNUL_ALU != 0?"}, {"start": "00:09:07", "is_lecture": false, "end": "00:09:14", "is_worked_example": true, "text": "In other words, in which cycle did we initiate the replacement of an instruction in the ALU stage with a NOP?"}, {"start": "00:09:14", "is_lecture": false, "end": "00:09:18", "is_worked_example": true, "text": "This does not occur in our example."}, {"start": "00:09:18", "is_lecture": false, "end": "00:09:23", "is_worked_example": true, "text": "Next, we want to consider our bypass paths."}, {"start": "00:09:23", "is_lecture": false, "end": "00:09:29", "is_worked_example": true, "text": "In which cycle was either bypass coming from the ALU stage?"}, {"start": "00:09:29", "is_lecture": false, "end": "00:09:39", "is_worked_example": true, "text": "In cycle 1006, the BNE reads the result of the CMPEQ instruction from the ALU stage."}, {"start": "00:09:39", "is_lecture": false, "end": "00:09:44", "is_worked_example": true, "text": "In which cycle was either bypass coming from the MEM stage?"}, {"start": "00:09:44", "is_lecture": false, "end": "00:09:53", "is_worked_example": true, "text": "In cycle 1005, the CMPEQ reads the result of the ADDC instruction from the MEM stage."}, {"start": "00:09:53", "is_lecture": false, "end": "00:10:00", "is_worked_example": true, "text": "Finally, in which cycle was either bypass coming from the WB stage?"}, {"start": "00:10:00", "is_lecture": false, "end": "00:10:09", "is_worked_example": true, "text": "In cycle 1005, the CMPEQ reads the result of the LD instruction from the WB stage."}]}, "C03S02B02-WE.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c3/c3s2/2?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c3s2v2", "items": [{"start": "00:00:00", "is_lecture": false, "end": "00:00:07", "is_worked_example": true, "text": "For this problem, assume that you have discovered a room full of discarded 5-stage pipelined betas."}, {"start": "00:00:07", "is_lecture": false, "end": "00:00:10", "is_worked_example": true, "text": "These betas fall into four categories."}, {"start": "00:00:10", "is_lecture": false, "end": "00:00:16", "is_worked_example": true, "text": "The first is completely functional 5-stage Betas with full bypass and annulment logic."}, {"start": "00:00:16", "is_lecture": false, "end": "00:00:20", "is_worked_example": true, "text": "The second are betas with a bad register file."}, {"start": "00:00:20", "is_lecture": false, "end": "00:00:24", "is_worked_example": true, "text": "In these betas, all data read directly from the register file is zero."}, {"start": "00:00:24", "is_lecture": false, "end": "00:00:30", "is_worked_example": true, "text": "Note that if the data is read from a bypass path, then the correct value will be read."}, {"start": "00:00:30", "is_lecture": false, "end": "00:00:34", "is_worked_example": true, "text": "The third set are betas without bypass paths."}, {"start": "00:00:34", "is_lecture": false, "end": "00:00:39", "is_worked_example": true, "text": "And finally, the fourth are betas without annulment of branch delay slots."}, {"start": "00:00:39", "is_lecture": false, "end": "00:00:44", "is_worked_example": true, "text": "The problem is that the betas are not labeled, so we do not know which falls into which category."}, {"start": "00:00:44", "is_lecture": false, "end": "00:00:49", "is_worked_example": true, "text": "You come up with the test program shown here."}, {"start": "00:00:49", "is_lecture": false, "end": "00:00:58", "is_worked_example": true, "text": "Your plan is to single step through the program using each Beta chip, carefully noting the address that the final JMP loads into the PC."}, {"start": "00:00:58", "is_lecture": false, "end": "00:01:04", "is_worked_example": true, "text": "Your goal is to determine which of the four classes each chip falls into via this JMP address."}, {"start": "00:01:04", "is_lecture": false, "end": "00:01:13", "is_worked_example": true, "text": "Notice that on a fully functional beta, this code would execute the instructions sequentially skipping the MULC instruction."}, {"start": "00:01:13", "is_lecture": false, "end": "00:01:23", "is_worked_example": true, "text": "Here we see a pipeline diagram showing execution of this program on a fully functional beta from category C1."}, {"start": "00:01:23", "is_lecture": false, "end": "00:01:37", "is_worked_example": true, "text": "It shows that the although the MULC instruction is fetched in cycle 2, it gets annulled when the BEQ is in the RF stage and it determines that the branch to label X, or the SUBC instruction, will be taken."}, {"start": "00:01:37", "is_lecture": false, "end": "00:01:42", "is_worked_example": true, "text": "The MULC is annulled by inserting a NOP in its place."}, {"start": "00:01:42", "is_lecture": false, "end": "00:01:48", "is_worked_example": true, "text": "The ADDC and BEQ instructions read R31 from the register file."}, {"start": "00:01:48", "is_lecture": false, "end": "00:01:56", "is_worked_example": true, "text": "The SUBC, however, gets the value of R2 via the bypass path from the  BEQ instruction which is in the MEM stage."}, {"start": "00:01:56", "is_lecture": false, "end": "00:02:00", "is_worked_example": true, "text": "The ADD then reads R0 and R2."}, {"start": "00:02:00", "is_lecture": false, "end": "00:02:07", "is_worked_example": true, "text": "R0 has already made it back to the register file because the ADDC instruction completed by the end of cycle 4."}, {"start": "00:02:07", "is_lecture": false, "end": "00:02:15", "is_worked_example": true, "text": "R2, however, is read via the bypass path from the SUBC instruction which is in the ALU stage."}, {"start": "00:02:15", "is_lecture": false, "end": "00:02:25", "is_worked_example": true, "text": "Finally, the JMP, reads the value of R3 via the bypass path from the ADD instruction which is in the ALU stage in cycle 6."}, {"start": "00:02:25", "is_lecture": false, "end": "00:02:33", "is_worked_example": true, "text": "When run on a fully functional beta with bypass paths and annulment of branch delay slots, the code behaves as follows:"}, {"start": "00:02:33", "is_lecture": false, "end": "00:02:37", "is_worked_example": true, "text": "The ADDC sets R0 = 4."}, {"start": "00:02:37", "is_lecture": false, "end": "00:02:41", "is_worked_example": true, "text": "The BEQ stores PC + 4 into R2."}, {"start": "00:02:41", "is_lecture": false, "end": "00:02:53", "is_worked_example": true, "text": "Since the ADDC is at address 0, the BEQ is at address 4, so PC + 4 = 8 is stored into R2, and the program branches to label X."}, {"start": "00:02:53", "is_lecture": false, "end": "00:03:02", "is_worked_example": true, "text": "Next, the SUBC subtracts 4 from the latest value of R2 and stores the result which is 4 back into R2."}, {"start": "00:03:02", "is_lecture": false, "end": "00:03:10", "is_worked_example": true, "text": "The ADD adds R0 and R2, or 4 and 4, and stores the result which is 8 into R3."}, {"start": "00:03:10", "is_lecture": false, "end": "00:03:15", "is_worked_example": true, "text": "The JMP jumps to the address in R3 which is 8."}, {"start": "00:03:15", "is_lecture": false, "end": "00:03:25", "is_worked_example": true, "text": "When run on C2 which has a bad register file that always outputs a zero, the behavior of the program changes a bit."}, {"start": "00:03:25", "is_lecture": false, "end": "00:03:33", "is_worked_example": true, "text": "The ADDC and BEQ instructions which use R31 which is 0 anyways behave in the same way as before."}, {"start": "00:03:33", "is_lecture": false, "end": "00:03:41", "is_worked_example": true, "text": "The SUBC, which gets the value of R2 from the bypass path, reads the correct value for R2 which is 8."}, {"start": "00:03:41", "is_lecture": false, "end": "00:03:51", "is_worked_example": true, "text": "Recall that only reads directly from the register file return a zero, whereas if the data is coming from a bypass path, then you get the correct value."}, {"start": "00:03:51", "is_lecture": false, "end": "00:03:53", "is_worked_example": true, "text": "So the SUBC produces a 4."}, {"start": "00:03:53", "is_lecture": false, "end": "00:03:59", "is_worked_example": true, "text": "The ADD reads R0 from the register file and R2 from the bypass path."}, {"start": "00:03:59", "is_lecture": false, "end": "00:04:07", "is_worked_example": true, "text": "The result is that the value of R0 is read as if it was 0 while that of R2 is correct and is 4."}, {"start": "00:04:07", "is_lecture": false, "end": "00:04:18", "is_worked_example": true, "text": "So R3 gets the value 4 assigned to it, and that is the address that the JMP instruction jumps to because it too reads its register from a bypass path."}, {"start": "00:04:18", "is_lecture": false, "end": "00:04:28", "is_worked_example": true, "text": "When run on C3 which does not have any bypass paths, some of the instructions will read stale values of their source operands."}, {"start": "00:04:28", "is_lecture": false, "end": "00:04:31", "is_worked_example": true, "text": "Let's go through the example in detail."}, {"start": "00:04:31", "is_lecture": false, "end": "00:04:41", "is_worked_example": true, "text": "The ADDC and BEQ instructions read R31 which is 0 from the register file so what they ultimately produce does not change."}, {"start": "00:04:41", "is_lecture": false, "end": "00:04:53", "is_worked_example": true, "text": "However, you must keep in mind that the updated value of the destination register will not get updated until after that instruction completes the WB stage of the pipeline."}, {"start": "00:04:53", "is_lecture": false, "end": "00:05:05", "is_worked_example": true, "text": "When the SUBC reads R2, it gets a stale value of R2 because the BEQ instruction has not yet completed, so it assumes that R2 = 0."}, {"start": "00:05:05", "is_lecture": false, "end": "00:05:09", "is_worked_example": true, "text": "It then subtracts 4 from that and tries to write -4 into R2."}, {"start": "00:05:09", "is_lecture": false, "end": "00:05:12", "is_worked_example": true, "text": "Next the ADD runs."}, {"start": "00:05:12", "is_lecture": false, "end": "00:05:25", "is_worked_example": true, "text": "Recall that the ADD would normally read R0 from the register file because by the time the ADD is in the RF stage, the ADDC which writes to R0 has completed all the pipeline stages."}, {"start": "00:05:25", "is_lecture": false, "end": "00:05:29", "is_worked_example": true, "text": "The ADD also normally read R2 from the bypass path."}, {"start": "00:05:29", "is_lecture": false, "end": "00:05:36", "is_worked_example": true, "text": "However, since C3 does not have bypass paths, it reads a stale value of R2 from the register file."}, {"start": "00:05:36", "is_lecture": false, "end": "00:05:53", "is_worked_example": true, "text": "To determine which stale value it reads, we need to examine the pipeline diagram to see if either the BEQ or the SUBC operations, both of which eventually update R2, have completed by the time the ADD reads its source operands."}, {"start": "00:05:53", "is_lecture": false, "end": "00:06:09", "is_worked_example": true, "text": "Looking at our pipeline diagram, we see that when the ADD is in the RF stage, neither the BEQ nor the SUBC have completed, thus the value read for R2 is the initial value of R2 which is 0."}, {"start": "00:06:09", "is_lecture": false, "end": "00:06:21", "is_worked_example": true, "text": "We can now determine the behavior of the ADD instruction which is that it assumes R0 = 4 and R2 = 0 and will eventually write a 4 into R3."}, {"start": "00:06:21", "is_lecture": false, "end": "00:06:34", "is_worked_example": true, "text": "Finally, the JMP instruction wants to read the result of the ADD, however, since there are no bypass paths, it reads the original value of R3 which was 0 and jumps to address 0."}, {"start": "00:06:34", "is_lecture": false, "end": "00:06:55", "is_worked_example": true, "text": "Of course, had we looked at our code a little more closely, we could have determined this without all the intermediate steps because the ADD is the only instruction that tries to update R3, and you know that the JMP would normally get R3 from the bypass path which is not available, therefore it must read the original value of R3 which is 0."}, {"start": "00:06:55", "is_lecture": false, "end": "00:07:06", "is_worked_example": true, "text": "In category C4, the betas do not annul instructions that were fetched after a branch instruction but aren't supposed to get executed."}, {"start": "00:07:06", "is_lecture": false, "end": "00:07:14", "is_worked_example": true, "text": "This means that the MULC which is fetched after the BEQ is actually executed in the pipeline and affects the value of R2."}, {"start": "00:07:14", "is_lecture": false, "end": "00:07:18", "is_worked_example": true, "text": "Let's take a close look at what happens in each instruction."}, {"start": "00:07:18", "is_lecture": false, "end": "00:07:25", "is_worked_example": true, "text": "Once again we begin with the ADDC setting R0 to 4 and the BEQ setting R2 to 8."}, {"start": "00:07:25", "is_lecture": false, "end": "00:07:32", "is_worked_example": true, "text": "Since our bypass paths are now working, we can assume that we can immediately get the updated value."}, {"start": "00:07:32", "is_lecture": false, "end": "00:07:40", "is_worked_example": true, "text": "Next, we execute the MULC which takes the latest value of R2 from the bypass path and multiplies it by 2."}, {"start": "00:07:40", "is_lecture": false, "end": "00:07:43", "is_worked_example": true, "text": "So it sets R2 = 16."}, {"start": "00:07:43", "is_lecture": false, "end": "00:07:51", "is_worked_example": true, "text": "The SUBC now uses this value for R2 from which it subtracts 4 to produce R2 = 12."}, {"start": "00:07:51", "is_lecture": false, "end": "00:08:01", "is_worked_example": true, "text": "The ADD then reads R0 = 4 and adds to it R2 = 12 to produce R3 = 16."}, {"start": "00:08:01", "is_lecture": false, "end": "00:08:05", "is_worked_example": true, "text": "Finally, the JMP jumps to address 16."}, {"start": "00:08:05", "is_lecture": false, "end": "00:08:16", "is_worked_example": true, "text": "Since each of the four categories produces a unique jump address, this program can be used to sort out all the betas into the four categories."}]}, "C03S01B02-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c3/c3s1/2?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c3s1v2", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:07", "is_worked_example": false, "text": "Let's start by redrawing and simplifying the Beta data path so that it will be easier to reason about when we add pipelining."}, {"start": "00:00:07", "is_lecture": true, "end": "00:00:15", "is_worked_example": false, "text": "The first simplification is to focus on sequential execution and so leave out the branch addressing and PC mux logic."}, {"start": "00:00:15", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "Our simplified Beta always executes the next instruction from PC+4."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:25", "is_worked_example": false, "text": "We'll add back the branch and jump logic when we discuss control hazards."}, {"start": "00:00:25", "is_lecture": true, "end": "00:00:36", "is_worked_example": false, "text": "The second simplification is to have the register file appear twice in the diagram so that we can tease apart the read and write operations that occur at different stages of instruction execution."}, {"start": "00:00:36", "is_lecture": true, "end": "00:00:44", "is_worked_example": false, "text": "The top Register File shows the combinational read ports, used to when reading the register operands in the RF stage."}, {"start": "00:00:44", "is_lecture": true, "end": "00:00:52", "is_worked_example": false, "text": "The bottom Register File shows the clocked write port, used to write the result into the destination register at the end of the WB stage."}, {"start": "00:00:52", "is_lecture": true, "end": "00:01:00", "is_worked_example": false, "text": "Physically, there's only one set of 32 registers, we've just drawn the read and write circuity as separate components in the diagram."}, {"start": "00:01:00", "is_lecture": true, "end": "00:01:09", "is_worked_example": false, "text": "If we add pipeline registers to the simplified diagram, we see that execution proceeds through the five stages from top to bottom."}, {"start": "00:01:09", "is_lecture": true, "end": "00:01:21", "is_worked_example": false, "text": "If we consider execution of instruction sequences with no data hazards, information is flowing down the pipeline and the pipeline will correctly overlap the execution of all the instructions in the pipeline."}, {"start": "00:01:21", "is_lecture": true, "end": "00:01:26", "is_worked_example": false, "text": "The diagram shows the components needed to implement each of the five stages."}, {"start": "00:01:26", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "The IF stage contains the program counter and the main memory interface for fetching instructions."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:26", "is_worked_example": false, "text": "The RF stage has the register file and operand multiplexers."}, {"start": "00:01:26", "is_lecture": true, "end": "00:01:41", "is_worked_example": false, "text": "The ALU stage uses the operands and computes the result."}, {"start": "00:01:41", "is_lecture": true, "end": "00:01:46", "is_worked_example": false, "text": "The MEM stage handles the memory access for load and store operations."}, {"start": "00:01:46", "is_lecture": true, "end": "00:01:51", "is_worked_example": false, "text": "And the WB stage writes the result into the destination register."}, {"start": "00:01:51", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "In each clock cycle, each stage does its part in the execution of a particular instruction."}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:01", "is_worked_example": false, "text": "In a given clock cycle, there are five instructions in the pipeline."}, {"start": "00:02:01", "is_lecture": true, "end": "00:02:05", "is_worked_example": false, "text": "Note that data accesses to main memory span almost two clock cycles."}, {"start": "00:02:05", "is_lecture": true, "end": "00:02:14", "is_worked_example": false, "text": "Data accesses are initiated at the beginning of the MEM stage and returning data is only needed just before the end of the WB stage."}, {"start": "00:02:14", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "The memory is itself pipelined and can simultaneously finish the access from an earlier instruction while starting an access for the next instruction."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:29", "is_worked_example": false, "text": "This simplified diagram isn't showing how the control logic is split across the pipeline stages."}, {"start": "00:02:29", "is_lecture": true, "end": "00:02:30", "is_worked_example": false, "text": "How does that work?"}, {"start": "00:02:30", "is_lecture": true, "end": "00:02:40", "is_worked_example": false, "text": "Note that we've included instruction registers as part of each pipeline stage, so that each stage can compute the control signals it needs from its instruction register."}, {"start": "00:02:40", "is_lecture": true, "end": "00:02:47", "is_worked_example": false, "text": "The encoded instruction is simply passed from one stage to the next as the instruction flows through the pipeline.."}, {"start": "00:02:47", "is_lecture": true, "end": "00:02:52", "is_worked_example": false, "text": "Each stage computes its control signals from the opcode field of its instruction register."}, {"start": "00:02:52", "is_lecture": true, "end": "00:02:57", "is_worked_example": false, "text": "The RF stage needs the RA, RB, and literal fields from its instruction register."}, {"start": "00:02:57", "is_lecture": true, "end": "00:03:02", "is_worked_example": false, "text": "And the WB stage needs the RC field from its instruction register."}, {"start": "00:03:02", "is_lecture": true, "end": "00:03:10", "is_worked_example": false, "text": "The required logic is very similar to the unpipelined implementation, it's just been split up and moved to the appropriate pipeline stage."}, {"start": "00:03:10", "is_lecture": true, "end": "00:03:16", "is_worked_example": false, "text": "We'll see that we will have to add some additional control logic to deal correctly with pipeline hazards."}, {"start": "00:03:16", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "Our simplified diagram isn't so simple anymore!"}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:27", "is_worked_example": false, "text": "To see how the pipeline works, let's follow along as it executes this sequence of six instructions."}, {"start": "00:03:27", "is_lecture": true, "end": "00:03:33", "is_worked_example": false, "text": "Note that the instructions are reading and writing from different registers, so there are no potential data hazards."}, {"start": "00:03:33", "is_lecture": true, "end": "00:03:03", "is_worked_example": false, "text": "And there are no branches and jumps, so there are no potential control hazards."}, {"start": "00:03:03", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "Since there are no potential hazards, the instruction executions can be overlapped and their overlapped execution in the pipeline will work correctly."}, {"start": "00:03:47", "is_lecture": true, "end": "00:03:49", "is_worked_example": false, "text": "Okay, here we go!"}, {"start": "00:03:49", "is_lecture": true, "end": "00:04:03", "is_worked_example": false, "text": "During cycle 1, the IF stage sends the value from the program counter to main memory to fetch the first instruction (the green LD instruction), which will be stored in the RF-stage instruction register at the end of the cycle."}, {"start": "00:04:03", "is_lecture": true, "end": "00:04:09", "is_worked_example": false, "text": "Meanwhile, it's also computing PC+4, which will be the next value of the program counter."}, {"start": "00:04:09", "is_lecture": true, "end": "00:04:16", "is_worked_example": false, "text": "We've colored the next value blue to indicate that it's the address of the blue instruction in the sequence."}, {"start": "00:04:16", "is_lecture": true, "end": "00:04:23", "is_worked_example": false, "text": "We'll add the appropriately colored label on the right of each pipeline stage to indicate which instruction the stage is processing."}, {"start": "00:04:23", "is_lecture": true, "end": "00:04:34", "is_worked_example": false, "text": "At the start of cycle 2, we see that values in the PC and instruction registers for the RF stage now correspond to the green instruction."}, {"start": "00:04:34", "is_lecture": true, "end": "00:04:42", "is_worked_example": false, "text": "During the cycle the register file will be reading the register operands, in this case R1, which is needed for the green instruction."}, {"start": "00:04:42", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "Since the green instruction is a LD, ASEL is 0 and BSEL is 1, selecting the appropriate values to be written into the A and B operand registers at the end of the cycle."}, {"start": "00:04:53", "is_lecture": true, "end": "00:05:03", "is_worked_example": false, "text": "Concurrently, the IF stage is fetching the blue instruction from main memory and computing an updated PC value for the next cycle."}, {"start": "00:05:03", "is_lecture": true, "end": "00:05:20", "is_worked_example": false, "text": "In cycle 3, the green instruction is now in the ALU stage, where the ALU is adding the values in its operand registers (in this case the value of R1 and the constant 4) and the result will be stored in Y_MEM register at the end of the cycle."}, {"start": "00:05:20", "is_lecture": true, "end": "00:05:25", "is_worked_example": false, "text": "In cycle 4, we're overlapping execution of four instructions."}, {"start": "00:05:25", "is_lecture": true, "end": "00:05:30", "is_worked_example": false, "text": "The MEM stage initiates a memory read for the green LD instruction."}, {"start": "00:05:30", "is_lecture": true, "end": "00:05:39", "is_worked_example": false, "text": "Note that the read data will first become available in the WB stage -- it's not available to CPU in the current clock cycle."}, {"start": "00:05:39", "is_lecture": true, "end": "00:05:47", "is_worked_example": false, "text": "In cycle 5, the results of the main memory read initiated in cycle 4 are available for writing to the register file in the WB stage."}, {"start": "00:05:47", "is_lecture": true, "end": "00:05:56", "is_worked_example": false, "text": "So execution of the green LD instruction will be complete when the memory data is written to R2 at the end of cycle 5."}, {"start": "00:05:56", "is_lecture": true, "end": "00:06:02", "is_worked_example": false, "text": "Meanwhile, the MEM stage is initiating a memory read for the blue LD instruction."}, {"start": "00:06:02", "is_lecture": true, "end": "00:06:07", "is_worked_example": false, "text": "The pipeline continues to complete successive instructions in successive clock cycles."}, {"start": "00:06:07", "is_lecture": true, "end": "00:06:10", "is_worked_example": false, "text": "The latency for a particular instruction is 5 clock cycles."}, {"start": "00:06:10", "is_lecture": true, "end": "00:06:15", "is_worked_example": false, "text": "The throughput of the pipelined CPU is 1 instruction/cycle."}, {"start": "00:06:15", "is_lecture": true, "end": "00:06:24", "is_worked_example": false, "text": "This is the same as the unpipelined implementation, except that the clock period is shorter because each pipeline stage has fewer components."}, {"start": "00:06:24", "is_lecture": true, "end": "00:06:34", "is_worked_example": false, "text": "Note that the effects of the green LD, i.e., filling R2 with a new value, don't happen until the rising edge of the clock at the end of cycle 5."}, {"start": "00:06:34", "is_lecture": true, "end": "00:06:41", "is_worked_example": false, "text": "In other words, the results of the green LD aren't available to other instructions until cycle 6."}, {"start": "00:06:41", "is_lecture": true, "end": "00:06:49", "is_worked_example": false, "text": "If there were instructions in the pipeline that read R2 before cycle 6, they would have gotten an old value!"}, {"start": "00:06:49", "is_lecture": true, "end": "00:06:51", "is_worked_example": false, "text": "This is an example of a data hazard."}, {"start": "00:06:51", "is_lecture": true, "end": "00:06:56", "is_worked_example": false, "text": "Not a problem for us, since our instruction sequence didn't trigger this data hazard."}, {"start": "00:06:56", "is_lecture": true, "end": "00:07:01", "is_worked_example": false, "text": "Tackling data hazards is our next task."}]}, "C09S01B07-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c9/c9s1/7?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c9s1v7", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:08", "is_worked_example": false, "text": "If the necessary synchronization requires acquiring more than one lock, there are some special considerations that need to be taken into account."}, {"start": "00:00:08", "is_lecture": true, "end": "00:00:14", "is_worked_example": false, "text": "For example, the code below implements the transfer of funds from one bank account to another."}, {"start": "00:00:14", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "The code assumes there is a separate semaphore lock for each account and since it needs to adjust the balance of two accounts, it acquires the lock for each account."}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:30", "is_worked_example": false, "text": "Consider what happens if two customers try simultaneous transfers between their two accounts."}, {"start": "00:00:30", "is_lecture": true, "end": "00:00:36", "is_worked_example": false, "text": "The top customer will try to acquire the locks for accounts 6005 and 6004."}, {"start": "00:00:36", "is_lecture": true, "end": "00:00:42", "is_worked_example": false, "text": "The bottom customer tries to acquire the same locks, but in the opposite order."}, {"start": "00:00:42", "is_lecture": true, "end": "00:00:48", "is_worked_example": false, "text": "Once a customer has acquired both locks, the transfer code will complete, releasing the locks."}, {"start": "00:00:48", "is_lecture": true, "end": "00:01:00", "is_worked_example": false, "text": "But what happens if the top customer acquires his first lock (for account 6005) and the bottom customer simultaneously acquires his first lock (for account 6004)."}, {"start": "00:01:00", "is_lecture": true, "end": "00:01:11", "is_worked_example": false, "text": "So far, so good, but now each customer will be not be successful in acquiring their second lock, since those locks are already held by the other customer!"}, {"start": "00:01:11", "is_lecture": true, "end": "00:01:19", "is_worked_example": false, "text": "This situation is called a \"deadlock\" or \"deadly embrace\" because there is no way execution for either process will resume."}, {"start": "00:01:19", "is_lecture": true, "end": "00:01:24", "is_worked_example": false, "text": "Both will wait indefinitely to acquire a lock that will never be available."}, {"start": "00:01:24", "is_lecture": true, "end": "00:01:30", "is_worked_example": false, "text": "Obviously, synchronization involving multiple resources requires a bit more thought."}, {"start": "00:01:30", "is_lecture": true, "end": "00:01:35", "is_worked_example": false, "text": "The problem of deadlock is elegantly illustrated by the Dining Philosophers problem."}, {"start": "00:01:35", "is_lecture": true, "end": "00:01:39", "is_worked_example": false, "text": "Here there are, say, 5 philosophers waiting to eat."}, {"start": "00:01:39", "is_lecture": true, "end": "00:01:44", "is_worked_example": false, "text": "Each requires two chopsticks in order to proceed, and there are 5 chopsticks on the table."}, {"start": "00:01:44", "is_lecture": true, "end": "00:01:48", "is_worked_example": false, "text": "The philosophers follow a simple algorithm."}, {"start": "00:01:48", "is_lecture": true, "end": "00:01:53", "is_worked_example": false, "text": "First they pick up the chopstick on their left, then the chopstick on their right."}, {"start": "00:01:53", "is_lecture": true, "end": "00:02:02", "is_worked_example": false, "text": "When they have both chopsticks they eat until they're done, at which point they return both chopsticks to the table, perhaps enabling one of their neighbors to pick them up and begin eating."}, {"start": "00:02:02", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "Again, we see the basic setup of needing two (or more) resources before the task can complete."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:12", "is_worked_example": false, "text": "Hopefully you can see the problem that may arise..."}, {"start": "00:02:12", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "If all philosophers pick up the chopstick on their left, then all the chopsticks have been acquired, and none of the philosophers will be able to acquire their second chopstick and eat."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:24", "is_worked_example": false, "text": "Another deadlock!"}, {"start": "00:02:24", "is_lecture": true, "end": "00:02:28", "is_worked_example": false, "text": "Here are the conditions required for a deadlock:"}, {"start": "00:02:28", "is_lecture": true, "end": "00:02:33", "is_worked_example": false, "text": "1. Mutual exclusion, where a particular resource can only be acquired by one process at a time."}, {"start": "00:02:33", "is_lecture": true, "end": "00:02:40", "is_worked_example": false, "text": "2. Hold-and-wait, where a process holds allocated resources while waiting to acquire the next resource."}, {"start": "00:02:40", "is_lecture": true, "end": "00:02:47", "is_worked_example": false, "text": "3. No preemption, where a resource cannot be removed from the process which acquired it."}, {"start": "00:02:47", "is_lecture": true, "end": "00:02:52", "is_worked_example": false, "text": "Resources are only released after the process has completed its transaction."}, {"start": "00:02:52", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "4. Circular wait, where resources needed by one process are held by another, and vice versa."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:03", "is_worked_example": false, "text": "How can we solve the problem of deadlocks when acquiring multiple resources?"}, {"start": "00:03:03", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "Either we avoid the problem to begin with, or we detect that deadlock has occurred and implement a recovery strategy."}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:13", "is_worked_example": false, "text": "Both techniques are used in practice."}, {"start": "00:03:13", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "In the Dining Philosophers problem, deadlock can be avoided with a small modification to the algorithm."}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:26", "is_worked_example": false, "text": "We start by assigning a unique number to each chopstick to establish a global ordering of all the resources,"}, {"start": "00:03:26", "is_lecture": true, "end": "00:03:35", "is_worked_example": false, "text": "then rewrite the code to acquire resources using the global ordering to determine which resource to acquire first, which second, and so on."}, {"start": "00:03:35", "is_lecture": true, "end": "00:03:42", "is_worked_example": false, "text": "With the chopsticks numbered, the philosophers pick up the lowest-numbered chopstick from either their left or right."}, {"start": "00:03:42", "is_lecture": true, "end": "00:03:48", "is_worked_example": false, "text": "Then they pick up the other, higher-numbered chopstick, eat, and then return the chopsticks to the table."}, {"start": "00:03:48", "is_lecture": true, "end": "00:03:51", "is_worked_example": false, "text": "How does this avoid deadlock?"}, {"start": "00:03:51", "is_lecture": true, "end": "00:03:56", "is_worked_example": false, "text": "Deadlock happens when all the chopsticks have been picked up but no philosopher can eat."}, {"start": "00:03:56", "is_lecture": true, "end": "00:04:08", "is_worked_example": false, "text": "If all the chopsticks have been been picked up, that means some philosopher has picked up the highest-numbered chopstick and so must have earlier picked up the lower-numbered chopstick on his other side."}, {"start": "00:04:08", "is_lecture": true, "end": "00:04:15", "is_worked_example": false, "text": "So that philosopher can eat then return both chopsticks to the table, breaking the hold-and-wait cycle."}, {"start": "00:04:15", "is_lecture": true, "end": "00:04:28", "is_worked_example": false, "text": "So if all the processes in the system can agree upon a global ordering for the resources they require, then acquire them in order, there will be no possibility of a deadlock caused by a hold-and-wait cycle."}, {"start": "00:04:28", "is_lecture": true, "end": "00:04:33", "is_worked_example": false, "text": "A global ordering is easy to arrange in our banking code for the transfer transaction."}, {"start": "00:04:33", "is_lecture": true, "end": "00:04:41", "is_worked_example": false, "text": "We'll modify the code to first acquire the lock for the lower-numbered account, then acquire the lock for the higher-numbered account."}, {"start": "00:04:41", "is_lecture": true, "end": "00:04:47", "is_worked_example": false, "text": "Now, both customers will first try to acquire the lock for the 6004 account."}, {"start": "00:04:47", "is_lecture": true, "end": "00:04:54", "is_worked_example": false, "text": "The customer that succeeds then can acquire the lock for the 6005 account and complete the transaction."}, {"start": "00:04:54", "is_lecture": true, "end": "00:05:02", "is_worked_example": false, "text": "The key to deadlock avoidance was that customers contented for the lock for the *first* resource they both needed."}, {"start": "00:05:02", "is_lecture": true, "end": "00:05:14", "is_worked_example": false, "text": "Acquiring that lock ensured they would be able to acquire the remainder of the shared resources without fear that they would already be allocated to another process in a way that could cause a hold-and-wait cycle."}, {"start": "00:05:14", "is_lecture": true, "end": "00:05:23", "is_worked_example": false, "text": "Establishing and using a global order for shared resources is possible when we can modify all processes to cooperate."}, {"start": "00:05:23", "is_lecture": true, "end": "00:05:27", "is_worked_example": false, "text": "Avoiding deadlock without changing the processes is a harder problem."}, {"start": "00:05:27", "is_lecture": true, "end": "00:05:40", "is_worked_example": false, "text": "For example, at the operating system level, it would be possible to modify the WAIT SVC to detect circular wait and terminate one of the WAITing processes, releasing its resources and breaking the deadlock."}, {"start": "00:05:40", "is_lecture": true, "end": "00:05:45", "is_worked_example": false, "text": "The other strategy we mentioned was detection and recovery."}, {"start": "00:05:45", "is_lecture": true, "end": "00:05:56", "is_worked_example": false, "text": "Database systems detect when there's been an external access to the shared data used by a particular transaction, which causes the database to abort the transaction."}, {"start": "00:05:56", "is_lecture": true, "end": "00:06:08", "is_worked_example": false, "text": "When issuing a transaction to a database, the programmer specifies what should happen if the transaction is aborted, e.g., she can specify that the transaction be retried."}, {"start": "00:06:08", "is_lecture": true, "end": "00:06:20", "is_worked_example": false, "text": "The database remembers all the changes to shared data that happen during a transaction and only changes the master copy of the shared data when it is sure that the transaction will not be aborted,"}, {"start": "00:06:20", "is_lecture": true, "end": "00:06:23", "is_worked_example": false, "text": "at which point the changes are committed to the database."}, {"start": "00:06:23", "is_lecture": true, "end": "00:06:30", "is_worked_example": false, "text": "In summary, we saw that organizing an application as communicating processes is often a convenient way to go."}, {"start": "00:06:30", "is_lecture": true, "end": "00:06:43", "is_worked_example": false, "text": "We used semaphores to synchronize the execution of the different processes, providing guarantees that certain precedence constraints would be met, even between statements in different processes."}, {"start": "00:06:43", "is_lecture": true, "end": "00:06:54", "is_worked_example": false, "text": "We also introduced the notion of critical code sections and mutual exclusion constraints that guaranteed that a code sequence would be executed without interruption by another process."}, {"start": "00:06:54", "is_lecture": true, "end": "00:07:00", "is_worked_example": false, "text": "We saw that semaphores could also be used to implement those mutual exclusion constraints."}, {"start": "00:07:00", "is_lecture": true, "end": "00:07:08", "is_worked_example": false, "text": "Finally we discussed the problem of deadlock that can occur when multiple processes must acquire multiple shared resources,"}, {"start": "00:07:08", "is_lecture": true, "end": "00:07:16", "is_worked_example": false, "text": "and we proposed several solutions based on a global ordering of resources or the ability to restart a transaction."}, {"start": "00:07:16", "is_lecture": true, "end": "00:07:27", "is_worked_example": false, "text": "Synchronization primitives play a key role in the world of \"big data\" where there are vast amounts of shared data, or when trying to coordinate the execution of thousands of processes in the cloud."}, {"start": "00:07:27", "is_lecture": true, "end": "00:07:36", "is_worked_example": false, "text": "Understanding synchronization issues and their solutions is a key skill when writing most modern applications."}]}, "C09S01B06-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c9/c9s1/6?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c9s1v6", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:03", "is_worked_example": false, "text": "Now let's figure out how to implement semaphores."}, {"start": "00:00:03", "is_lecture": true, "end": "00:00:14", "is_worked_example": false, "text": "They are themselves shared data and implementing the WAIT and SIGNAL operations will require read/modify/write sequences that must be executed as critical sections."}, {"start": "00:00:14", "is_lecture": true, "end": "00:00:20", "is_worked_example": false, "text": "Normally we'd use a lock semaphore to implement the mutual exclusion constraint for critical sections."}, {"start": "00:00:20", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "But obviously we can't use semaphores to implement semaphores!"}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:31", "is_worked_example": false, "text": "We have what's called a bootstrapping problem: we need to implement the required functionality from scratch."}, {"start": "00:00:31", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "Happily, if we're running on a timeshared processor with an uninterruptible OS kernel, we can use the supervisor call (SVC) mechanism to implement the required functionality."}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:49", "is_worked_example": false, "text": "We can also extend the ISA to include a special test-and-set instruction that will let us implement a simple lock semaphore,"}, {"start": "00:00:49", "is_lecture": true, "end": "00:00:55", "is_worked_example": false, "text": "which can then be used to protect critical sections that implement more complex semaphore semantics."}, {"start": "00:00:55", "is_lecture": true, "end": "00:01:10", "is_worked_example": false, "text": "Single instructions are inherently atomic and, in a multi-core processor, will do what we want if the shared main memory supports reading the old value and writing a new value to a specific memory location as a single memory access."}, {"start": "00:01:10", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "There are other, more complex, software-only solutions that rely only on the atomicity of individual reads and writes to implement a simple lock."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:23", "is_worked_example": false, "text": "For example, see \"Dekker's Algorithm\" on Wikipedia."}, {"start": "00:01:23", "is_lecture": true, "end": "00:01:26", "is_worked_example": false, "text": "We'll look in more detail at the first two approaches."}, {"start": "00:01:26", "is_lecture": true, "end": "00:01:30", "is_worked_example": false, "text": "Here are the OS handlers for the WAIT and SIGNAL supervisor calls."}, {"start": "00:01:30", "is_lecture": true, "end": "00:01:40", "is_worked_example": false, "text": "Since SVCs are run kernel mode, they can't be interrupted, so the handler code is naturally executed as a critical section."}, {"start": "00:01:40", "is_lecture": true, "end": "00:01:47", "is_worked_example": false, "text": "Both handlers expect the address of the semaphore location to be passed as an argument in the user's R0."}, {"start": "00:01:47", "is_lecture": true, "end": "00:02:00", "is_worked_example": false, "text": "The WAIT handler checks the semaphore's value and if it's non-zero, the value is decremented and the handler resumes execution of the user's program at the instruction following the WAIT SVC."}, {"start": "00:02:00", "is_lecture": true, "end": "00:02:14", "is_worked_example": false, "text": "If the semaphore is 0, the code arranges to re-execute the WAIT SVC when the user program resumes execution and then calls SLEEP to mark the process as inactive until the corresponding WAKEUP call is made."}, {"start": "00:02:14", "is_lecture": true, "end": "00:02:24", "is_worked_example": false, "text": "The SIGNAL handler is simpler: it increments the semaphore value and calls WAKEUP to mark as active any processes that were WAITing for this particular semaphore."}, {"start": "00:02:24", "is_lecture": true, "end": "00:02:33", "is_worked_example": false, "text": "Eventually the round-robin scheduler will select a process that was WAITing and it will be able to decrement the semaphore and proceed."}, {"start": "00:02:33", "is_lecture": true, "end": "00:02:43", "is_worked_example": false, "text": "Note that the code makes no provision for fairness, i.e., there's no guarantee that a WAITing process will eventually succeed in finding the semaphore non-zero."}, {"start": "00:02:43", "is_lecture": true, "end": "00:02:57", "is_worked_example": false, "text": "The scheduler has a specific order in which it runs processes, so the next-in-sequence WAITing process will always get the semaphore even if there are later-in-sequence processes that have been WAITing longer."}, {"start": "00:02:57", "is_lecture": true, "end": "00:03:07", "is_worked_example": false, "text": "If fairness is desired, WAIT could maintain a queue of waiting processes and use the queue to determine which process is next in line, independent of scheduling order."}, {"start": "00:03:07", "is_lecture": true, "end": "00:03:12", "is_worked_example": false, "text": "Many ISAs support an instruction like the TEST-and-CLEAR instruction shown here."}, {"start": "00:03:12", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "The TCLR instruction reads the current value of a memory location and then sets it to zero, all as a single operation."}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:25", "is_worked_example": false, "text": "It's like a LD except that it zeros the memory location after reading its value."}, {"start": "00:03:25", "is_lecture": true, "end": "00:03:33", "is_worked_example": false, "text": "To implement TCLR, the memory needs to support read-and-clear operations, as well as normal reads and writes."}, {"start": "00:03:33", "is_lecture": true, "end": "00:03:38", "is_worked_example": false, "text": "The assembly code at the bottom of the slide shows how to use TCLR to implement a simple lock."}, {"start": "00:03:38", "is_lecture": true, "end": "00:03:43", "is_worked_example": false, "text": "The program uses TCLR to access the value of the lock semaphore."}, {"start": "00:03:43", "is_lecture": true, "end": "00:03:51", "is_worked_example": false, "text": "If the returned value in RC is zero, then some other process has the lock and the program loops to try TCLR again."}, {"start": "00:03:51", "is_lecture": true, "end": "00:03:59", "is_worked_example": false, "text": "If the returned value is non-zero, the lock has been acquired and execution of the critical section can proceed."}, {"start": "00:03:59", "is_lecture": true, "end": "00:04:07", "is_worked_example": false, "text": "In this case, TCLR has also set the lock to zero, so that other processes will be prevented from entering the critical section."}, {"start": "00:04:07", "is_lecture": true, "end": "00:04:16", "is_worked_example": false, "text": "When the critical section has finished executing, a ST instruction is used to set the semaphore to a non-zero value."}]}, "C08S01B06-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c8/c8s1/6?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c8s1v6", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:10", "is_worked_example": false, "text": "So far in constructing our timesharing system, we've worked hard to build an execution environment that gives each process the illusion of running on its own independent virtual machine."}, {"start": "00:00:10", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "The processes appear to run concurrently although we're really quickly switching between running processes on a single hardware system."}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "This often leads to better overall utilization since if a particular process is waiting for an I/O event, we can devote the unneeded cycles to running other processes."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:43", "is_worked_example": false, "text": "The downside of timesharing is that it can be hard to predict exactly how long a process will take to complete since the CPU time it will receive depends on how much time the other processes are using."}, {"start": "00:00:43", "is_lecture": true, "end": "00:00:49", "is_worked_example": false, "text": "So we'd need to know how many other processes there are, whether they're waiting for I/O events, etc."}, {"start": "00:00:49", "is_lecture": true, "end": "00:00:54", "is_worked_example": false, "text": "In a timesharing system we can't make any guarantees on completion times."}, {"start": "00:00:54", "is_lecture": true, "end": "00:01:05", "is_worked_example": false, "text": "And we chose to have the OS play the intermediary between interrupt events triggered by the outside world and the user-mode programs where the event processing occurs."}, {"start": "00:01:05", "is_lecture": true, "end": "00:01:11", "is_worked_example": false, "text": "In other words, we've separated event handling (where the data is stored by the OS)"}, {"start": "00:01:11", "is_lecture": true, "end": "00:01:16", "is_worked_example": false, "text": "and event processing (where the data is passed to user-mode programs via SVCs)."}, {"start": "00:01:16", "is_lecture": true, "end": "00:01:27", "is_worked_example": false, "text": "This means that using a conventional timesharing system, it's hard to ensure that event processing will be complete by a specified event deadline,"}, {"start": "00:01:27", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "i.e., before the end of a specified time period after the event was triggered."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:43", "is_worked_example": false, "text": "Since modern CPU chips provide inexpensive, high-performance, general-purpose computing, they are often used as the \"brains\" of control systems where deadlines are a fact of life."}, {"start": "00:01:43", "is_lecture": true, "end": "00:01:49", "is_worked_example": false, "text": "For example, consider the electronic stability control (ESC) system on modern cars."}, {"start": "00:01:49", "is_lecture": true, "end": "00:01:59", "is_worked_example": false, "text": "This system helps drivers maintain control of their vehicle during steering and braking maneuvers by keeping the car headed in the driver's intended direction."}, {"start": "00:01:59", "is_lecture": true, "end": "00:02:06", "is_worked_example": false, "text": "The computer at the heart of the system measures the forces on the car, the direction of steering, and the rotation of the wheels"}, {"start": "00:02:06", "is_lecture": true, "end": "00:02:12", "is_worked_example": false, "text": "to determine if there's been a loss of control due to a loss of traction, i.e., is the car \"spinning out\"?"}, {"start": "00:02:12", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "If so, the ESC uses rapid automatic braking of individual wheels to prevent the car's heading from veering from the driver's intended heading."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:33", "is_worked_example": false, "text": "With ESC you can slam on your brakes or swerve to avoid an obstacle and not worry that the car will suddenly fishtail out of control."}, {"start": "00:02:33", "is_lecture": true, "end": "00:02:37", "is_worked_example": false, "text": "You can feel the system working as a chatter in the brakes."}, {"start": "00:02:37", "is_lecture": true, "end": "00:02:48", "is_worked_example": false, "text": "To be effective, the ESC system has to guarantee the correct braking action at each wheel within a certain time of receiving dangerous sensor settings."}, {"start": "00:02:48", "is_lecture": true, "end": "00:02:57", "is_worked_example": false, "text": "This means that it has to be able to guarantee that certain subroutines will run to completion within some predetermined time of a sensor event."}, {"start": "00:02:57", "is_lecture": true, "end": "00:03:04", "is_worked_example": false, "text": "To be able to make these guarantees we'll have to come up with a better way to schedule process execution --"}, {"start": "00:03:04", "is_lecture": true, "end": "00:03:07", "is_worked_example": false, "text": "round-robin scheduling won't get the job done!"}, {"start": "00:03:07", "is_lecture": true, "end": "00:03:12", "is_worked_example": false, "text": "Systems that can make such guarantees are called \"real-time systems\"."}, {"start": "00:03:12", "is_lecture": true, "end": "00:03:24", "is_worked_example": false, "text": "One measure of performance in a real-time system is the interrupt latency L, the amount of time that elapses between a request to run some code and when that code actually starts executing."}, {"start": "00:03:24", "is_lecture": true, "end": "00:03:34", "is_worked_example": false, "text": "If there's a deadline D associated with servicing the request, we can compute the maximum allowable latency that still permits the service routine to complete by the deadline."}, {"start": "00:03:34", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "In other words, what's the largest L such that L_max+S = D?"}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:42", "is_worked_example": false, "text": "Bad things can happen if we miss certain deadlines."}, {"start": "00:03:42", "is_lecture": true, "end": "00:03:45", "is_worked_example": false, "text": "Maybe that's why we call them \"dead\"-lines :)"}, {"start": "00:03:45", "is_lecture": true, "end": "00:03:54", "is_worked_example": false, "text": "In those cases we want our real time system to guarantee that the actual latency is always less than the maximum allowable latency."}, {"start": "00:03:54", "is_lecture": true, "end": "00:03:59", "is_worked_example": false, "text": "These critical deadlines give rise to what we call \"hard real-time constraints\"."}, {"start": "00:03:59", "is_lecture": true, "end": "00:04:02", "is_worked_example": false, "text": "What factors contribute to interrupt latency?"}, {"start": "00:04:02", "is_lecture": true, "end": "00:04:12", "is_worked_example": false, "text": "Well, while handling an interrupt it takes times to save the process state, switch to the kernel context, and dispatch to the correct interrupt handler."}, {"start": "00:04:12", "is_lecture": true, "end": "00:04:18", "is_worked_example": false, "text": "When writing our OS, we can work to minimize the amount of code involved in the setup phase of an interrupt handler."}, {"start": "00:04:18", "is_lecture": true, "end": "00:04:24", "is_worked_example": false, "text": "We also have to avoid long periods of time when the processor cannot be interrupted."}, {"start": "00:04:24", "is_lecture": true, "end": "00:04:36", "is_worked_example": false, "text": "Some ISAs have complex multi-cycle instructions, e.g., block move instructions where a single instruction makes many memory accesses as it moves a block of data from one location to another."}, {"start": "00:04:36", "is_lecture": true, "end": "00:04:43", "is_worked_example": false, "text": "In designing the ISA, we need to avoid such instructions or design them so that they can be interrupted and restarted."}, {"start": "00:04:43", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "The biggest problem comes when we're executing another interrupt handler in kernel mode."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "In kernel mode, interrupts are disabled, so the actual latency will be determined by the time it takes to complete the current interrupt handler in addition to the other costs mentioned above."}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:06", "is_worked_example": false, "text": "This latency is not under the control of the CPU designer and will depend on the particular application."}, {"start": "00:05:06", "is_lecture": true, "end": "00:05:11", "is_worked_example": false, "text": "Writing programs with hard real-time constraints can get complicated!"}, {"start": "00:05:11", "is_lecture": true, "end": "00:05:16", "is_worked_example": false, "text": "Our goal is to bound and minimize interrupt latency."}, {"start": "00:05:16", "is_lecture": true, "end": "00:05:21", "is_worked_example": false, "text": "We'll do this by optimizing the cost of taking an interrupt and dispatching to the correct handler code."}, {"start": "00:05:21", "is_lecture": true, "end": "00:05:25", "is_worked_example": false, "text": "We'll avoid instructions whose execution time is data dependent."}, {"start": "00:05:25", "is_lecture": true, "end": "00:05:29", "is_worked_example": false, "text": "And we'll work to minimize the time spent in kernel mode."}, {"start": "00:05:29", "is_lecture": true, "end": "00:05:37", "is_worked_example": false, "text": "But even with all these measures, we'll see that in some cases we'll have to modify our system to allow interrupts even in kernel mode."}, {"start": "00:05:37", "is_lecture": true, "end": "00:05:47", "is_worked_example": false, "text": "Next we'll look at some concrete examples and see what mechanisms are required to make guarantees about hard real-time constraints."}]}, "C11S01B06-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c11/c11s1/6?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c11s1v6", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:12", "is_worked_example": false, "text": "Let's wrap up our discussion of system-level interconnect by considering how best to connect N components that need to send messages to one another, e.g., CPUs on a multicore chip."}, {"start": "00:00:12", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "Today such chips have a handful of cores, but soon they may have 100s or 1000s of cores."}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:22", "is_worked_example": false, "text": "We'll build our communications network using point-to-point links."}, {"start": "00:00:22", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "In our analysis, each point-to-point link is counted at a cost of 1 hardware unit."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:31", "is_worked_example": false, "text": "Sending a message across a link requires one time unit."}, {"start": "00:00:31", "is_lecture": true, "end": "00:00:38", "is_worked_example": false, "text": "And we'll assume that different links can operate in parallel, so more links will mean more message traffic."}, {"start": "00:00:38", "is_lecture": true, "end": "00:00:50", "is_worked_example": false, "text": "We'll do an asymptotic analysis of the throughput (total messages per unit time), latency (worst-case time to deliver a single message), and hardware cost."}, {"start": "00:00:50", "is_lecture": true, "end": "00:00:56", "is_worked_example": false, "text": "In other words, we'll make a rough estimate how these quantities change as N grows."}, {"start": "00:00:56", "is_lecture": true, "end": "00:01:02", "is_worked_example": false, "text": "Note that in general the throughput and hardware cost are proportional to the number of point-to-point links."}, {"start": "00:01:02", "is_lecture": true, "end": "00:01:09", "is_worked_example": false, "text": "Our baseline is the backplane bus discussed earlier, where all the components share a single communication channel."}, {"start": "00:01:09", "is_lecture": true, "end": "00:01:19", "is_worked_example": false, "text": "With only a single channel, bus throughput is 1 message per unit time and a message can travel between any two components in one time unit."}, {"start": "00:01:19", "is_lecture": true, "end": "00:01:26", "is_worked_example": false, "text": "Since each component has to have an interface to the shared channel, the total hardware cost is O(n)."}, {"start": "00:01:26", "is_lecture": true, "end": "00:01:35", "is_worked_example": false, "text": "In a ring network each component sends its messages to a single neighbor and the links are arranged so that its possible to reach all components."}, {"start": "00:01:35", "is_lecture": true, "end": "00:01:41", "is_worked_example": false, "text": "There are N links in total, so the throughput and cost are both O(n)."}, {"start": "00:01:41", "is_lecture": true, "end": "00:01:50", "is_worked_example": false, "text": "The worst case latency is also O(n) since a message might have to travel across N-1 links to reach the neighbor that's immediately upstream."}, {"start": "00:01:50", "is_lecture": true, "end": "00:02:02", "is_worked_example": false, "text": "Ring topologies are useful when message latency isn't important or when most messages are to the component that's immediately downstream, i.e., the components form a processing pipeline."}, {"start": "00:02:02", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "The most general network topology is when every component has a direct link to every other component."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:15", "is_worked_example": false, "text": "There are O(N**2) links so the throughput and cost are both O(N**2)."}, {"start": "00:02:15", "is_lecture": true, "end": "00:02:20", "is_worked_example": false, "text": "And the latency is 1 time unit since each destination is directly accessible."}, {"start": "00:02:20", "is_lecture": true, "end": "00:02:27", "is_worked_example": false, "text": "Although expensive, complete graphs offer very high throughput with very low latencies."}, {"start": "00:02:27", "is_lecture": true, "end": "00:02:38", "is_worked_example": false, "text": "A variant of the complete graph is the crossbar switch where a particular row and column can be connected to form a link between particular A and B components"}, {"start": "00:02:38", "is_lecture": true, "end": "00:02:44", "is_worked_example": false, "text": "with the restriction that each row and each column can only carry 1 message during each time unit."}, {"start": "00:02:44", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "Assume that the first row and first column connect to the same component, and so on, i.e., that the example crossbar switch is being used to connect 4 components."}, {"start": "00:02:53", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "Then there are O(n) messages delivered each time unit, with a latency of 1."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:07", "is_worked_example": false, "text": "There are N**2 switches in the crossbar, so the cost is O(N**2) even though there are only O(n) links."}, {"start": "00:03:07", "is_lecture": true, "end": "00:03:15", "is_worked_example": false, "text": "In mesh networks, components are connected to some fixed number of neighboring components, in either 2 or 3 dimensions."}, {"start": "00:03:15", "is_lecture": true, "end": "00:03:23", "is_worked_example": false, "text": "Hence the total number of links is proportional to the number of components, so both throughput and cost are O(n)."}, {"start": "00:03:23", "is_lecture": true, "end": "00:03:34", "is_worked_example": false, "text": "The worst-case latencies for mesh networks are proportional to length of the sides, so the latency is O(sqrt n) for 2D meshes and O(cube root n) for 3D meshes."}, {"start": "00:03:34", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "The orderly layout, constant per-node hardware costs, and modest worst-case latency make 2D 4-neighbor meshes a popular choice for the current generation of experimental multi-core processors."}, {"start": "00:03:47", "is_lecture": true, "end": "00:03:55", "is_worked_example": false, "text": "Hypercube and tree networks offer logarithmic latencies, which for large N may be faster than mesh networks."}, {"start": "00:03:55", "is_lecture": true, "end": "00:04:09", "is_worked_example": false, "text": "The original CM-1 Connection Machine designed in the 80's used a hypercube network to connect up to 65,536 very simple processors, each connected to 16 neighbors."}, {"start": "00:04:09", "is_lecture": true, "end": "00:04:17", "is_worked_example": false, "text": "Later generations incorporated smaller numbers of more sophisticated processors, still connected by a hypercube network."}, {"start": "00:04:17", "is_lecture": true, "end": "00:04:28", "is_worked_example": false, "text": "In the early 90's the last generation of Connection Machines used a tree network, with the clever innovation that the links towards the root of the tree had a higher message capacity."}, {"start": "00:04:28", "is_lecture": true, "end": "00:04:34", "is_worked_example": false, "text": "Here's a summary of the theoretical latencies we calculated for the various topologies."}, {"start": "00:04:34", "is_lecture": true, "end": "00:04:43", "is_worked_example": false, "text": "As a reality check, it's important to realize that the lower bound on the worst-case distance between components in our 3-dimensional world is O(cube root of N)."}, {"start": "00:04:43", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "In the case of a 2D layout, the worst-case distance is O(sqrt N)."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:49", "is_worked_example": false, "text": "Since we know that the time to transmit a message is proportional to the distance traveled, we should modify our latency calculations to reflect this physical constraint."}, {"start": "00:04:49", "is_lecture": true, "end": "00:05:10", "is_worked_example": false, "text": "Note that the bus and crossbar involve N connections to a single link, so here the lower-bound on the latency needs to reflect the capacitive load added by each connection."}, {"start": "00:05:10", "is_lecture": true, "end": "00:05:12", "is_worked_example": false, "text": "The winner?"}, {"start": "00:05:12", "is_lecture": true, "end": "00:05:25", "is_worked_example": false, "text": "Mesh networks avoid the need for longer wires as the number of connected components grows and appear to be an attractive alternative for high-capacity communication networks connecting 1000's of processors."}, {"start": "00:05:25", "is_lecture": true, "end": "00:05:27", "is_worked_example": false, "text": "Summarizing our discussion:"}, {"start": "00:05:27", "is_lecture": true, "end": "00:05:38", "is_worked_example": false, "text": "point-to-point links are in common use today for system-level interconnect, and as a result our systems are faster, more reliable, more energy-efficient and smaller than ever before."}, {"start": "00:05:38", "is_lecture": true, "end": "00:05:51", "is_worked_example": false, "text": "Multi-signal parallel buses are still used for very-high-bandwidth connections to memories, with a lot of very careful engineering to avoid the electrical problems observed in earlier bus implementations."}, {"start": "00:05:51", "is_lecture": true, "end": "00:06:07", "is_worked_example": false, "text": "Wireless connections are in common use to connect mobile devices to nearby components and there has been interesting work on how to allow mobile devices to discover what peripherals are nearby and enable them to connect automatically."}, {"start": "00:06:07", "is_lecture": true, "end": "00:06:12", "is_worked_example": false, "text": "The upcoming generation of multi-core chips will have 10's to 100's of processing cores."}, {"start": "00:06:12", "is_lecture": true, "end": "00:06:22", "is_worked_example": false, "text": "There is a lot ongoing research to determine which communication topology would offer the best combination of high communication bandwidth and low latency."}, {"start": "00:06:22", "is_lecture": true, "end": "00:06:28", "is_worked_example": false, "text": "The next ten years will be an interesting time for on-chip network engineers!"}]}, "C11S01B05-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c11/c11s1/5?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c11s1v5", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "Serial point-to-point links are the modern replacement for the parallel communications bus with all its electrical and timing issues. 0..08 Each link is unidirectional and has only a single driver and the receiver recovers the clock signal from the data stream, so there are no complications from sharing the channel, clock skew, and electrical problems."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "The very controlled electrical environment enables very high signaling rates, well up into the gigahertz range using today's technologies."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:33", "is_worked_example": false, "text": "If more throughput is needed, you can use multiple serial links in parallel."}, {"start": "00:00:33", "is_lecture": true, "end": "00:00:44", "is_worked_example": false, "text": "Extra logic is needed to reassemble the original data from multiple packets sent in parallel over multiple links, but the cost of the required logic gates is very modest in current technologies."}, {"start": "00:00:44", "is_lecture": true, "end": "00:00:52", "is_worked_example": false, "text": "Note that the expansion strategy of modern systems still uses the notion of an add-in card that plugs into the motherboard."}, {"start": "00:00:52", "is_lecture": true, "end": "00:01:00", "is_worked_example": false, "text": "But instead of connecting to a parallel bus, the add-in card connects to one or more point-to-point communication links."}, {"start": "00:01:00", "is_lecture": true, "end": "00:01:07", "is_worked_example": false, "text": "Here's the system-level communications diagram for a recent system based on an Intel Core i7 CPU chip."}, {"start": "00:01:07", "is_lecture": true, "end": "00:01:12", "is_worked_example": false, "text": "The CPU is connected directly to the memories for the highest-possible memory bandwidth,"}, {"start": "00:01:12", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "but talks to all the other components over the QuickPath Interconnect (QPI), which has 20 differential signaling paths in each direction."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:27", "is_worked_example": false, "text": "QPI supports up to 6.4 billion 20-bit transfers in each direction every second."}, {"start": "00:01:27", "is_lecture": true, "end": "00:01:41", "is_worked_example": false, "text": "All the other communication channels (USB, PCIe, networks, Serial ATA, Audio, etc.) are also serial links, providing various communication bandwidths depending on the application."}, {"start": "00:01:41", "is_lecture": true, "end": "00:01:53", "is_worked_example": false, "text": "Reading about the QPI channel used by the CPU reminded me a lot of the one ring that could be used to control all of Middle Earth in the Tolkien trilogy Lord of the Rings."}, {"start": "00:01:53", "is_lecture": true, "end": "00:02:01", "is_worked_example": false, "text": "Why mess around with a lot of specialized communication channels when you have a single solution that's powerful enough to solve all your communication needs?"}, {"start": "00:02:01", "is_lecture": true, "end": "00:02:08", "is_worked_example": false, "text": "PCI Express (PCIe) is often used as the communication link between components on the system motherboard."}, {"start": "00:02:08", "is_lecture": true, "end": "00:02:20", "is_worked_example": false, "text": "A single PCIe version 2 \"lane\" transmits data at 5 Gb/sec using low-voltage differential signaling (LVDS) over wires designed to have a 100-Ohm characteristic impedance."}, {"start": "00:02:20", "is_lecture": true, "end": "00:02:26", "is_worked_example": false, "text": "The PCIe lane is under the control of the same sort of network stack as described earlier."}, {"start": "00:02:26", "is_lecture": true, "end": "00:02:30", "is_worked_example": false, "text": "The physical layer transmits packet data through the wire."}, {"start": "00:02:30", "is_lecture": true, "end": "00:02:43", "is_worked_example": false, "text": "Each packet starts with a training sequence to synchronize the receiver's clock-recovery circuitry, followed by a unique start sequence, then the packet's data payload, and ends with a unique end sequence."}, {"start": "00:02:43", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "The physical layer payload is organized as a sequence number, a transaction-layer payload and a cyclical redundancy check sequence that's used to validate the data."}, {"start": "00:02:53", "is_lecture": true, "end": "00:03:02", "is_worked_example": false, "text": "Using the sequence number, the data link layer can tell when a packet has been dropped and request the transmitter restart transmission at the missing packet."}, {"start": "00:03:02", "is_lecture": true, "end": "00:03:05", "is_worked_example": false, "text": "It also deals with flow control issues."}, {"start": "00:03:05", "is_lecture": true, "end": "00:03:16", "is_worked_example": false, "text": "Finally, the transaction layer reassembles the message from the transaction layer payloads from all the lanes and uses the header to identify the intended recipient at the receive end."}, {"start": "00:03:16", "is_lecture": true, "end": "00:03:28", "is_worked_example": false, "text": "Altogether, a significant amount of logic is needed to send and receive messages on multiple PCIe lanes, but the cost is quite acceptable when using today's integrated circuit technologies."}, {"start": "00:03:28", "is_lecture": true, "end": "00:03:38", "is_worked_example": false, "text": "Using 8 lanes, the maximum transfer rate is 4 GB/sec, capable of satisfying the needs of high-performance peripherals such as graphics cards."}, {"start": "00:03:38", "is_lecture": true, "end": "00:03:49", "is_worked_example": false, "text": "So knowledge from the networking world has reshaped how components communicate on the motherboard, driving the transition from parallel buses to a handful of serial point-to-point links."}, {"start": "00:03:49", "is_lecture": true, "end": "00:03:56", "is_worked_example": false, "text": "As a result today's systems are faster, more reliable, more energy-efficient and smaller than ever before."}]}, "C05S01B02-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c5/c5s1/2?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c5s1v2", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:03", "is_worked_example": false, "text": "Here's how our virtual memory system will work."}, {"start": "00:00:03", "is_lecture": true, "end": "00:00:11", "is_worked_example": false, "text": "The memory addresses generated by the CPU are called virtual addresses to distinguish them from the physical addresses used by main memory."}, {"start": "00:00:11", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "In between the CPU and main memory there's a new piece of hardware called the memory management unit (MMU)."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "The MMU's job is to translate virtual addresses to physical addresses."}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:26", "is_worked_example": false, "text": "\"But wait!\" you say."}, {"start": "00:00:26", "is_lecture": true, "end": "00:00:30", "is_worked_example": false, "text": "\"Doesn't the cache go between the CPU and main memory?\""}, {"start": "00:00:30", "is_lecture": true, "end": "00:00:36", "is_worked_example": false, "text": "You're right and at the end of this lecture we'll talk about how to use both an MMU and a cache."}, {"start": "00:00:36", "is_lecture": true, "end": "00:00:40", "is_worked_example": false, "text": "But for now, let's assume there's only an MMU and no cache."}, {"start": "00:00:40", "is_lecture": true, "end": "00:00:48", "is_worked_example": false, "text": "The MMU hardware translates virtual addresses to physical addresses using a simple table lookup."}, {"start": "00:00:48", "is_lecture": true, "end": "00:00:51", "is_worked_example": false, "text": "This table is called the page map or page table."}, {"start": "00:00:51", "is_lecture": true, "end": "00:01:00", "is_worked_example": false, "text": "Conceptually, the MMU uses the virtual address as index to select an entry in the table, which tells us the corresponding physical address."}, {"start": "00:01:00", "is_lecture": true, "end": "00:01:05", "is_worked_example": false, "text": "The table allows a particular virtual address to be found anywhere in main memory."}, {"start": "00:01:05", "is_lecture": true, "end": "00:01:12", "is_worked_example": false, "text": "In normal operation we'd want to ensure that two virtual addresses don't map to the same physical address."}, {"start": "00:01:12", "is_lecture": true, "end": "00:01:19", "is_worked_example": false, "text": "But it would be okay if some of the virtual addresses did not have a translation to a physical address."}, {"start": "00:01:19", "is_lecture": true, "end": "00:01:29", "is_worked_example": false, "text": "This would indicate that the contents of the requested virtual address haven't yet been loaded into main memory, so the MMU would signal a memory-management exception to the CPU,"}, {"start": "00:01:29", "is_lecture": true, "end": "00:01:38", "is_worked_example": false, "text": "which could assign a location in physical memory and perform the required I/O operation to initialize that location from secondary storage."}, {"start": "00:01:38", "is_lecture": true, "end": "00:01:47", "is_worked_example": false, "text": "The MMU table gives the system a lot of control over how physical memory is accessed by the program running on the CPU."}, {"start": "00:01:47", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "For example, we could arrange to run multiple programs in quick succession (a technique called time sharing) by changing the page map when we change programs."}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:06", "is_worked_example": false, "text": "Main memory locations accessible to one program could be made inaccessible to another program by proper management of their respective page maps."}, {"start": "00:02:06", "is_lecture": true, "end": "00:02:17", "is_worked_example": false, "text": "And we could use memory-management exceptions to load program contents into main memory on demand instead of having to load the entire program before execution starts."}, {"start": "00:02:17", "is_lecture": true, "end": "00:02:24", "is_worked_example": false, "text": "In fact, we only need to ensure the current working set of a program is actually resident in main memory."}, {"start": "00:02:24", "is_lecture": true, "end": "00:02:29", "is_worked_example": false, "text": "Locations not currently being used could live in secondary storage until needed."}, {"start": "00:02:29", "is_lecture": true, "end": "00:02:37", "is_worked_example": false, "text": "In this lecture and next, we'll see how the MMU plays a central role in the design of a modern timesharing computer system."}, {"start": "00:02:37", "is_lecture": true, "end": "00:02:45", "is_worked_example": false, "text": "Of course, we'd need an impossibly large table to separately map each virtual address to a physical address."}, {"start": "00:02:45", "is_lecture": true, "end": "00:02:52", "is_worked_example": false, "text": "So instead we divide both the virtual and physical address spaces into fixed-sized blocks, called pages."}, {"start": "00:02:52", "is_lecture": true, "end": "00:03:03", "is_worked_example": false, "text": "Page sizes are always a power-of-2 bytes, say 2^p bytes, so p is the number address bits needed to select a particular location on the page."}, {"start": "00:03:03", "is_lecture": true, "end": "00:03:08", "is_worked_example": false, "text": "We'll the use low-order p bits of the virtual or physical address as the page offset."}, {"start": "00:03:08", "is_lecture": true, "end": "00:03:15", "is_worked_example": false, "text": "The remaining address bits tell us which page is being accessed and are called the page number."}, {"start": "00:03:15", "is_lecture": true, "end": "00:03:24", "is_worked_example": false, "text": "A typical page size is 4KB to 16KB, which correspond to p=12 and p=14 respectively."}, {"start": "00:03:24", "is_lecture": true, "end": "00:03:26", "is_worked_example": false, "text": "Suppose p=12."}, {"start": "00:03:26", "is_lecture": true, "end": "00:03:38", "is_worked_example": false, "text": "If the CPU produces a 32-bit virtual address, the low-order 12 bits of the virtual address are the page offset and the high-order 20 bits are the virtual page number."}, {"start": "00:03:38", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "Similarly, the low-order p bits of the physical address are the page offset and the remaining physical address bits are the physical page number."}, {"start": "00:03:47", "is_lecture": true, "end": "00:03:53", "is_worked_example": false, "text": "The key idea is that the MMU will manage pages, not individual locations."}, {"start": "00:03:53", "is_lecture": true, "end": "00:03:57", "is_worked_example": false, "text": "We'll move entire pages from secondary storage into main memory."}, {"start": "00:03:57", "is_lecture": true, "end": "00:04:07", "is_worked_example": false, "text": "By the principal of locality, if a program access one location on a page, we expect it will soon access other nearby locations."}, {"start": "00:04:07", "is_lecture": true, "end": "00:04:18", "is_worked_example": false, "text": "By choosing the page offset from the low-order address bits, we'll ensure that nearby locations live on the same page (unless of course we're near one end of the page or the other)."}, {"start": "00:04:18", "is_lecture": true, "end": "00:04:22", "is_worked_example": false, "text": "So pages naturally capture the notion of locality."}, {"start": "00:04:22", "is_lecture": true, "end": "00:04:27", "is_worked_example": false, "text": "And since pages are large, by dealing with pages when accessing secondary storage,"}, {"start": "00:04:27", "is_lecture": true, "end": "00:04:35", "is_worked_example": false, "text": "we'll take advantage that reading or writing many locations is only slightly more time consuming than accessing the first location."}, {"start": "00:04:35", "is_lecture": true, "end": "00:04:40", "is_worked_example": false, "text": "The MMU will map virtual page numbers to physical page numbers."}, {"start": "00:04:40", "is_lecture": true, "end": "00:04:47", "is_worked_example": false, "text": "It does this by using the virtual page number (VPN) as an index into the page table."}, {"start": "00:04:47", "is_lecture": true, "end": "00:04:55", "is_worked_example": false, "text": "Each entry in the page table indicates if the page is resident in main memory and, if it is, provides the appropriate physical page number (PPN)."}, {"start": "00:04:55", "is_lecture": true, "end": "00:05:02", "is_worked_example": false, "text": "The PPN is combined with the page offset to form the physical address for main memory."}, {"start": "00:05:02", "is_lecture": true, "end": "00:05:13", "is_worked_example": false, "text": "If the requested virtual page is NOT resident in main memory, the MMU signals a memory-management exception, called a page fault, to the CPU"}, {"start": "00:05:13", "is_lecture": true, "end": "00:05:20", "is_worked_example": false, "text": "so it can load the appropriate page from secondary storage and set up the appropriate mapping in the MMU."}, {"start": "00:05:20", "is_lecture": true, "end": "00:05:31", "is_worked_example": false, "text": "Our plan to use main memory as page cache is called \"paging\" or sometimes \"demand paging\" since movements of pages to and from secondary storage is determined by the demands of the program."}, {"start": "00:05:31", "is_lecture": true, "end": "00:05:34", "is_worked_example": false, "text": "So here's the plan."}, {"start": "00:05:34", "is_lecture": true, "end": "00:05:44", "is_worked_example": false, "text": "Initially all the virtual pages for a program reside in secondary storage and the MMU is empty, i.e., there are no pages resident in physical memory."}, {"start": "00:05:44", "is_lecture": true, "end": "00:05:56", "is_worked_example": false, "text": "The CPU starts running the program and each virtual address it generates, either for an instruction fetch or data access, is passed to the MMU to be mapped to a physical address in main memory."}, {"start": "00:05:56", "is_lecture": true, "end": "00:06:02", "is_worked_example": false, "text": "If the virtual address is resident in physical memory, the main memory hardware can complete the access."}, {"start": "00:06:02", "is_lecture": true, "end": "00:06:15", "is_worked_example": false, "text": "If the virtual address in NOT resident in physical memory, the MMU signals a page fault exception, forcing the CPU to switch execution to special code called the page fault handler."}, {"start": "00:06:15", "is_lecture": true, "end": "00:06:25", "is_worked_example": false, "text": "The handler allocates a physical page to hold the requested virtual page and loads the virtual page from secondary storage into main memory."}, {"start": "00:06:25", "is_lecture": true, "end": "00:06:37", "is_worked_example": false, "text": "It then adjusts the page map entry for the requested virtual page to show that it is now resident and to indicate the physical page number for the newly allocated and initialized physical page."}, {"start": "00:06:37", "is_lecture": true, "end": "00:06:45", "is_worked_example": false, "text": "When trying to allocate a physical page, the handler may discover that all physical pages are currently in use."}, {"start": "00:06:45", "is_lecture": true, "end": "00:06:54", "is_worked_example": false, "text": "In this case it chooses an existing page to replace, e.g., a resident virtual page that hasn't been recently accessed."}, {"start": "00:06:54", "is_lecture": true, "end": "00:07:05", "is_worked_example": false, "text": "It swaps the contents of the chosen virtual page out to secondary storage and updates the page map entry for the replaced virtual page to indicate it is no longer resident."}, {"start": "00:07:05", "is_lecture": true, "end": "00:07:11", "is_worked_example": false, "text": "Now there's a free physical page to re-use to hold the contents of the virtual page that was missing."}, {"start": "00:07:11", "is_lecture": true, "end": "00:07:22", "is_worked_example": false, "text": "The working set of the program, i.e., the set of pages the program is currently accessing, is loaded into main memory through a series of page faults."}, {"start": "00:07:22", "is_lecture": true, "end": "00:07:36", "is_worked_example": false, "text": "After a flurry of page faults when the program starts running, the working set changes slowly, so the frequency of page faults drops dramatically, perhaps close to zero if the program is small and well-behaved."}, {"start": "00:07:36", "is_lecture": true, "end": "00:07:42", "is_worked_example": false, "text": "It is possible to write programs that consistently generate page faults, a phenomenon called thrashing."}, {"start": "00:07:42", "is_lecture": true, "end": "00:07:55", "is_worked_example": false, "text": "Given the long access times of secondary storage, a program that's thrashing runs *very* slowly, usually so slowly that users give up and rewrite the program to behave more sensibly."}, {"start": "00:07:55", "is_lecture": true, "end": "00:07:59", "is_worked_example": false, "text": "The design of the page map is straightforward."}, {"start": "00:07:59", "is_lecture": true, "end": "00:08:03", "is_worked_example": false, "text": "There's one entry in the page map for each virtual page."}, {"start": "00:08:03", "is_lecture": true, "end": "00:08:17", "is_worked_example": false, "text": "For example, if the CPU generates a 32-bit virtual address and the page size is 2^12 bytes, the virtual page number has 32-12 = 20 bits and the page table will have 2^20 entries."}, {"start": "00:08:17", "is_lecture": true, "end": "00:08:27", "is_worked_example": false, "text": "Each entry in the page table contains a \"resident bit\" (R) which is set to 1 when the virtual page is resident in physical memory."}, {"start": "00:08:27", "is_lecture": true, "end": "00:08:32", "is_worked_example": false, "text": "If R is 0, an access to that virtual page will cause a page fault."}, {"start": "00:08:32", "is_lecture": true, "end": "00:08:41", "is_worked_example": false, "text": "If R is 1, the entry also contains the PPN, indicating where to find the virtual page in main memory."}, {"start": "00:08:41", "is_lecture": true, "end": "00:08:46", "is_worked_example": false, "text": "There's one additional state bit called the \"dirty bit\" (D)."}, {"start": "00:08:46", "is_lecture": true, "end": "00:08:57", "is_worked_example": false, "text": "When a page has just been loaded from secondary storage, it's \"clean\", i.e, the contents of physical memory match the contents of the page in secondary storage."}, {"start": "00:08:57", "is_lecture": true, "end": "00:09:00", "is_worked_example": false, "text": "So the D bit is set to 0."}, {"start": "00:09:00", "is_lecture": true, "end": "00:09:14", "is_worked_example": false, "text": "If subsequently the CPU stores into a location on the page, the D bit for the page is set to 1, indicating the page is \"dirty\", i.e., the contents of memory now differ from the contents of secondary storage."}, {"start": "00:09:14", "is_lecture": true, "end": "00:09:25", "is_worked_example": false, "text": "If a dirty page is ever chosen for replacement, its contents must be written to secondary storage in order to save the changes before the page gets reused."}, {"start": "00:09:25", "is_lecture": true, "end": "00:09:30", "is_worked_example": false, "text": "Some MMUs have additional state bits in each page table entry."}, {"start": "00:09:30", "is_lecture": true, "end": "00:09:38", "is_worked_example": false, "text": "For example, there could be a \"read-only\" bit which, when set, would generate an exception if the program attempts to store into the page."}, {"start": "00:09:38", "is_lecture": true, "end": "00:09:46", "is_worked_example": false, "text": "This would be useful for protecting code pages from accidentally being corrupted by errant data accesses, a very handy debugging feature."}, {"start": "00:09:46", "is_lecture": true, "end": "00:09:50", "is_worked_example": false, "text": "Here's an example of the MMU in action."}, {"start": "00:09:50", "is_lecture": true, "end": "00:09:59", "is_worked_example": false, "text": "To make things simple, assume that the virtual address is 12 bits, consisting of an 8-bit page offset and a 4-bit virtual page number."}, {"start": "00:09:59", "is_lecture": true, "end": "00:10:03", "is_worked_example": false, "text": "So there are 2^4 = 16 virtual pages."}, {"start": "00:10:03", "is_lecture": true, "end": "00:10:11", "is_worked_example": false, "text": "The physical address is 11 bits, divided into the same 8-bit page offset and a 3-bit physical page number."}, {"start": "00:10:11", "is_lecture": true, "end": "00:10:14", "is_worked_example": false, "text": "So there are 2^3 = 8 physical pages."}, {"start": "00:10:14", "is_lecture": true, "end": "00:10:23", "is_worked_example": false, "text": "On the left we see a diagram showing the contents of the 16-entry page map, i.e., an entry for each virtual page."}, {"start": "00:10:23", "is_lecture": true, "end": "00:10:33", "is_worked_example": false, "text": "Each page table entry includes a dirty bit (D), a resident bit (R) and a 3-bit physical page number, for a total of 5 bits."}, {"start": "00:10:33", "is_lecture": true, "end": "00:10:40", "is_worked_example": false, "text": "So the page map has 16 entries, each with 5-bits, for a total of 16*5 = 80 bits."}, {"start": "00:10:40", "is_lecture": true, "end": "00:10:47", "is_worked_example": false, "text": "The first entry in the table is for virtual page 0, the second entry for virtual page 1, and so on."}, {"start": "00:10:47", "is_lecture": true, "end": "00:10:54", "is_worked_example": false, "text": "In the middle of the slide there's a diagram of physical memory showing the 8 physical pages."}, {"start": "00:10:54", "is_lecture": true, "end": "00:11:00", "is_worked_example": false, "text": "The annotation for each physical page shows the virtual page number of its contents."}, {"start": "00:11:00", "is_lecture": true, "end": "00:11:05", "is_worked_example": false, "text": "Note that there's no particular order to how virtual pages are stored in physical memory --"}, {"start": "00:11:05", "is_lecture": true, "end": "00:11:11", "is_worked_example": false, "text": "which page holds what is determined by which pages are free at the time of a page fault."}, {"start": "00:11:11", "is_lecture": true, "end": "00:11:18", "is_worked_example": false, "text": "In general, after the program has run for a while, we'd expected to find the sort of jumbled ordering we see here."}, {"start": "00:11:18", "is_lecture": true, "end": "00:11:29", "is_worked_example": false, "text": "Let's follow along as the MMU handles the request for virtual address 0x2C8, generated by the execution of the LD instruction shown here."}, {"start": "00:11:29", "is_lecture": true, "end": "00:11:39", "is_worked_example": false, "text": "Splitting the virtual address into page number and offset, we see that the VPN is 2 and the offset is 0xC8."}, {"start": "00:11:39", "is_lecture": true, "end": "00:11:49", "is_worked_example": false, "text": "Looking at the page map entry with index 2, we see that the R bit is 1, indicating that virtual page 2 is resident in physical memory."}, {"start": "00:11:49", "is_lecture": true, "end": "00:11:56", "is_worked_example": false, "text": "The PPN field of entry tells us that virtual page 2 can be found in physical page 4."}, {"start": "00:11:56", "is_lecture": true, "end": "00:12:08", "is_worked_example": false, "text": "Combining the PPN with the 8-bit offset, we find that the contents of virtual address 0x2C8 can be found in main memory location 0x4C8."}, {"start": "00:12:08", "is_lecture": true, "end": "00:12:13", "is_worked_example": false, "text": "Note that the offset is unchanged by the translation process --"}, {"start": "00:12:13", "is_lecture": true, "end": "00:12:19", "is_worked_example": false, "text": "the offset into the physical page is always the same as the offset into the virtual page."}]}, "C12S01B02-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c12/c12s1/2?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c12s1v2", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:04", "is_worked_example": false, "text": "For some applications, data naturally comes in vector or matrix form."}, {"start": "00:00:04", "is_lecture": true, "end": "00:00:14", "is_worked_example": false, "text": "For example, a vector of digitized samples representing an audio waveform over time, or a matrix of pixel colors in a 2D image from a camera."}, {"start": "00:00:14", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "When processing that data, it's common to perform the same sequence of operations on each data element."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "The example code shown here is computing a vector sum, where each component of one vector is added to the corresponding component of another vector."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "By replicating the datapath portion of our CPU, we can design special-purpose vector processors capable of performing the same operation on many data elements in parallel."}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:50", "is_worked_example": false, "text": "Here we see that the register file and ALU have been replicated and the control signals from decoding the current instruction are shared by all the datapaths."}, {"start": "00:00:50", "is_lecture": true, "end": "00:01:01", "is_worked_example": false, "text": "Data is fetched from memory in big blocks (very much like fetching a cache line) and the specified register in each datapath is loaded with one of the words from the block."}, {"start": "00:01:01", "is_lecture": true, "end": "00:01:07", "is_worked_example": false, "text": "Similarly each datapath can contribute a word to be stored as a contiguous block in main memory."}, {"start": "00:01:07", "is_lecture": true, "end": "00:01:18", "is_worked_example": false, "text": "In such machines, the width of the data buses to and from main memory is many words wide, so a single memory access provides data for all the datapaths in parallel."}, {"start": "00:01:18", "is_lecture": true, "end": "00:01:28", "is_worked_example": false, "text": "Executing a single instruction on a machine with N datapaths is equivalent to executing N instructions on a conventional machine with a single datapath."}, {"start": "00:01:28", "is_lecture": true, "end": "00:01:35", "is_worked_example": false, "text": "The result achieves a lot of parallelism without the complexities of out-of-order superscalar execution."}, {"start": "00:01:35", "is_lecture": true, "end": "00:01:39", "is_worked_example": false, "text": "Suppose we had a vector processor with 16 datapaths."}, {"start": "00:01:39", "is_lecture": true, "end": "00:01:45", "is_worked_example": false, "text": "Let's compare its performance on a vector-sum operation to that of a conventional pipelined Beta processor."}, {"start": "00:01:45", "is_lecture": true, "end": "00:01:51", "is_worked_example": false, "text": "Here's the Beta code, carefully organized to avoid any data hazards during execution."}, {"start": "00:01:51", "is_lecture": true, "end": "00:02:00", "is_worked_example": false, "text": "There are 9 instructions in the loop, taking 10 cycles to execute if we count the NOP introduced into the pipeline when the BNE at the end of the loop is taken."}, {"start": "00:02:00", "is_lecture": true, "end": "00:02:08", "is_worked_example": false, "text": "It takes 160 cycles to sum all 16 elements assuming no additional cycles are required due to cache misses."}, {"start": "00:02:08", "is_lecture": true, "end": "00:02:15", "is_worked_example": false, "text": "And here's the corresponding code for a vector processor where we've assumed constant-sized 16-element vectors."}, {"start": "00:02:15", "is_lecture": true, "end": "00:02:26", "is_worked_example": false, "text": "Note that \"V\" registers refer to a particular location in the register file associated with each datapath, while the \"R\" registers are the conventional Beta registers used for address computations, etc."}, {"start": "00:02:26", "is_lecture": true, "end": "00:02:33", "is_worked_example": false, "text": "It would only take 4 cycles for the vector processor to complete the desired operations, a speed-up of 40."}, {"start": "00:02:33", "is_lecture": true, "end": "00:02:37", "is_worked_example": false, "text": "This example shows the best-possible speed-up."}, {"start": "00:02:37", "is_lecture": true, "end": "00:02:44", "is_worked_example": false, "text": "The key to a good speed-up is our ability to \"vectorize\" the code and take advantage of all the datapaths operating in parallel."}, {"start": "00:02:44", "is_lecture": true, "end": "00:02:57", "is_worked_example": false, "text": "This isn't possible for every application, but for tasks like audio or video encoding and decoding, and all sorts of digital signal processing, vectorization is very doable."}, {"start": "00:02:57", "is_lecture": true, "end": "00:03:05", "is_worked_example": false, "text": "Memory operations enjoy a similar performance improvement since the access overhead is amortized over large blocks of data."}, {"start": "00:03:05", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "You might wonder if it's possible to efficiently perform data-dependent operations on a vector processor."}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "Data-dependent operations appear as conditional statements on conventional machines, where the body of the statement is executed if the condition is true."}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:29", "is_worked_example": false, "text": "If testing and branching is under the control of the single-instruction execution engine, how can we take advantage of the parallel datapaths?"}, {"start": "00:03:29", "is_lecture": true, "end": "00:03:34", "is_worked_example": false, "text": "The trick is provide each datapath with a local predicate flag."}, {"start": "00:03:34", "is_lecture": true, "end": "00:03:46", "is_worked_example": false, "text": "Use a vectorized compare instruction (CMPLT.V) to perform the a[i] < b[i] comparisons in parallel and remember the result locally in each datapath's predicate flag."}, {"start": "00:03:46", "is_lecture": true, "end": "00:03:55", "is_worked_example": false, "text": "Then extend the vector ISA to include \"predicated instructions\" which check the local predicate to see if they should execute or do nothing."}, {"start": "00:03:55", "is_lecture": true, "end": "00:04:05", "is_worked_example": false, "text": "In this example, ADDC.V.iftrue only performs the ADDC on the local data if the local predicate flag is true."}, {"start": "00:04:05", "is_lecture": true, "end": "00:04:14", "is_worked_example": false, "text": "Instruction predication is also used in many non-vector architectures to avoid the execution-time penalties associated with mis-predicted conditional branches."}, {"start": "00:04:14", "is_lecture": true, "end": "00:04:24", "is_worked_example": false, "text": "They are particularly useful for simple arithmetic and boolean operations (i.e., very short instruction sequences) that should be executed only if a condition is met."}, {"start": "00:04:24", "is_lecture": true, "end": "00:04:36", "is_worked_example": false, "text": "The x86 ISA includes a conditional move instruction, and in the 32-bit ARM ISA almost all instructions can be conditionally executed."}, {"start": "00:04:36", "is_lecture": true, "end": "00:04:44", "is_worked_example": false, "text": "The power of vector processors comes from having 1 instruction initiate N parallel operations on N pairs of operands."}, {"start": "00:04:44", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "Most modern CPUs incorporate vector extensions that operate in parallel on 8-, 16-, 32- or 64-bit operands organized as blocks of 128-, 256-, or 512-bit data."}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:05", "is_worked_example": false, "text": "Often all that's needed is some simple additional logic on an ALU designed to process full-width operands."}, {"start": "00:05:05", "is_lecture": true, "end": "00:05:14", "is_worked_example": false, "text": "The parallelism is baked into the vector program, not discovered on-the-fly by the instruction dispatch and execution machinery."}, {"start": "00:05:14", "is_lecture": true, "end": "00:05:26", "is_worked_example": false, "text": "Writing the specialized vector programs is a worthwhile investment for certain library functions which see a lot use in processing today's information streams with their heavy use of images, and A/V material."}, {"start": "00:05:26", "is_lecture": true, "end": "00:05:39", "is_worked_example": false, "text": "Perhaps the best example of architectures with many datapaths operating in parallel are the graphics processing units (GPUs) found in almost all computer graphics systems."}, {"start": "00:05:39", "is_lecture": true, "end": "00:05:48", "is_worked_example": false, "text": "GPU datapaths are typically specialized for 32- and 64-bit floating point operations found in the algorithms needed to display in real-time"}, {"start": "00:05:48", "is_lecture": true, "end": "00:05:55", "is_worked_example": false, "text": "a 3D scene represented as billions of triangular patches as a 2D image on the computer screen."}, {"start": "00:05:55", "is_lecture": true, "end": "00:06:04", "is_worked_example": false, "text": "Coordinate transformation, pixel shading and antialiasing, texture mapping, etc., are examples of \"embarrassingly parallel\" computations"}, {"start": "00:06:04", "is_lecture": true, "end": "00:06:10", "is_worked_example": false, "text": "where the parallelism comes from having to perform the same computation independently on millions of different data objects."}, {"start": "00:06:10", "is_lecture": true, "end": "00:06:20", "is_worked_example": false, "text": "Similar problems can be found in the fields of bioinformatics, big data processing, neural net emulation used in deep machine learning, and so on."}, {"start": "00:06:20", "is_lecture": true, "end": "00:06:29", "is_worked_example": false, "text": "Increasingly, GPUs are used in many interesting scientific and engineering calculations and not just as graphics engines."}, {"start": "00:06:29", "is_lecture": true, "end": "00:06:36", "is_worked_example": false, "text": "Data-level parallelism provides significant performance improvements in a variety of useful situations."}, {"start": "00:06:36", "is_lecture": true, "end": "00:06:44", "is_worked_example": false, "text": "So current and future ISAs will almost certainly include support for vector operations."}]}, "C12S01B03-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c12/c12s1/3?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c12s1v3", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:12", "is_worked_example": false, "text": "In discussing out-of-order superscalar pipelined CPUs we commented that the costs grow very quickly relative to the performance gains, leading to the cost-performance curve shown here."}, {"start": "00:00:12", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "If we move down the curve, we can arrive at more efficient architectures that give, say, 1/2 the performance at a 1/4 of the cost."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "When our applications involve independent computations that can be performed in a parallel,"}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:33", "is_worked_example": false, "text": "it may be that we would be able to use two cores to provide the same performance as the original expensive core, but a fraction of the cost."}, {"start": "00:00:33", "is_lecture": true, "end": "00:00:42", "is_worked_example": false, "text": "If the available parallelism allows us to use additional cores, we'll see a linear relationship between increased performance vs. increased cost."}, {"start": "00:00:42", "is_lecture": true, "end": "00:00:53", "is_worked_example": false, "text": "The key, of course, is that desired computations can be divided into multiple tasks that can run independently, with little or no need for communication or coordination between the tasks."}, {"start": "00:00:53", "is_lecture": true, "end": "00:00:58", "is_worked_example": false, "text": "What is the optimal tradeoff between core cost and the number of cores?"}, {"start": "00:00:58", "is_lecture": true, "end": "00:01:04", "is_worked_example": false, "text": "If our computation is arbitrarily divisible without incurring additional overhead,"}, {"start": "00:01:04", "is_lecture": true, "end": "00:01:12", "is_worked_example": false, "text": "then we would continue to move down the curve until we found the cost-performance point that gave us the desired performance at the least cost."}, {"start": "00:01:12", "is_lecture": true, "end": "00:01:25", "is_worked_example": false, "text": "In reality, dividing the computation across many cores does involve some overhead, e.g., distributing the data and code, then collecting and aggregating the results, so the optimal tradeoff is harder to find."}, {"start": "00:01:25", "is_lecture": true, "end": "00:01:31", "is_worked_example": false, "text": "Still, the idea of using a larger number of smaller, more efficient cores seems attractive."}, {"start": "00:01:31", "is_lecture": true, "end": "00:01:39", "is_worked_example": false, "text": "Many applications have some computations that can be performed in parallel, but also have computations that won't benefit from parallelism."}, {"start": "00:01:39", "is_lecture": true, "end": "00:01:52", "is_worked_example": false, "text": "To understand the speedup we might expect from exploiting parallelism, it's useful to perform the calculation proposed by computer scientist Gene Amdahl in 1967, now known as Amdahl's Law."}, {"start": "00:01:52", "is_lecture": true, "end": "00:01:58", "is_worked_example": false, "text": "Suppose we're considering an enhancement that speeds up some fraction F of the task at hand by a factor of S."}, {"start": "00:01:58", "is_lecture": true, "end": "00:02:06", "is_worked_example": false, "text": "As shown in the figure, the gray portion of the task now takes F/S of the time that it used to require."}, {"start": "00:02:06", "is_lecture": true, "end": "00:02:12", "is_worked_example": false, "text": "Some simple arithmetic lets us calculate the overall speedup we get from using the enhancement."}, {"start": "00:02:12", "is_lecture": true, "end": "00:02:25", "is_worked_example": false, "text": "One conclusion we can draw is that we'll benefit the most from enhancements that affect a large portion of the required computations, i.e., we want to make F as large a possible."}, {"start": "00:02:25", "is_lecture": true, "end": "00:02:31", "is_worked_example": false, "text": "What's the best speedup we can hope for if we have many cores that can be used to speed up the parallel part of the task?"}, {"start": "00:02:31", "is_lecture": true, "end": "00:02:39", "is_worked_example": false, "text": "Here's the speedup formula based on F and S, where in this case F is the parallel fraction of the task."}, {"start": "00:02:39", "is_lecture": true, "end": "00:02:50", "is_worked_example": false, "text": "If we assume that the parallel fraction of the task can be speed up arbitrarily by using more and more cores, we see that the best possible overall speed up is 1/(1-F)."}, {"start": "00:02:50", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "For example, you write a program that can do 90% of its work in parallel, but the other 10% must be done sequentially."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:06", "is_worked_example": false, "text": "The best overall speedup that can be achieved is a factor of 10, no matter how many cores you have at your disposal."}, {"start": "00:03:06", "is_lecture": true, "end": "00:03:16", "is_worked_example": false, "text": "Turning the question around, suppose you have a 1000-core machine which you hope to be able to use to achieve a speedup of 500 on your target application."}, {"start": "00:03:16", "is_lecture": true, "end": "00:03:23", "is_worked_example": false, "text": "You would need to be able parallelize 99.8% of the computation in order to reach your goal!"}, {"start": "00:03:23", "is_lecture": true, "end": "00:03:30", "is_worked_example": false, "text": "Clearly multicore machines are most useful when the target task has lots of natural parallelism."}, {"start": "00:03:30", "is_lecture": true, "end": "00:03:41", "is_worked_example": false, "text": "Using multiple independent cores to execute a parallel task is called thread-level parallelism (TLP), where each core executes a separate computation \"thread\"."}, {"start": "00:03:41", "is_lecture": true, "end": "00:03:50", "is_worked_example": false, "text": "The threads are independent programs, so the execution model is potentially more flexible than the lock-step execution provided by vector machines."}, {"start": "00:03:50", "is_lecture": true, "end": "00:04:01", "is_worked_example": false, "text": "When there are a small number of threads, you often see the cores sharing a common main memory, allowing the threads to communicate and synchronize by sharing a common address space."}, {"start": "00:04:01", "is_lecture": true, "end": "00:04:04", "is_worked_example": false, "text": "We'll discuss this further in the next section."}, {"start": "00:04:04", "is_lecture": true, "end": "00:04:10", "is_worked_example": false, "text": "This is the approach used in current multicore processors, which have between 2 and 12 cores."}, {"start": "00:04:10", "is_lecture": true, "end": "00:04:20", "is_worked_example": false, "text": "Shared memory becomes a real bottleneck when there 10's or 100's of cores, since collectively they quickly overwhelm the available memory bandwidth."}, {"start": "00:04:20", "is_lecture": true, "end": "00:04:27", "is_worked_example": false, "text": "In these architectures, threads communicate using a communication network to pass messages back and forth."}, {"start": "00:04:27", "is_lecture": true, "end": "00:04:31", "is_worked_example": false, "text": "We discussed possible network topologies in an earlier lecture."}, {"start": "00:04:31", "is_lecture": true, "end": "00:04:45", "is_worked_example": false, "text": "A cost-effective on-chip approach is to use a nearest-neighbor mesh network, which supports many parallel point-to-point communications, while still allowing multi-hop communication between any two cores."}, {"start": "00:04:45", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "Message passing is also used in computing clusters, where many ordinary CPUs collaborate on large tasks."}, {"start": "00:04:53", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "There's a standardized message passing interface (MPI) and"}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:09", "is_worked_example": false, "text": "specialized, very high throughput, low latency message-passing communication networks (e.g., Infiniband) that make it easy to build high-performance computing clusters."}, {"start": "00:05:09", "is_lecture": true, "end": "00:05:17", "is_worked_example": false, "text": "In the next couple of sections we'll look more closely at some of the issues involved in building shared-memory multicore processors."}]}, "C08S01B01-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c8/c8s1/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c8s1v1", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:05", "is_worked_example": false, "text": "Let's turn our attention to how the operating system (OS) deals with input/output devices."}, {"start": "00:00:05", "is_lecture": true, "end": "00:00:08", "is_worked_example": false, "text": "There are actually two parts to the discussion."}, {"start": "00:00:08", "is_lecture": true, "end": "00:00:13", "is_worked_example": false, "text": "First, we'll talk about how the OS interacts with the devices themselves."}, {"start": "00:00:13", "is_lecture": true, "end": "00:00:17", "is_worked_example": false, "text": "This will involve a combination of interrupt handlers and kernel buffers."}, {"start": "00:00:17", "is_lecture": true, "end": "00:00:25", "is_worked_example": false, "text": "Then we'll discuss how supervisor calls access the kernel buffers in response to requests from user-mode processes."}, {"start": "00:00:25", "is_lecture": true, "end": "00:00:33", "is_worked_example": false, "text": "As we'll see, this can get a bit tricky when the OS cannot complete the request at the time the SVC was executed."}, {"start": "00:00:33", "is_lecture": true, "end": "00:00:35", "is_worked_example": false, "text": "Here's the plan!"}, {"start": "00:00:35", "is_lecture": true, "end": "00:00:40", "is_worked_example": false, "text": "When the user types a key on the keyboard, the keyboard triggers an interrupt request to the CPU."}, {"start": "00:00:40", "is_lecture": true, "end": "00:00:49", "is_worked_example": false, "text": "The interrupt suspends execution of the currently-running process and executes the handler whose job it is to deal with this particular I/O event."}, {"start": "00:00:49", "is_lecture": true, "end": "00:00:59", "is_worked_example": false, "text": "In this case, the keyboard handler reads the character from the keyboard and saves it in a kernel buffer associated with the process that has been chosen to receive incoming keystrokes."}, {"start": "00:00:59", "is_lecture": true, "end": "00:01:05", "is_worked_example": false, "text": "In the language of OSes, we'd say that process has the keyboard focus."}, {"start": "00:01:05", "is_lecture": true, "end": "00:01:12", "is_worked_example": false, "text": "This transfer takes just a handful of instructions and when the handler exits, we resume running the interrupted process."}, {"start": "00:01:12", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "Assuming the interrupt request is serviced promptly, the CPU can easily keep up with the arrival of typed characters."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:24", "is_worked_example": false, "text": "Humans are pretty slow compared to the rate of executing instructions!"}, {"start": "00:01:24", "is_lecture": true, "end": "00:01:29", "is_worked_example": false, "text": "But the buffer in the kernel can hold only so many characters before it fills up."}, {"start": "00:01:29", "is_lecture": true, "end": "00:01:31", "is_worked_example": false, "text": "What happens then?"}, {"start": "00:01:31", "is_lecture": true, "end": "00:01:33", "is_worked_example": false, "text": "Well, there are a couple of choices."}, {"start": "00:01:33", "is_lecture": true, "end": "00:01:41", "is_worked_example": false, "text": "Overwriting characters received earlier doesn't make much sense: why keep later characters if the earlier ones have been discarded."}, {"start": "00:01:41", "is_lecture": true, "end": "00:01:49", "is_worked_example": false, "text": "Better that the CPU discard any characters received after the buffer was full, but it should give some indication that it's doing so."}, {"start": "00:01:49", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "And, in fact, many systems beep at the user to signal that the character they've just typed is being ignored."}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "At some later time, a user-mode program executes a ReadKey() supervisor call, requesting that the OS return the next character in R0."}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:19", "is_worked_example": false, "text": "In the OS, the ReadKey SVC handler grabs the next character from the buffer, places it in the user's R0, and resumes execution at the instruction following the SVC."}, {"start": "00:02:19", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "There are few tricky bits we need to figure out."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:35", "is_worked_example": false, "text": "The ReadKey() SVC is what we call a \"blocking I/O\" request, i.e., the program assumes that when the SVC returns, the next character is in R0."}, {"start": "00:02:35", "is_lecture": true, "end": "00:02:44", "is_worked_example": false, "text": "If there isn't (yet) a character to be returned, execution should be \"blocked\", i.e., suspended, until such time that a character is available."}, {"start": "00:02:44", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "Many OSes also provide for non-blocking I/O requests, which always return immediately with both a status flag and a result."}, {"start": "00:02:53", "is_lecture": true, "end": "00:03:02", "is_worked_example": false, "text": "The program can check the status flag to see if there was a character and do the right thing if there wasn't, e.g., reissue the request at a later time."}, {"start": "00:03:02", "is_lecture": true, "end": "00:03:21", "is_worked_example": false, "text": "Note that the user-mode program didn't have any direct interaction with the keyboard, i.e., it's not constantly polling the device to see if there's a keystroke to be processed. .312 Instead, we're using an \"event-driven\" approach, where the device signals the OS, via an interrupt, when it needs attention."}, {"start": "00:03:21", "is_lecture": true, "end": "00:03:24", "is_worked_example": false, "text": "This is an elegant separation of responsibilities."}, {"start": "00:03:24", "is_lecture": true, "end": "00:03:32", "is_worked_example": false, "text": "Imagine how cumbersome it would be if every program had to check constantly to see if there were pending I/O operations."}, {"start": "00:03:32", "is_lecture": true, "end": "00:03:43", "is_worked_example": false, "text": "Our event-driven organization provides for on-demand servicing of devices, but doesn't devote CPU resources to the I/O subsystem until there's actually work to be done."}, {"start": "00:03:43", "is_lecture": true, "end": "00:03:50", "is_worked_example": false, "text": "The interrupt-driven OS interactions with I/O devices are completely transparent to user programs."}, {"start": "00:03:50", "is_lecture": true, "end": "00:03:56", "is_worked_example": false, "text": "Here's sketch of what the OS keyboard handler code might actually look like."}, {"start": "00:03:56", "is_lecture": true, "end": "00:04:04", "is_worked_example": false, "text": "Depending on the hardware, the CPU might access device status and data using special I/O instructions in the ISA."}, {"start": "00:04:04", "is_lecture": true, "end": "00:04:15", "is_worked_example": false, "text": "For example, in the simulated Beta used for lab assignments, there's a RDCHAR() instruction for reading keyboard characters and a CLICK() instruction for reading the coordinates of a mouse click."}, {"start": "00:04:15", "is_lecture": true, "end": "00:04:25", "is_worked_example": false, "text": "Another common approach is to use \"memory-mapped I/O\", where a portion of the kernel address space is devoted to servicing I/O devices."}, {"start": "00:04:25", "is_lecture": true, "end": "00:04:38", "is_worked_example": false, "text": "In this scheme, ordinary LD and ST store instructions are used to access specific addresses, which the CPU recognizes as accesses to the keyboard or mouse device interfaces."}, {"start": "00:04:38", "is_lecture": true, "end": "00:04:41", "is_worked_example": false, "text": "This is the scheme shown in the code here."}, {"start": "00:04:41", "is_lecture": true, "end": "00:04:51", "is_worked_example": false, "text": "The C data structure represents the two I/O locations devoted to the keyboard: one for status and one for the actual keyboard data."}, {"start": "00:04:51", "is_lecture": true, "end": "00:05:02", "is_worked_example": false, "text": "The keyboard interrupt handler reads the keystroke data from the keyboard and places the character into the next location in the circular character buffer in the kernel."}, {"start": "00:05:02", "is_lecture": true, "end": "00:05:08", "is_worked_example": false, "text": "In real life keyboard processing is usually a bit more complicated."}, {"start": "00:05:08", "is_lecture": true, "end": "00:05:16", "is_worked_example": false, "text": "What one actually reads from a keyboard is a key number and a flag indicating whether the event is a key press or a key release."}, {"start": "00:05:16", "is_lecture": true, "end": "00:05:23", "is_worked_example": false, "text": "Knowing the keyboard layout, the OS translates the key number into the appropriate ASCII character,"}, {"start": "00:05:23", "is_lecture": true, "end": "00:05:30", "is_worked_example": false, "text": "dealing with complications like holding down the shift key or control key to indicate a capital character or a control character."}, {"start": "00:05:30", "is_lecture": true, "end": "00:05:43", "is_worked_example": false, "text": "And certain combination of keystrokes, e.g., CTRL-ALT-DEL on a Windows system, are interpreted as special user commands to start running particular applications like the Task Manager."}, {"start": "00:05:43", "is_lecture": true, "end": "00:05:55", "is_worked_example": false, "text": "Many OSes let the user specify whether they want \"raw\" keyboard input (i.e., the key numbers and status) or \"digested\" input (i.e., ASCII characters)."}, {"start": "00:05:55", "is_lecture": true, "end": "00:06:00", "is_worked_example": false, "text": "Whew!  Who knew that processing keystrokes could be so complicated!"}, {"start": "00:06:00", "is_lecture": true, "end": "00:06:07", "is_worked_example": false, "text": "Next, we'll figure out how to code the associated supervisor call that lets user programs read characters."}]}, "C03S01B01-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c3/c3s1/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c3s1v1", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:12", "is_worked_example": false, "text": "In this lecture, we're going to use the circuit pipelining techniques we learned in Part 1 of the course to improve the performance of the 32-bit Beta CPU design we developed in Part 2 of the course."}, {"start": "00:00:12", "is_lecture": true, "end": "00:00:16", "is_worked_example": false, "text": "This CPU design executes one Beta instruction per clock cycle."}, {"start": "00:00:16", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "Hopefully you remember the design!"}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "If not, you might find it worthwhile to review Lecture 13, Building the Beta, from Part 2."}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:34", "is_worked_example": false, "text": "At the beginning of the clock cycle, this circuit loads a new value into the program counter, which is then sent to main memory as the address of the instruction to be executed this cycle."}, {"start": "00:00:34", "is_lecture": true, "end": "00:00:46", "is_worked_example": false, "text": "When the 32-bit word containing the binary encoding of the instruction is returned by the memory, the opcode field is decoded by the control logic to determine the control signals for the rest of the data path."}, {"start": "00:00:46", "is_lecture": true, "end": "00:00:53", "is_worked_example": false, "text": "The operands are read from the register file and routed to the ALU to perform the desired operation."}, {"start": "00:00:53", "is_lecture": true, "end": "00:01:04", "is_worked_example": false, "text": "For memory operations, the output of the ALU serves as the memory address and, in the case of load instructions, the main memory supplies the data to be written into the register file at the end of the cycle."}, {"start": "00:01:04", "is_lecture": true, "end": "00:01:08", "is_worked_example": false, "text": "PC+4 and ALU values can also be written to the register file."}, {"start": "00:01:08", "is_lecture": true, "end": "00:01:16", "is_worked_example": false, "text": "The clock period of the Beta is determined by the cumulative delay through all the components involved in instruction execution."}, {"start": "00:01:16", "is_lecture": true, "end": "00:01:19", "is_worked_example": false, "text": "Today's question is: how can we make this faster?"}, {"start": "00:01:19", "is_lecture": true, "end": "00:01:24", "is_worked_example": false, "text": "We can characterize the time spent executing a program as the product of three terms."}, {"start": "00:01:24", "is_lecture": true, "end": "00:01:28", "is_worked_example": false, "text": "The first term is the total number of instructions executed."}, {"start": "00:01:28", "is_lecture": true, "end": "00:01:35", "is_worked_example": false, "text": "Since the program usually contains loops and procedure calls, many of the encoded instructions will be executed many times."}, {"start": "00:01:35", "is_lecture": true, "end": "00:01:43", "is_worked_example": false, "text": "We want the total count of instructions executed, not the static size of the program as measured by the number of encoded instructions in memory."}, {"start": "00:01:43", "is_lecture": true, "end": "00:01:49", "is_worked_example": false, "text": "The second term is the average number of clock cycles it takes to execute a single instruction."}, {"start": "00:01:49", "is_lecture": true, "end": "00:01:53", "is_worked_example": false, "text": "And the third term is the duration of a single clock cycle."}, {"start": "00:01:53", "is_lecture": true, "end": "00:02:04", "is_worked_example": false, "text": "As CPU designers it's the last two terms which are under our control: the cycles per instruction (CPI) and the clock period (t_CLK)."}, {"start": "00:02:04", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "To affect the first term, we would need to change the ISA or write a better compiler!"}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:17", "is_worked_example": false, "text": "Our design for the Beta was able to execute every instruction in a single clock cycle, so our CPI is 1."}, {"start": "00:02:17", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "As we discussed in the previous slide, t_CLK is determined by the longest path through the Beta circuitry."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:31", "is_worked_example": false, "text": "For example, consider the execution of an OP-class instruction, which involves two register operands and an ALU operation."}, {"start": "00:02:31", "is_lecture": true, "end": "00:02:36", "is_worked_example": false, "text": "The arrow shows all the components that are involved in the execution of the instruction."}, {"start": "00:02:36", "is_lecture": true, "end": "00:02:43", "is_worked_example": false, "text": "Aside from a few muxes, the main memory, register file, and ALU must all have time to do their thing."}, {"start": "00:02:43", "is_lecture": true, "end": "00:02:49", "is_worked_example": false, "text": "The worst-case execution time is for the LD instruction."}, {"start": "00:02:49", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "In one clock cycle we need to fetch the instruction from main memory (t_IFETCH),"}, {"start": "00:02:53", "is_lecture": true, "end": "00:02:56", "is_worked_example": false, "text": "read the operands from the register file (t_RF),"}, {"start": "00:02:56", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "perform the address addition in the ALU (t_ALU),"}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:01", "is_worked_example": false, "text": "read the requested location from main memory (t_MEM),"}, {"start": "00:03:01", "is_lecture": true, "end": "00:03:06", "is_worked_example": false, "text": "and finally write the memory data to the destination register (t_WB)."}, {"start": "00:03:06", "is_lecture": true, "end": "00:03:15", "is_worked_example": false, "text": "The component delays add up and the result is a fairly long clock period and hence it will take a long time to run the program."}, {"start": "00:03:15", "is_lecture": true, "end": "00:03:19", "is_worked_example": false, "text": "And our two example execution paths illustrate another issue:"}, {"start": "00:03:19", "is_lecture": true, "end": "00:03:31", "is_worked_example": false, "text": "we're forced to choose the clock period to accommodate the worst-case execution time, even though we may be able to execute some instructions faster since their execution path through the circuitry is shorter."}, {"start": "00:03:31", "is_lecture": true, "end": "00:03:38", "is_worked_example": false, "text": "We're making all the instructions slower just because there's one instruction that has a long critical path."}, {"start": "00:03:38", "is_lecture": true, "end": "00:03:50", "is_worked_example": false, "text": "So why not have simple instructions execute in one clock cycle and more complex instructions take multiple cycles instead of forcing all instructions to execute in a single, long clock cycle?"}, {"start": "00:03:50", "is_lecture": true, "end": "00:03:59", "is_worked_example": false, "text": "As we'll see in the next few slides, we have a good answer to this question, one that will allow us to execute *all* instructions with a short clock period."}, {"start": "00:03:59", "is_lecture": true, "end": "00:04:02", "is_worked_example": false, "text": "We're going to use pipelining to address these issues."}, {"start": "00:04:02", "is_lecture": true, "end": "00:04:10", "is_worked_example": false, "text": "We're going to divide the execution of an instruction into a sequence of steps, where each step is performed in successive stages of the pipeline."}, {"start": "00:04:10", "is_lecture": true, "end": "00:04:17", "is_worked_example": false, "text": "So it will take multiple clock cycles to execute an instruction as it travels through the stages of the execution pipeline."}, {"start": "00:04:17", "is_lecture": true, "end": "00:04:26", "is_worked_example": false, "text": "But since there are only one or two components in each stage of the pipeline, the clock period can be much shorter and the throughput of the CPU can be much higher."}, {"start": "00:04:26", "is_lecture": true, "end": "00:04:32", "is_worked_example": false, "text": "The increased throughput is the result of overlapping the execution of consecutive instructions."}, {"start": "00:04:32", "is_lecture": true, "end": "00:04:40", "is_worked_example": false, "text": "At any given time, there will be multiple instructions in the CPU, each at a different stage of its execution."}, {"start": "00:04:40", "is_lecture": true, "end": "00:04:50", "is_worked_example": false, "text": "The time to execute all the steps for a particular instruction (i.e., the instruction latency) may be somewhat higher than in our unpipelined implementation."}, {"start": "00:04:50", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "But we will finish the last step of executing some instruction in each clock cycle, so the instruction throughput is 1 per clock cycle."}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:05", "is_worked_example": false, "text": "And since the clock cycle of our pipelined CPU is quite a bit shorter, the instruction throughput is quite a bit higher."}, {"start": "00:05:05", "is_lecture": true, "end": "00:05:10", "is_worked_example": false, "text": "All this sounds great, but, not surprisingly, there are few issues we'll have to deal with."}, {"start": "00:05:10", "is_lecture": true, "end": "00:05:14", "is_worked_example": false, "text": "There are many ways to pipeline the execution of an instruction."}, {"start": "00:05:14", "is_lecture": true, "end": "00:05:25", "is_worked_example": false, "text": "We're going to look at the design of the classic 5-stage instruction execution pipeline, which was widely used in the integrated circuit CPU designs of the 1980's."}, {"start": "00:05:25", "is_lecture": true, "end": "00:05:33", "is_worked_example": false, "text": "The 5 pipeline stages correspond to the steps of executing an instruction in a von-Neumann stored-program architecture."}, {"start": "00:05:33", "is_lecture": true, "end": "00:05:41", "is_worked_example": false, "text": "The first stage (IF) is responsible for fetching the binary-encoded instruction from the main memory location indicated by the program counter."}, {"start": "00:05:41", "is_lecture": true, "end": "00:05:50", "is_worked_example": false, "text": "The 32-bit instruction is passed to the register file stage (RF) where the required register operands are read from the register file."}, {"start": "00:05:50", "is_lecture": true, "end": "00:05:58", "is_worked_example": false, "text": "The operand values are passed to the ALU stage (ALU), which performs the requested operation."}, {"start": "00:05:58", "is_lecture": true, "end": "00:06:12", "is_worked_example": false, "text": "The memory stage (MEM) performs the second access to main memory to read or write the data for LD, LDR, or ST instructions, using the value from the ALU stage as the memory address."}, {"start": "00:06:12", "is_lecture": true, "end": "00:06:18", "is_worked_example": false, "text": "For load instructions, the output of the MEM stage is the read data from main memory."}, {"start": "00:06:18", "is_lecture": true, "end": "00:06:23", "is_worked_example": false, "text": "For all other instructions, the output of the MEM stage is simply the value from the ALU stage."}, {"start": "00:06:23", "is_lecture": true, "end": "00:06:32", "is_worked_example": false, "text": "In the final write-back stage (WB), the result from the earlier stages is written to the destination register in the register file."}, {"start": "00:06:32", "is_lecture": true, "end": "00:06:41", "is_worked_example": false, "text": "Looking at the execution path from the previous slide, we see that each of the main components of the unpipelined design is now in its own pipeline stage."}, {"start": "00:06:41", "is_lecture": true, "end": "00:06:46", "is_worked_example": false, "text": "So the clock period will now be determined by the slowest of these components."}, {"start": "00:06:46", "is_lecture": true, "end": "00:06:55", "is_worked_example": false, "text": "Having divided instruction execution into five stages, would we expect the clock period to be one fifth of its original value?"}, {"start": "00:06:55", "is_lecture": true, "end": "00:07:03", "is_worked_example": false, "text": "Well, that would only happen if we were able to divide the execution so that each stage performed exactly one fifth of the total work."}, {"start": "00:07:03", "is_lecture": true, "end": "00:07:14", "is_worked_example": false, "text": "In real life, the major components have somewhat different latencies, so the improvement in instruction throughput will be a little less than the factor of 5 a perfect 5-stage pipeline could achieve."}, {"start": "00:07:14", "is_lecture": true, "end": "00:07:26", "is_worked_example": false, "text": "If we have a slow component, e.g., the ALU, we might choose to pipeline that component into further stages, or, interleave multiple ALUs to achieve the same effect."}, {"start": "00:07:26", "is_lecture": true, "end": "00:07:31", "is_worked_example": false, "text": "But for this lecture, we'll go with the 5-stage pipeline described above."}, {"start": "00:07:31", "is_lecture": true, "end": "00:07:34", "is_worked_example": false, "text": "So why isn't this a 20-minute lecture?"}, {"start": "00:07:34", "is_lecture": true, "end": "00:07:37", "is_worked_example": false, "text": "After all we know how pipeline combinational circuits:"}, {"start": "00:07:37", "is_lecture": true, "end": "00:07:47", "is_worked_example": false, "text": "We can build a valid k-stage pipeline by drawing k contours across the circuit diagram and adding a pipeline register wherever a contour crosses a signal."}, {"start": "00:07:47", "is_lecture": true, "end": "00:07:48", "is_worked_example": false, "text": "What's the big deal here?"}, {"start": "00:07:48", "is_lecture": true, "end": "00:07:52", "is_worked_example": false, "text": "Well, is this circuit combinational?  No!"}, {"start": "00:07:52", "is_lecture": true, "end": "00:07:54", "is_worked_example": false, "text": "There's state in the registers and memories."}, {"start": "00:07:54", "is_lecture": true, "end": "00:08:00", "is_worked_example": false, "text": "This means that the result of executing a given instruction may depend on the results from earlier instructions."}, {"start": "00:08:00", "is_lecture": true, "end": "00:08:08", "is_worked_example": false, "text": "There are loops in the circuit where data from later pipeline stages affects the execution of earlier pipeline stages."}, {"start": "00:08:08", "is_lecture": true, "end": "00:08:16", "is_worked_example": false, "text": "For example, the write to the register file at the end of the WB stage will change values read from the register file in the RF stage."}, {"start": "00:08:16", "is_lecture": true, "end": "00:08:27", "is_worked_example": false, "text": "In other words, there are execution dependencies between instructions and these dependencies will need to be taken into account when we're trying to pipeline instruction execution."}, {"start": "00:08:27", "is_lecture": true, "end": "00:08:32", "is_worked_example": false, "text": "We'll be addressing these issues as we examine the operation of our execution pipeline."}, {"start": "00:08:32", "is_lecture": true, "end": "00:08:38", "is_worked_example": false, "text": "Sometimes execution of a given instruction will depend on the results of executing a previous instruction."}, {"start": "00:08:38", "is_lecture": true, "end": "00:08:41", "is_worked_example": false, "text": "Two are two types of problematic dependencies."}, {"start": "00:08:41", "is_lecture": true, "end": "00:08:50", "is_worked_example": false, "text": "The first, termed a data hazard, occurs when the execution of the current instruction depends on data produced by an earlier instruction."}, {"start": "00:08:50", "is_lecture": true, "end": "00:08:57", "is_worked_example": false, "text": "For example, an instruction that reads R0 will depend on the execution of an earlier instruction that wrote R0."}, {"start": "00:08:57", "is_lecture": true, "end": "00:09:04", "is_worked_example": false, "text": "The second, termed a control hazard, occurs when a branch, jump, or exception changes the order of execution."}, {"start": "00:09:04", "is_lecture": true, "end": "00:09:11", "is_worked_example": false, "text": "For example, the choice of which instruction to execute after a BNE depends on whether the branch is taken or not."}, {"start": "00:09:11", "is_lecture": true, "end": "00:09:23", "is_worked_example": false, "text": "Instruction execution triggers a hazard when the instruction on which it depends is also in the pipeline, i.e., the earlier instruction hasn't finished execution!"}, {"start": "00:09:23", "is_lecture": true, "end": "00:09:28", "is_worked_example": false, "text": "We'll need to adjust execution in our pipeline to avoid these hazards."}, {"start": "00:09:28", "is_lecture": true, "end": "00:09:30", "is_worked_example": false, "text": "Here's our plan of attack:"}, {"start": "00:09:30", "is_lecture": true, "end": "00:09:42", "is_worked_example": false, "text": "We'll start by designing a 5-stage pipeline that works with sequences of instructions that don't trigger hazards, i.e., where instruction execution doesn't depend on earlier instructions still in the pipeline."}, {"start": "00:09:42", "is_lecture": true, "end": "00:09:46", "is_worked_example": false, "text": "Then we'll fix our pipeline to deal correctly with data hazards."}, {"start": "00:09:46", "is_lecture": true, "end": "00:09:50", "is_worked_example": false, "text": "And finally, we'll address control hazards."}]}, "C09S01B02-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c9/c9s1/2?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c9s1v2", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:06", "is_worked_example": false, "text": "What we'd like to do is to create a single abstraction that can be used to address all our synchronization needs."}, {"start": "00:00:06", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "In the early 1960's, the Dutch computer scientist Edsger Dijkstra proposed a new abstract data type called the semaphore, which has an integer value greater than or equal to 0."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "A programmer can declare a semaphore as shown here, specifying its initial value."}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:31", "is_worked_example": false, "text": "The semaphore lives in a memory location shared by all the processes that need to synchronize their operation."}, {"start": "00:00:31", "is_lecture": true, "end": "00:00:36", "is_worked_example": false, "text": "The semaphore is accessed with two operations: WAIT and SIGNAL."}, {"start": "00:00:36", "is_lecture": true, "end": "00:00:46", "is_worked_example": false, "text": "The WAIT operation will wait until the specified semaphore has a value greater than 0, then it will decrement the semaphore value and return to the calling program."}, {"start": "00:00:46", "is_lecture": true, "end": "00:00:54", "is_worked_example": false, "text": "If the semaphore value is 0 when WAIT is called, conceptually execution is suspended until the semaphore value is non-zero."}, {"start": "00:00:54", "is_lecture": true, "end": "00:01:04", "is_worked_example": false, "text": "In a simple (inefficient) implementation, the WAIT routine loops, periodically testing the value of the semaphore, proceeding when its value is non-zero."}, {"start": "00:01:04", "is_lecture": true, "end": "00:01:08", "is_worked_example": false, "text": "The SIGNAL operation increments the value of the specified semaphore."}, {"start": "00:01:08", "is_lecture": true, "end": "00:01:14", "is_worked_example": false, "text": "If there any processes WAITing on that semaphore, exactly one of them may now proceed."}, {"start": "00:01:14", "is_lecture": true, "end": "00:01:21", "is_worked_example": false, "text": "We'll have to be careful with the implementation of SIGNAL and WAIT to ensure that the \"exactly one\" constraint is satisfied,"}, {"start": "00:01:21", "is_lecture": true, "end": "00:01:29", "is_worked_example": false, "text": "i.e., that two processes both WAITing on the same semaphore won't both think they can decrement it and proceed after a SIGNAL."}, {"start": "00:01:29", "is_lecture": true, "end": "00:01:38", "is_worked_example": false, "text": "A semaphore initialized with the value K guarantees that the i_th call to SIGNAL will precede (i+K)_th call to WAIT."}, {"start": "00:01:38", "is_lecture": true, "end": "00:01:42", "is_worked_example": false, "text": "In a moment, we'll see some concrete examples that will make this clear."}, {"start": "00:01:42", "is_lecture": true, "end": "00:01:47", "is_worked_example": false, "text": "Note that in 6.004, we're ruling out semaphores with negative values."}, {"start": "00:01:47", "is_lecture": true, "end": "00:01:56", "is_worked_example": false, "text": "In the literature, you may see P(s) used in place of WAIT(s) and V(s) used in place of SIGNAL(s)."}, {"start": "00:01:56", "is_lecture": true, "end": "00:02:02", "is_worked_example": false, "text": "These operation names are derived from the Dutch words for \"test\" and \"increase\"."}, {"start": "00:02:02", "is_lecture": true, "end": "00:02:05", "is_worked_example": false, "text": "Let's see how to use semaphores to implement precedence constraints."}, {"start": "00:02:05", "is_lecture": true, "end": "00:02:10", "is_worked_example": false, "text": "Here are two processes, each running a program with 5 statements."}, {"start": "00:02:10", "is_lecture": true, "end": "00:02:16", "is_worked_example": false, "text": "Execution proceeds sequentially within each process, so A1 executes before A2, and so on."}, {"start": "00:02:16", "is_lecture": true, "end": "00:02:28", "is_worked_example": false, "text": "But there are no constraints on the order of execution between the processes, so statement B1 in Process B might be executed before or after any of the statements in Process A."}, {"start": "00:02:28", "is_lecture": true, "end": "00:02:38", "is_worked_example": false, "text": "Even if A and B are running in a timeshared environment on a single physical processor, execution may switch at any time between processes A and B."}, {"start": "00:02:38", "is_lecture": true, "end": "00:02:47", "is_worked_example": false, "text": "Suppose we wish to impose the constraint that the execution of statement A2 completes before execution of statement B4 begins."}, {"start": "00:02:47", "is_lecture": true, "end": "00:02:50", "is_worked_example": false, "text": "The red arrow shows the constraint we want."}, {"start": "00:02:50", "is_lecture": true, "end": "00:02:56", "is_worked_example": false, "text": "Here's the recipe for implementing this sort of simple precedence constraint using semaphores."}, {"start": "00:02:56", "is_lecture": true, "end": "00:03:03", "is_worked_example": false, "text": "First, declare a semaphore (called \"s\" in this example) and initialize its value to 0."}, {"start": "00:03:03", "is_lecture": true, "end": "00:03:08", "is_worked_example": false, "text": "Place a call to signal(s) at the start of the arrow."}, {"start": "00:03:08", "is_lecture": true, "end": "00:03:13", "is_worked_example": false, "text": "In this example, signal(s) is placed after the statement A2 in process A."}, {"start": "00:03:13", "is_lecture": true, "end": "00:03:17", "is_worked_example": false, "text": "Then place a call to wait(s) at the end of the arrow."}, {"start": "00:03:17", "is_lecture": true, "end": "00:03:22", "is_worked_example": false, "text": "In this example, wait(s) is placed before the statement B4 in process B."}, {"start": "00:03:22", "is_lecture": true, "end": "00:03:31", "is_worked_example": false, "text": "With these modifications, process A executes as before, with the signal to semaphore s happening after statement A2 is executed."}, {"start": "00:03:31", "is_lecture": true, "end": "00:03:45", "is_worked_example": false, "text": "Statements B1 through B3 also execute as before, but when the wait(s) is executed, execution of process B is suspended until the signal(s) statement has finished execution."}, {"start": "00:03:45", "is_lecture": true, "end": "00:03:52", "is_worked_example": false, "text": "This guarantees that execution of B4 will start only after execution of A2 has completed."}, {"start": "00:03:52", "is_lecture": true, "end": "00:04:03", "is_worked_example": false, "text": "By initializing the semaphore s to 0, we enforced the constraint that the first call to signal(s) had to complete before the first call to wait(s) would succeed."}, {"start": "00:04:03", "is_lecture": true, "end": "00:04:12", "is_worked_example": false, "text": "Another way to think about semaphores is as a management tool for a shared pool of K resources, where K is the initial value of the semaphore."}, {"start": "00:04:12", "is_lecture": true, "end": "00:04:18", "is_worked_example": false, "text": "You use the SIGNAL operation to add or return resources to the shared pool."}, {"start": "00:04:18", "is_lecture": true, "end": "00:04:22", "is_worked_example": false, "text": "And you use the WAIT operation to allocate a resource for your exclusive use."}, {"start": "00:04:22", "is_lecture": true, "end": "00:04:30", "is_worked_example": false, "text": "At any given time, the value of the semaphore gives the number of unallocated resources still available in the shared pool."}, {"start": "00:04:30", "is_lecture": true, "end": "00:04:41", "is_worked_example": false, "text": "Note that the WAIT and SIGNAL operations can be in the same process, or they may be in different processes, depending on when the resource is allocated and returned."}, {"start": "00:04:41", "is_lecture": true, "end": "00:04:45", "is_worked_example": false, "text": "We can use semaphores to manage our N-character FIFO buffer."}, {"start": "00:04:45", "is_lecture": true, "end": "00:04:49", "is_worked_example": false, "text": "Here we've defined a semaphore CHARS and initialized it to 0."}, {"start": "00:04:49", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "The value of CHARS will tell us how many characters are in the buffer."}, {"start": "00:04:53", "is_lecture": true, "end": "00:05:01", "is_worked_example": false, "text": "So SEND does a signal(CHARS) after it has added a character to the buffer, indicating the buffer now contains an additional character."}, {"start": "00:05:01", "is_lecture": true, "end": "00:05:08", "is_worked_example": false, "text": "And RCV does a wait(CHARS) to ensure the buffer has at least one character before reading from the buffer."}, {"start": "00:05:08", "is_lecture": true, "end": "00:05:17", "is_worked_example": false, "text": "Since CHARS was initialized to 0, we've enforced the constraint that the i_th call to signal(CHARS) precedes the completion of the i_th call to wait(CHARS)."}, {"start": "00:05:17", "is_lecture": true, "end": "00:05:23", "is_worked_example": false, "text": "In other words, RCV can't consume a character until it has been placed in the buffer by SEND."}, {"start": "00:05:23", "is_lecture": true, "end": "00:05:28", "is_worked_example": false, "text": "Does this mean our producer and consumer are now properly synchronized?"}, {"start": "00:05:28", "is_lecture": true, "end": "00:05:35", "is_worked_example": false, "text": "Using the CHARS semaphore, we implemented *one* of the two precedence constraints we identified as being necessary for correct operation."}, {"start": "00:05:35", "is_lecture": true, "end": "00:05:39", "is_worked_example": false, "text": "Next we'll see how to implement the other precedence constraint."}, {"start": "00:05:39", "is_lecture": true, "end": "00:05:45", "is_worked_example": false, "text": "What keeps the producer from putting more than N characters into the N-character buffer?"}, {"start": "00:05:45", "is_lecture": true, "end": "00:05:47", "is_worked_example": false, "text": "Nothing."}, {"start": "00:05:47", "is_lecture": true, "end": "00:05:55", "is_worked_example": false, "text": "Oops, the producer can start to overwrite characters placed in the buffer earlier even though they haven't yet been read by the consumer."}, {"start": "00:05:55", "is_lecture": true, "end": "00:06:03", "is_worked_example": false, "text": "This is called buffer overflow and the sequence of characters transmitted from producer to consumer becomes hopelessly corrupted."}, {"start": "00:06:03", "is_lecture": true, "end": "00:06:14", "is_worked_example": false, "text": "What we've guaranteed so far is that the consumer can read a character only after the producer has placed it in the buffer, i.e., the consumer can't read from an empty buffer."}, {"start": "00:06:14", "is_lecture": true, "end": "00:06:19", "is_worked_example": false, "text": "What we still need to guarantee is that the producer can't get too far ahead of the consumer."}, {"start": "00:06:19", "is_lecture": true, "end": "00:06:29", "is_worked_example": false, "text": "Since the buffer holds at most N characters, the producer can't send the (i+N)th character until the consumer has read the i_th character."}, {"start": "00:06:29", "is_lecture": true, "end": "00:06:36", "is_worked_example": false, "text": "Here we've added a second semaphore, SPACES, to manage the number of spaces in the buffer."}, {"start": "00:06:36", "is_lecture": true, "end": "00:06:39", "is_worked_example": false, "text": "Initially the buffer is empty, so it has N spaces."}, {"start": "00:06:39", "is_lecture": true, "end": "00:06:43", "is_worked_example": false, "text": "The producer must WAIT for a space to be available."}, {"start": "00:06:43", "is_lecture": true, "end": "00:06:53", "is_worked_example": false, "text": "When SPACES in non-zero, the WAIT succeeds, decrementing the number of available spaces by one and then the producer fills that space with the next character."}, {"start": "00:06:53", "is_lecture": true, "end": "00:06:59", "is_worked_example": false, "text": "The consumer signals the availability of another space after it reads a character from the buffer."}, {"start": "00:06:59", "is_lecture": true, "end": "00:07:01", "is_worked_example": false, "text": "There's a nice symmetry here."}, {"start": "00:07:01", "is_lecture": true, "end": "00:07:05", "is_worked_example": false, "text": "The producer consumes spaces and produces characters."}, {"start": "00:07:05", "is_lecture": true, "end": "00:07:09", "is_worked_example": false, "text": "The consumer consumes characters and produces spaces."}, {"start": "00:07:09", "is_lecture": true, "end": "00:07:20", "is_worked_example": false, "text": "Semaphores are used to track the availability of both resources (i.e., characters and spaces), synchronizing the execution of the producer and consumer."}, {"start": "00:07:20", "is_lecture": true, "end": "00:07:24", "is_worked_example": false, "text": "This works great when there is a single producer process and a single consumer process."}, {"start": "00:07:24", "is_lecture": true, "end": "00:07:31", "is_worked_example": false, "text": "Next we'll think about what will happen if we have multiple producers and multiple consumers."}]}, "C12S01B01-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c12/c12s1/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c12s1v1", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:09", "is_worked_example": false, "text": "The modern world has an insatiable appetite for computation, so system architects are always thinking about ways to make programs run faster."}, {"start": "00:00:09", "is_lecture": true, "end": "00:00:12", "is_worked_example": false, "text": "The running time of a program is the product of three terms:"}, {"start": "00:00:12", "is_lecture": true, "end": "00:00:26", "is_worked_example": false, "text": "The number of instructions in the program, multiplied by the average number of processor cycles required to execute each instruction (CPI), multiplied by the time required for each processor cycle (t_CLK)."}, {"start": "00:00:26", "is_lecture": true, "end": "00:00:31", "is_worked_example": false, "text": "To decrease the running time we need to decrease one or more of these terms."}, {"start": "00:00:31", "is_lecture": true, "end": "00:00:39", "is_worked_example": false, "text": "The number of instructions per program is determined by the ISA and by the compiler that produced the sequence of assembly language instructions to be executed."}, {"start": "00:00:39", "is_lecture": true, "end": "00:00:45", "is_worked_example": false, "text": "Both are fair game, but for this discussion, let's work on reducing the other two terms."}, {"start": "00:00:45", "is_lecture": true, "end": "00:00:55", "is_worked_example": false, "text": "As we've seen, pipelining reduces t_CLK by dividing instruction execution into a sequence of steps, each of which can complete its task in a shorter t_CLK."}, {"start": "00:00:55", "is_lecture": true, "end": "00:00:58", "is_worked_example": false, "text": "What about reducing CPI?"}, {"start": "00:00:58", "is_lecture": true, "end": "00:01:09", "is_worked_example": false, "text": "In our 5-stage pipelined implementation of the Beta, we designed the hardware to complete the execution of one instruction every clock cycle, so CPI_ideal is 1."}, {"start": "00:01:09", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "But sometimes the hardware has to introduce \"NOP bubbles\" into the pipeline to delay execution of a pipeline stage if the required operation couldn't (yet) be completed."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "This happens on taken branch instructions, when attempting to immediately use a value loaded from memory by the LD instruction, and when waiting for a cache miss to be satisfied from main memory."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:38", "is_worked_example": false, "text": "CPI_stall accounts for the cycles lost to the NOPs introduced into the pipeline."}, {"start": "00:01:38", "is_lecture": true, "end": "00:01:44", "is_worked_example": false, "text": "Its value depends on the frequency of taken branches and immediate use of LD results."}, {"start": "00:01:44", "is_lecture": true, "end": "00:01:47", "is_worked_example": false, "text": "Typically it's some fraction of a cycle."}, {"start": "00:01:47", "is_lecture": true, "end": "00:01:59", "is_worked_example": false, "text": "For example, if a 6-instruction loop with a LD takes 8 cycles to complete, CPI_stall for the loop would be 2/6, i.e., 2 extra cycles for every 6 instructions."}, {"start": "00:01:59", "is_lecture": true, "end": "00:02:10", "is_worked_example": false, "text": "Our classic 5-stage pipeline is an effective compromise that allows for a substantial reduction of t_CLK while keeping CPI_stall to a reasonably modest value."}, {"start": "00:02:10", "is_lecture": true, "end": "00:02:12", "is_worked_example": false, "text": "There is room for improvement."}, {"start": "00:02:12", "is_lecture": true, "end": "00:02:18", "is_worked_example": false, "text": "Since each stage is working on one instruction at a time, CPI_ideal is 1."}, {"start": "00:02:18", "is_lecture": true, "end": "00:02:33", "is_worked_example": false, "text": "Slow operations -- e.g, completing a multiply in the ALU stage, or accessing a large cache in the IF or MEM stages -- force t_CLK to be large to accommodate all the work that has to be done in one cycle."}, {"start": "00:02:33", "is_lecture": true, "end": "00:02:37", "is_worked_example": false, "text": "The order of the instructions in the pipeline is fixed."}, {"start": "00:02:37", "is_lecture": true, "end": "00:02:50", "is_worked_example": false, "text": "If, say, a LD instruction is delayed in the MEM stage because of a cache miss, all the instructions in earlier stages are also delayed even though their execution may not depend on the value produced by the LD."}, {"start": "00:02:50", "is_lecture": true, "end": "00:02:56", "is_worked_example": false, "text": "The order of instructions in the pipeline always reflects the order in which they were fetched by the IF stage."}, {"start": "00:02:56", "is_lecture": true, "end": "00:03:02", "is_worked_example": false, "text": "Let's look into what it would take to relax these constraints and hopefully improve program runtimes."}, {"start": "00:03:02", "is_lecture": true, "end": "00:03:08", "is_worked_example": false, "text": "Increasing the number of pipeline stages should allow us to decrease the clock cycle time."}, {"start": "00:03:08", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "We'd add stages to break up performance bottlenecks, e.g., adding additional pipeline stages (MEM1 and MEM2) to allow a longer time for memory operations to complete."}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:30", "is_worked_example": false, "text": "This comes at cost to CPI_stall since each additional MEM stage means that more NOP bubbles have to be introduced when there's a LD data hazard."}, {"start": "00:03:30", "is_lecture": true, "end": "00:03:35", "is_worked_example": false, "text": "Deeper pipelines mean that the processor will be executing more instructions in parallel."}, {"start": "00:03:35", "is_lecture": true, "end": "00:03:41", "is_worked_example": false, "text": "Let's interrupt enumerating our performance shopping list to think about limits to pipeline depth."}, {"start": "00:03:41", "is_lecture": true, "end": "00:03:46", "is_worked_example": false, "text": "Each additional pipeline stage includes some additional overhead costs to the time budget."}, {"start": "00:03:46", "is_lecture": true, "end": "00:03:51", "is_worked_example": false, "text": "We have to account for the propagation, setup, and hold times for the pipeline registers."}, {"start": "00:03:51", "is_lecture": true, "end": "00:04:00", "is_worked_example": false, "text": "And we usually have to allow a bit of extra time to account for clock skew, i.e., the difference in arrival time of the clock edge at each register."}, {"start": "00:04:00", "is_lecture": true, "end": "00:04:10", "is_worked_example": false, "text": "And, finally, since we can't always divide the work exactly evenly between the pipeline stages, there will be some wasted time in the stages that have less work."}, {"start": "00:04:10", "is_lecture": true, "end": "00:04:15", "is_worked_example": false, "text": "We'll capture all of these effects as an additional per-stage time overhead of O."}, {"start": "00:04:15", "is_lecture": true, "end": "00:04:24", "is_worked_example": false, "text": "If the original clock period was T, then with N pipeline stages, the clock period will be T/N + O."}, {"start": "00:04:24", "is_lecture": true, "end": "00:04:30", "is_worked_example": false, "text": "At the limit, as N becomes large, the speedup approaches T/O."}, {"start": "00:04:30", "is_lecture": true, "end": "00:04:37", "is_worked_example": false, "text": "In other words, the overhead starts to dominate as the time spent on work in each stage becomes smaller and smaller."}, {"start": "00:04:37", "is_lecture": true, "end": "00:04:43", "is_worked_example": false, "text": "At some point adding additional pipeline stages has almost no impact on the clock period."}, {"start": "00:04:43", "is_lecture": true, "end": "00:04:51", "is_worked_example": false, "text": "As a data point, the Intel Core-2 x86 chips (nicknamed \"Nehalem\") have a 14-stage execution pipeline."}, {"start": "00:04:51", "is_lecture": true, "end": "00:04:54", "is_worked_example": false, "text": "Okay, back to our performance shopping list..."}, {"start": "00:04:54", "is_lecture": true, "end": "00:05:03", "is_worked_example": false, "text": "There may be times we can arrange to execute two or more instructions in parallel, assuming that their executions are independent from each other."}, {"start": "00:05:03", "is_lecture": true, "end": "00:05:12", "is_worked_example": false, "text": "This would increase CPI_ideal at the cost of increasing the complexity of each pipeline stage to deal with concurrent execution of multiple instructions."}, {"start": "00:05:12", "is_lecture": true, "end": "00:05:21", "is_worked_example": false, "text": "If there's an instruction stalled in the pipeline by a data hazard, there may be following instructions whose execution could still proceed."}, {"start": "00:05:21", "is_lecture": true, "end": "00:05:27", "is_worked_example": false, "text": "Allowing instructions to pass each other in the pipeline is called out-of-order execution."}, {"start": "00:05:27", "is_lecture": true, "end": "00:05:34", "is_worked_example": false, "text": "We'd have to be careful to ensure that changing the execution order didn't affect the values produced by the program."}, {"start": "00:05:34", "is_lecture": true, "end": "00:05:45", "is_worked_example": false, "text": "More pipeline stages and wider pipeline stages increase the amount of work that has to be discarded on control hazards, potentially increasing CPI_stall."}, {"start": "00:05:45", "is_lecture": true, "end": "00:05:54", "is_worked_example": false, "text": "So it's important to minimize the number of control hazards by predicting the results of a branch (i.e., taken or not taken)"}, {"start": "00:05:54", "is_lecture": true, "end": "00:06:00", "is_worked_example": false, "text": "so that we increase the chances that the instructions in the pipeline are the ones we'll want to execute."}, {"start": "00:06:00", "is_lecture": true, "end": "00:06:09", "is_worked_example": false, "text": "Our ability to exploit wider pipelines and out-of-order execution depends on finding instructions that can be executed in parallel or in different orders."}, {"start": "00:06:09", "is_lecture": true, "end": "00:06:14", "is_worked_example": false, "text": "Collectively these properties are called \"instruction-level parallelism\" (ILP)."}, {"start": "00:06:14", "is_lecture": true, "end": "00:06:21", "is_worked_example": false, "text": "Here's an example that will let us explore the amount of ILP that might be available."}, {"start": "00:06:21", "is_lecture": true, "end": "00:06:26", "is_worked_example": false, "text": "On the left is an unoptimized loop that computes the product of the first N integers."}, {"start": "00:06:26", "is_lecture": true, "end": "00:06:33", "is_worked_example": false, "text": "On the right, we've rewritten the code, placing instructions that could be executed concurrently on the same line."}, {"start": "00:06:33", "is_lecture": true, "end": "00:06:37", "is_worked_example": false, "text": "First notice the red line following the BF instruction."}, {"start": "00:06:37", "is_lecture": true, "end": "00:06:42", "is_worked_example": false, "text": "Instructions below the line should only be executed if the BF is *not* taken."}, {"start": "00:06:42", "is_lecture": true, "end": "00:06:46", "is_worked_example": false, "text": "That doesn't mean we couldn't start executing them before the results of the branch are known,"}, {"start": "00:06:46", "is_lecture": true, "end": "00:06:53", "is_worked_example": false, "text": "but if we executed them before the branch, we would have to be prepared to throw away their results if the branch was taken."}, {"start": "00:06:53", "is_lecture": true, "end": "00:07:00", "is_worked_example": false, "text": "The possible execution order is constrained by the read-after-write (RAW) dependencies shown by the red arrows."}, {"start": "00:07:00", "is_lecture": true, "end": "00:07:09", "is_worked_example": false, "text": "We recognize these as the potential data hazards that occur when an operand value for one instruction depends on the result of an earlier instruction."}, {"start": "00:07:09", "is_lecture": true, "end": "00:07:21", "is_worked_example": false, "text": "In our 5-stage pipeline, we were able to resolve many of these hazards by bypassing values from the ALU, MEM, and WB stages back to the RF stage where operand values are determined."}, {"start": "00:07:21", "is_lecture": true, "end": "00:07:29", "is_worked_example": false, "text": "Of course, bypassing will only work when the instruction has been executed so its result is available for bypassing!"}, {"start": "00:07:29", "is_lecture": true, "end": "00:07:37", "is_worked_example": false, "text": "So, in this case, the arrows are showing us the constraints on execution order that guarantee bypassing will be possible."}, {"start": "00:07:37", "is_lecture": true, "end": "00:07:40", "is_worked_example": false, "text": "There are other constraints on execution order."}, {"start": "00:07:40", "is_lecture": true, "end": "00:07:47", "is_worked_example": false, "text": "The green arrow identifies a write-after-write (WAW) constraint between two instructions with the same destination register."}, {"start": "00:07:47", "is_lecture": true, "end": "00:08:02", "is_worked_example": false, "text": "In order to ensure the correct value is in R2 at the end of the loop, the LD(r,R2) instruction has to store its result into the register file after the result of the CMPLT instruction is stored into the register file."}, {"start": "00:08:02", "is_lecture": true, "end": "00:08:11", "is_worked_example": false, "text": "Similarly, the blue arrow shows a write-after-read (WAR) constraint that ensures that the correct values are used when accessing a register."}, {"start": "00:08:11", "is_lecture": true, "end": "00:08:19", "is_worked_example": false, "text": "In this case, LD(r,R2) must store into R2 after the Ra operand for the BF has been read from R2."}, {"start": "00:08:19", "is_lecture": true, "end": "00:08:28", "is_worked_example": false, "text": "As it turns out, WAW and WAR constraints can be eliminated if we give each instruction result a unique register name."}, {"start": "00:08:28", "is_lecture": true, "end": "00:08:38", "is_worked_example": false, "text": "This can actually be done relatively easily by the hardware by using a generous supply of temporary registers, but we won't go into the details of renaming here."}, {"start": "00:08:38", "is_lecture": true, "end": "00:08:46", "is_worked_example": false, "text": "The use of temporary registers also makes it easy to discard results of instructions executed before we know the outcomes of branches."}, {"start": "00:08:46", "is_lecture": true, "end": "00:08:54", "is_worked_example": false, "text": "In this example, we discovered that the potential concurrency was actually pretty good for the instructions following the BF."}, {"start": "00:08:54", "is_lecture": true, "end": "00:09:02", "is_worked_example": false, "text": "To take advantage of this potential concurrency, we'll need to modify the pipeline to execute some number N of instructions in parallel."}, {"start": "00:09:02", "is_lecture": true, "end": "00:09:14", "is_worked_example": false, "text": "If we can sustain that rate of execution, CPI_ideal would then be 1/N since we'd complete the execution of N instructions in each clock cycle as they exited the final pipeline stage."}, {"start": "00:09:14", "is_lecture": true, "end": "00:09:17", "is_worked_example": false, "text": "So what value should we choose for N?"}, {"start": "00:09:17", "is_lecture": true, "end": "00:09:28", "is_worked_example": false, "text": "Instructions that are executed by different ALU hardware are easy to execute in parallel, e.g., ADDs and SHIFTs, or integer and floating-point operations."}, {"start": "00:09:28", "is_lecture": true, "end": "00:09:34", "is_worked_example": false, "text": "Of course, if we provided multiple adders, we could execute multiple integer arithmetic instructions concurrently."}, {"start": "00:09:34", "is_lecture": true, "end": "00:09:45", "is_worked_example": false, "text": "Having separate hardware for address arithmetic (called LD/ST units) would support concurrent execution of LD/ST instructions and integer arithmetic instructions."}, {"start": "00:09:45", "is_lecture": true, "end": "00:09:54", "is_worked_example": false, "text": "This set of lecture slides from Duke gives a nice overview of techniques used in each pipeline stage to support concurrent execution."}, {"start": "00:09:54", "is_lecture": true, "end": "00:10:02", "is_worked_example": false, "text": "Basically by increasing the number of functional units in the ALU and the number of memory ports on the register file and main memory,"}, {"start": "00:10:02", "is_lecture": true, "end": "00:10:06", "is_worked_example": false, "text": "we would have what it takes to support concurrent execution of multiple instructions."}, {"start": "00:10:06", "is_lecture": true, "end": "00:10:13", "is_worked_example": false, "text": "So, what's the right tradeoff between increased circuit costs and increased concurrency?"}, {"start": "00:10:13", "is_lecture": true, "end": "00:10:24", "is_worked_example": false, "text": "As a data point, the Intel Nehelam core can complete up to 4 micro-operations per cycle, where each micro-operation corresponds to one of our simple RISC instructions."}, {"start": "00:10:24", "is_lecture": true, "end": "00:10:29", "is_worked_example": false, "text": "Here's a simplified diagram of a modern out-of-order superscalar processor."}, {"start": "00:10:29", "is_lecture": true, "end": "00:10:34", "is_worked_example": false, "text": "Instruction fetch and decode handles, say, 4 instructions at a time."}, {"start": "00:10:34", "is_lecture": true, "end": "00:10:41", "is_worked_example": false, "text": "The ability to sustain this execution rate depends heavily on the ability to predict the outcome of branch instructions,"}, {"start": "00:10:41", "is_lecture": true, "end": "00:10:47", "is_worked_example": false, "text": "ensuring that the wide pipeline will be mostly filled with instructions we actually want to execute."}, {"start": "00:10:47", "is_lecture": true, "end": "00:10:57", "is_worked_example": false, "text": "Good branch prediction requires the use of the history from previous branches and there's been a lot of cleverness devoted to getting good predictions from the least amount of hardware!"}, {"start": "00:10:57", "is_lecture": true, "end": "00:11:03", "is_worked_example": false, "text": "If you're interested in the details, search for \"branch predictor\" on Wikipedia."}, {"start": "00:11:03", "is_lecture": true, "end": "00:11:10", "is_worked_example": false, "text": "The register renaming happens during instruction decode, after which the instructions are ready to be dispatched to the functional units."}, {"start": "00:11:10", "is_lecture": true, "end": "00:11:21", "is_worked_example": false, "text": "If an instruction needs the result of an earlier instruction as an operand, the dispatcher has identified which functional unit will be producing the result."}, {"start": "00:11:21", "is_lecture": true, "end": "00:11:32", "is_worked_example": false, "text": "The instruction waits in a queue until the indicated functional unit produces the result and when all the operand values are known, the instruction is finally taken from the queue and executed."}, {"start": "00:11:32", "is_lecture": true, "end": "00:11:42", "is_worked_example": false, "text": "Since the instructions are executed by different functional units as soon as their operands are available, the order of execution may not be the same as in the original program."}, {"start": "00:11:42", "is_lecture": true, "end": "00:11:50", "is_worked_example": false, "text": "After execution, the functional units broadcast their results so that waiting instructions know when to proceed."}, {"start": "00:11:50", "is_lecture": true, "end": "00:12:00", "is_worked_example": false, "text": "The results are also collected in a large reorder buffer so that that they can be retired (i.e., write their results in the register file) in the correct order."}, {"start": "00:12:00", "is_lecture": true, "end": "00:12:01", "is_worked_example": false, "text": "Whew!"}, {"start": "00:12:01", "is_lecture": true, "end": "00:12:12", "is_worked_example": false, "text": "There's a lot of circuitry involved in keeping the functional units fed with instructions, knowing when instructions have all their operands, and organizing the execution results into the correct order."}, {"start": "00:12:12", "is_lecture": true, "end": "00:12:17", "is_worked_example": false, "text": "So how much speed up should we expect from all this machinery?"}, {"start": "00:12:17", "is_lecture": true, "end": "00:12:28", "is_worked_example": false, "text": "The effective CPI is very program-specific, depending as it does on cache hit rates, successful branch prediction, available ILP, and so on."}, {"start": "00:12:28", "is_lecture": true, "end": "00:12:34", "is_worked_example": false, "text": "Given the architecture described here the best speed up we could hope for is a factor of 4."}, {"start": "00:12:34", "is_lecture": true, "end": "00:12:44", "is_worked_example": false, "text": "Googling around, it seems that the reality is an average speed-up of 2, maybe slightly less, over what would be achieved by an in-order, single-issue processor."}, {"start": "00:12:44", "is_lecture": true, "end": "00:12:50", "is_worked_example": false, "text": "What can we expect for future performance improvements in out-of-order, superscalar pipelines?"}, {"start": "00:12:50", "is_lecture": true, "end": "00:12:56", "is_worked_example": false, "text": "Increases in pipeline depth can cause CPI_stall and timing overheads to rise."}, {"start": "00:12:56", "is_lecture": true, "end": "00:13:06", "is_worked_example": false, "text": "At the current pipeline depths the increase in CPI_stall is larger than the gains from decreased t_CLK and so further increases in depth are unlikely."}, {"start": "00:13:06", "is_lecture": true, "end": "00:13:21", "is_worked_example": false, "text": "A similar tradeoff exists between using more out-of-order execution to increase ILP and the increase in CPI_stall caused by the impact of mis-predicted branches and the inability to run main memories any faster."}, {"start": "00:13:21", "is_lecture": true, "end": "00:13:30", "is_worked_example": false, "text": "Power consumption increases more quickly than the performance gains from lower t_CLK and additional out-of-order execution logic."}, {"start": "00:13:30", "is_lecture": true, "end": "00:13:37", "is_worked_example": false, "text": "The additional complexity required to enable further improvements in branch prediction and concurrent execution seems very daunting."}, {"start": "00:13:37", "is_lecture": true, "end": "00:13:48", "is_worked_example": false, "text": "All of these factors suggest that is unlikely that we'll see substantial future improvements in the performance of out-of-order superscalar pipelined processors."}, {"start": "00:13:48", "is_lecture": true, "end": "00:13:55", "is_worked_example": false, "text": "So system architects have turned their attention to exploiting data-level parallelism (DLP) and thread-level parallelism (TLP)."}, {"start": "00:13:55", "is_lecture": true, "end": "00:13:57", "is_worked_example": false, "text": "These are our next two topics."}]}, "C06S01B05-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c6/c6s1/5?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c6s1v5", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:08", "is_worked_example": false, "text": "Another service provided by operating system is dealing properly with the attempt to execute instructions with \"illegal\" opcodes."}, {"start": "00:00:08", "is_lecture": true, "end": "00:00:15", "is_worked_example": false, "text": "Illegal is quotes because that just means opcodes whose operations aren't implemented directly by the hardware."}, {"start": "00:00:15", "is_lecture": true, "end": "00:00:21", "is_worked_example": false, "text": "As we'll see, it's possible extend the functionality of the hardware via software emulation."}, {"start": "00:00:21", "is_lecture": true, "end": "00:00:33", "is_worked_example": false, "text": "The action of the CPU upon encountering an illegal instruction (sometimes referred to as an unimplemented user operation or UUO) is very similar to how it processes interrupts."}, {"start": "00:00:33", "is_lecture": true, "end": "00:00:39", "is_worked_example": false, "text": "Think of illegal instructions as an interrupt caused directly by the CPU!"}, {"start": "00:00:39", "is_lecture": true, "end": "00:00:54", "is_worked_example": false, "text": "As for interrupts, the execution of the current instruction is suspended and the control signals are set to values to capture PC+4 in the XP register and set the PC to, in this case, 0x80000004."}, {"start": "00:00:54", "is_lecture": true, "end": "00:01:05", "is_worked_example": false, "text": "Note that bit 31 of the new PC, aka the supervisor bit, is set to 1, meaning that the OS handler will have access to the kernel-mode context."}, {"start": "00:01:05", "is_lecture": true, "end": "00:01:12", "is_worked_example": false, "text": "Here's some code similar to that found in the Tiny Operating System (TinyOS), which you'll be experimenting with in the final lab assignment."}, {"start": "00:01:12", "is_lecture": true, "end": "00:01:17", "is_worked_example": false, "text": "Let's do a quick walk-through of the code executed when an illegal instruction is executed."}, {"start": "00:01:17", "is_lecture": true, "end": "00:01:22", "is_worked_example": false, "text": "Starting at location 0, we see the branches to the handlers for the various interrupts and exceptions."}, {"start": "00:01:22", "is_lecture": true, "end": "00:01:29", "is_worked_example": false, "text": "In the case of an illegal instruction, the BR(I_IllOp) in location 4 will be executed."}, {"start": "00:01:29", "is_lecture": true, "end": "00:01:34", "is_worked_example": false, "text": "Immediately following is where the OS data structures are allocated."}, {"start": "00:01:34", "is_lecture": true, "end": "00:01:43", "is_worked_example": false, "text": "This includes space for the OS stack, UserMState where user-mode register values are stored during interrupts, and the process table,"}, {"start": "00:01:43", "is_lecture": true, "end": "00:01:50", "is_worked_example": false, "text": "providing long-term storage for the complete state of each process while another process is executing."}, {"start": "00:01:50", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "When writing in assembly language, it's convenient to define macros for operations that are used repeatedly."}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "We can use a macro call whenever we want to perform the action and the assembler will insert the body of the macro in place of the macro call, performing a lexical substitution of the macro's arguments."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:16", "is_worked_example": false, "text": "Here's a macro for a two-instruction sequence that extracts a particular field of bits from a 32-bit value."}, {"start": "00:02:16", "is_lecture": true, "end": "00:02:21", "is_worked_example": false, "text": "M is the bit number of the left-most bit, N is the bit number of the right-most bit."}, {"start": "00:02:21", "is_lecture": true, "end": "00:02:31", "is_worked_example": false, "text": "Bits are numbered 0 through 31, where bit 31 is the most-significant bit, i.e., the one at the left end of the 32-bit binary value."}, {"start": "00:02:31", "is_lecture": true, "end": "00:02:42", "is_worked_example": false, "text": "And here are some macros that expand into instruction sequences that save and restore the CPU registers to or from the UserMState temporary storage area."}, {"start": "00:02:42", "is_lecture": true, "end": "00:02:47", "is_worked_example": false, "text": "With those macros in hand, let's see how illegal opcodes are handled."}, {"start": "00:02:47", "is_lecture": true, "end": "00:02:56", "is_worked_example": false, "text": "Like all interrupt handlers, the first action is to save the user-mode registers in the temporary storage area and initialize the OS stack."}, {"start": "00:02:56", "is_lecture": true, "end": "00:03:01", "is_worked_example": false, "text": "Next, we fetch the illegal instruction from the user-mode program."}, {"start": "00:03:01", "is_lecture": true, "end": "00:03:08", "is_worked_example": false, "text": "Note that the saved PC+4 value is a virtual address in the context of the interrupted program."}, {"start": "00:03:08", "is_lecture": true, "end": "00:03:15", "is_worked_example": false, "text": "So we'll need to use the MMU routines to compute the correct physical address -- more about this on the next slide."}, {"start": "00:03:15", "is_lecture": true, "end": "00:03:26", "is_worked_example": false, "text": "Then we'll use the opcode of the illegal instruction as an index into a table of subroutine addresses, one for each of the 64 possible opcodes."}, {"start": "00:03:26", "is_lecture": true, "end": "00:03:34", "is_worked_example": false, "text": "Once we have the address of the handler for this particular illegal opcode, we JMP there to deal with the situation."}, {"start": "00:03:34", "is_lecture": true, "end": "00:03:42", "is_worked_example": false, "text": "Selecting a destination from a table of addresses is called \"dispatching\" and the table is called the \"dispatch table\"."}, {"start": "00:03:42", "is_lecture": true, "end": "00:05:42", "is_worked_example": false, "text": "If the dispatch table contains many different entries, dispatching is much more efficient in time and space than a long series of compares and branches."}, {"start": "00:05:42", "is_lecture": true, "end": "00:03:59", "is_worked_example": false, "text": "In this case, the table is indicating that the handler for most illegal opcodes is the UUOError routine,"}, {"start": "00:03:59", "is_lecture": true, "end": "00:04:07", "is_worked_example": false, "text": "so it might have smaller and faster simply to test for the two illegal opcodes the OS is going to emulate."}, {"start": "00:04:07", "is_lecture": true, "end": "00:04:17", "is_worked_example": false, "text": "Illegal opcode 1 will be used to implement procedure calls from user-mode to the OS, which we call supervisor calls."}, {"start": "00:04:17", "is_lecture": true, "end": "00:04:19", "is_worked_example": false, "text": "More on this in the next segment."}, {"start": "00:04:19", "is_lecture": true, "end": "00:04:30", "is_worked_example": false, "text": "As an example of having the OS emulate an instruction, we'll use illegal opcode 2 as the opcode for the SWAPREG instruction, which we'll discuss now."}, {"start": "00:04:30", "is_lecture": true, "end": "00:04:37", "is_worked_example": false, "text": "But first just a quick look at how the OS converts user-mode virtual addresses into physical addresses it can use."}, {"start": "00:04:37", "is_lecture": true, "end": "00:04:43", "is_worked_example": false, "text": "We'll build on the MMU VtoP procedure, described in the previous lecture."}, {"start": "00:04:43", "is_lecture": true, "end": "00:04:50", "is_worked_example": false, "text": "This procedure expects as its arguments the virtual page number and offset fields of the virtual address, so,"}, {"start": "00:04:50", "is_lecture": true, "end": "00:04:57", "is_worked_example": false, "text": "following our convention for passing arguments to C procedures, these are pushed onto the stack in reverse order."}, {"start": "00:04:57", "is_lecture": true, "end": "00:05:01", "is_worked_example": false, "text": "The corresponding physical address is returned in R0."}, {"start": "00:05:01", "is_lecture": true, "end": "00:05:08", "is_worked_example": false, "text": "We can then use the calculated physical address to read the desired location from physical memory."}, {"start": "00:05:08", "is_lecture": true, "end": "00:05:12", "is_worked_example": false, "text": "Okay, back to dealing with illegal opcodes."}, {"start": "00:05:12", "is_lecture": true, "end": "00:05:15", "is_worked_example": false, "text": "Here's the handler for opcodes that are truly illegal."}, {"start": "00:05:15", "is_lecture": true, "end": "00:05:23", "is_worked_example": false, "text": "In this case the OS uses various kernel routines to print out a helpful error message on the user's console, then crashes the system!"}, {"start": "00:05:23", "is_lecture": true, "end": "00:05:30", "is_worked_example": false, "text": "You may have seen these \"blue screens of death\" if you run the Windows operating system, full of cryptic hex numbers."}, {"start": "00:05:30", "is_lecture": true, "end": "00:05:36", "is_worked_example": false, "text": "Actually, this wouldn't be the best approach to handling an illegal opcode in a user's program."}, {"start": "00:05:36", "is_lecture": true, "end": "00:05:44", "is_worked_example": false, "text": "In a real operating system, it would be better to save the state of the process in a special debugging file historically referred to as a \"core dump\""}, {"start": "00:05:44", "is_lecture": true, "end": "00:05:52", "is_worked_example": false, "text": "and then terminate this particular process, perhaps printing a short error message on the user's console to let them know what happened."}, {"start": "00:05:52", "is_lecture": true, "end": "00:05:59", "is_worked_example": false, "text": "Then later the user could start a debugging program to examine the dump file to see where their bug is."}, {"start": "00:05:59", "is_lecture": true, "end": "00:06:11", "is_worked_example": false, "text": "Finally, here's the handler that will emulate the actions of the SWAPREG instruction, after which program execution will resume as if the instruction had been implemented in hardware."}, {"start": "00:06:11", "is_lecture": true, "end": "00:06:16", "is_worked_example": false, "text": "SWAPREG is an instruction that swaps the values in the two specified registers."}, {"start": "00:06:16", "is_lecture": true, "end": "00:06:26", "is_worked_example": false, "text": "To define a new instruction, we'd first have to let the assembler know to convert the swapreg(ra,rc) assembly language statement into binary."}, {"start": "00:06:26", "is_lecture": true, "end": "00:06:34", "is_worked_example": false, "text": "In this case we'll use a binary format similar to the ADDC instruction, but setting the unused literal field to 0."}, {"start": "00:06:34", "is_lecture": true, "end": "00:06:42", "is_worked_example": false, "text": "The encoding for the RA and RC registers occur in their usual fields and the opcode field is set to 2."}, {"start": "00:06:42", "is_lecture": true, "end": "00:06:45", "is_worked_example": false, "text": "Emulation is surprisingly simple."}, {"start": "00:06:45", "is_lecture": true, "end": "00:06:58", "is_worked_example": false, "text": "First we extract the RA and RC fields from the binary for the swapreg instruction and convert those values into the appropriate byte offsets for accessing the temporary array of saved register values."}, {"start": "00:06:58", "is_lecture": true, "end": "00:07:06", "is_worked_example": false, "text": "Then we use RA and RC offsets to access the user-mode register values that have been saved in UserMState."}, {"start": "00:07:06", "is_lecture": true, "end": "00:07:18", "is_worked_example": false, "text": "We'll make the appropriate interchange, leaving the updated register values in UserMState, where they'll be loaded into the CPU registers upon returning from the illegal instruction interrupt handler."}, {"start": "00:07:18", "is_lecture": true, "end": "00:07:25", "is_worked_example": false, "text": "Finally, we'll branch to the kernel code that restores the process state and resumes execution."}, {"start": "00:07:25", "is_lecture": true, "end": "00:07:28", "is_worked_example": false, "text": "We'll see this code in the next segment."}]}, "C03S01B03-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c3/c3s1/3?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c3s1v3", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:10", "is_worked_example": false, "text": "The data path diagram isn't all that useful in diagramming the pipelined execution of an instruction sequence since we need a new copy of the diagram for each clock cycle."}, {"start": "00:00:10", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "A more compact and easier-to-read diagram of pipelined execution is provided by the pipeline diagrams we met back in Part 1 of the course."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:27", "is_worked_example": false, "text": "There's one row in the diagram for each pipeline stage and one column for each cycle of execution."}, {"start": "00:00:27", "is_lecture": true, "end": "00:00:34", "is_worked_example": false, "text": "Entries in the table show which instruction is in each pipeline stage at each cycle."}, {"start": "00:00:34", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "In normal operation, a particular instruction moves diagonally through the diagram as it proceeds through the five pipeline stages."}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:49", "is_worked_example": false, "text": "To understand data hazards, let's first remind ourselves of when the register file is read and written for a particular instruction."}, {"start": "00:00:49", "is_lecture": true, "end": "00:00:58", "is_worked_example": false, "text": "Register reads happen when the instruction is in the RF stage, i.e., when we're reading the instruction's register operands."}, {"start": "00:00:58", "is_lecture": true, "end": "00:01:03", "is_worked_example": false, "text": "Register writes happen at the end of the cycle when the instruction is in the WB stage."}, {"start": "00:01:03", "is_lecture": true, "end": "00:01:12", "is_worked_example": false, "text": "For example, for the first LD instruction, we read R1 during cycle 2 and write R2 at the end of cycle 5."}, {"start": "00:01:12", "is_lecture": true, "end": "00:01:17", "is_worked_example": false, "text": "Or consider the register file operations in cycle 6:"}, {"start": "00:01:17", "is_lecture": true, "end": "00:01:28", "is_worked_example": false, "text": "we're reading R12 and R13 for the MUL instruction in the RF stage, and writing R4 at the end of the cycle for the LD instruction in the WB stage."}, {"start": "00:01:28", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "Okay, now let's see what happens when there are data hazards."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:40", "is_worked_example": false, "text": "In this instruction sequence, the ADDC instruction writes its result in R2, which is immediately read by the following SUBC instruction."}, {"start": "00:01:40", "is_lecture": true, "end": "00:01:46", "is_worked_example": false, "text": "Correct execution of the SUBC instruction clearly depends on the results of the ADDC instruction."}, {"start": "00:01:46", "is_lecture": true, "end": "00:01:50", "is_worked_example": false, "text": "This what we'd call a read-after-write dependency."}, {"start": "00:01:50", "is_lecture": true, "end": "00:02:00", "is_worked_example": false, "text": "This pipeline diagram shows the cycle-by-cycle execution where we've circled the cycles during which ADDC writes R2 and SUBC reads R2."}, {"start": "00:02:00", "is_lecture": true, "end": "00:02:01", "is_worked_example": false, "text": "Oops!"}, {"start": "00:02:01", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "ADDC doesn't write R2 until the end of cycle 5, but SUBC is trying to read the R2 value in cycle 3."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:17", "is_worked_example": false, "text": "The value in R2 in the register file in cycle 3 doesn't yet reflect the execution of the ADDC instruction."}, {"start": "00:02:17", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "So as things stand the pipeline would *not* correctly execute this instruction sequence."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:27", "is_worked_example": false, "text": "This instruction sequence has triggered a data hazard."}, {"start": "00:02:27", "is_lecture": true, "end": "00:02:35", "is_worked_example": false, "text": "We want the pipelined CPU to generate the same program results as the unpipelined CPU, so we'll need to figure out a fix."}, {"start": "00:02:35", "is_lecture": true, "end": "00:02:40", "is_worked_example": false, "text": "There are three general strategies we can pursue to fix pipeline hazards."}, {"start": "00:02:40", "is_lecture": true, "end": "00:02:48", "is_worked_example": false, "text": "Any of the techniques will work, but as we'll see they have different tradeoffs for instruction throughput and circuit complexity."}, {"start": "00:02:48", "is_lecture": true, "end": "00:02:56", "is_worked_example": false, "text": "The first strategy is to stall instructions in the RF stage until the result they need has been written to the register file."}, {"start": "00:02:56", "is_lecture": true, "end": "00:03:04", "is_worked_example": false, "text": "\"Stall\" means that we don't reload the instruction register at the end of the cycle, so we'll try to execute the same instruction in the next cycle."}, {"start": "00:03:04", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "If we stall one pipeline stage, all earlier stages must also be stalled since they are blocked by the stalled instruction."}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:17", "is_worked_example": false, "text": "If an instruction is stalled in the RF stage, then the IF stage is also stalled."}, {"start": "00:03:17", "is_lecture": true, "end": "00:03:23", "is_worked_example": false, "text": "Stalling will always work, but has a negative impact on instruction throughput."}, {"start": "00:03:23", "is_lecture": true, "end": "00:03:29", "is_worked_example": false, "text": "Stall for too many cycles and you'll loose the performance advantages of pipelined execution!"}, {"start": "00:03:29", "is_lecture": true, "end": "00:03:36", "is_worked_example": false, "text": "The second strategy is to route the needed value to earlier pipeline stages as soon as its computed."}, {"start": "00:03:36", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "This called bypassing or forwarding."}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "As it turns out, the value we need often exists somewhere in the pipelined data path, it just hasn't been written yet to the register file."}, {"start": "00:03:47", "is_lecture": true, "end": "00:03:53", "is_worked_example": false, "text": "If the value exists and can be forwarded to where it's needed, we won't need to stall."}, {"start": "00:03:53", "is_lecture": true, "end": "00:03:58", "is_worked_example": false, "text": "We'll be able to use this strategy to avoid stalling on most types of data hazards."}, {"start": "00:03:58", "is_lecture": true, "end": "00:04:02", "is_worked_example": false, "text": "The third strategy is called speculation."}, {"start": "00:04:02", "is_lecture": true, "end": "00:04:06", "is_worked_example": false, "text": "We'll make an intelligent guess for the needed value and continue execution."}, {"start": "00:04:06", "is_lecture": true, "end": "00:04:11", "is_worked_example": false, "text": "Once the actual value is determined, if we guessed correctly, we're all set."}, {"start": "00:04:11", "is_lecture": true, "end": "00:04:18", "is_worked_example": false, "text": "If we guessed incorrectly, we have to back up execution and restart with the correct value."}, {"start": "00:04:18", "is_lecture": true, "end": "00:04:23", "is_worked_example": false, "text": "Obviously speculation only makes sense if it's possible to make accurate guesses."}, {"start": "00:04:23", "is_lecture": true, "end": "00:04:28", "is_worked_example": false, "text": "We'll be able to use this strategy to avoid stalling on control hazards."}, {"start": "00:04:28", "is_lecture": true, "end": "00:04:33", "is_worked_example": false, "text": "Let's see how the first two strategies work when dealing with our data hazard."}, {"start": "00:04:33", "is_lecture": true, "end": "00:04:43", "is_worked_example": false, "text": "Applying the stall strategy to our data hazard, we need to stall the SUBC instruction in the RF stage until the ADDC instruction writes its result in R2."}, {"start": "00:04:43", "is_lecture": true, "end": "00:04:54", "is_worked_example": false, "text": "So in the pipeline diagram, SUBC is stalled three times in the RF stage until it can finally access the R2 value from the register file in cycle 6."}, {"start": "00:04:54", "is_lecture": true, "end": "00:05:00", "is_worked_example": false, "text": "Whenever the RF stage is stalled, the IF stage is also stalled."}, {"start": "00:05:00", "is_lecture": true, "end": "00:05:02", "is_worked_example": false, "text": "You can see that in the diagram too."}, {"start": "00:05:02", "is_lecture": true, "end": "00:05:07", "is_worked_example": false, "text": "But when RF is stalled, what should the ALU stage do in the next cycle?"}, {"start": "00:05:07", "is_lecture": true, "end": "00:05:13", "is_worked_example": false, "text": "The RF stage hasn't finished its job and so can't pass along its instruction!"}, {"start": "00:05:13", "is_lecture": true, "end": "00:05:23", "is_worked_example": false, "text": "The solution is for the RF stage to make-up an innocuous instruction for the ALU stage, what's called a NOP instruction, short for \"no operation\"."}, {"start": "00:05:23", "is_lecture": true, "end": "00:05:31", "is_worked_example": false, "text": "A NOP instruction has no effect on the CPU state, i.e., it doesn't change the contents of the register file or main memory."}, {"start": "00:05:31", "is_lecture": true, "end": "00:05:39", "is_worked_example": false, "text": "For example any OP-class or OPC-class instruction that has R31 as its destination register is a NOP."}, {"start": "00:05:39", "is_lecture": true, "end": "00:05:45", "is_worked_example": false, "text": "The NOPs introduced into the pipeline by the stalled RF stage are shown in red."}, {"start": "00:05:45", "is_lecture": true, "end": "00:05:52", "is_worked_example": false, "text": "Since the SUBC is stalled in the RF stage for three cycles, three NOPs are introduced into the pipeline."}, {"start": "00:05:52", "is_lecture": true, "end": "00:05:56", "is_worked_example": false, "text": "We sometimes refer to these NOPs as \"bubbles\" in the pipeline."}, {"start": "00:05:56", "is_lecture": true, "end": "00:06:00", "is_worked_example": false, "text": "How does the pipeline know when to stall?"}, {"start": "00:06:00", "is_lecture": true, "end": "00:06:13", "is_worked_example": false, "text": "It can compare the register numbers in the RA and RB fields of the instruction in the RF stage with the register numbers in the RC field of instructions in the ALU, MEM, and WB stage."}, {"start": "00:06:13", "is_lecture": true, "end": "00:06:20", "is_worked_example": false, "text": "If there's a match, there's a data hazard and the RF stage should be stalled."}, {"start": "00:06:20", "is_lecture": true, "end": "00:06:23", "is_worked_example": false, "text": "The stall will continue until there's no hazard detected."}, {"start": "00:06:23", "is_lecture": true, "end": "00:06:26", "is_worked_example": false, "text": "There are a few details to take care of:"}, {"start": "00:06:26", "is_lecture": true, "end": "00:06:39", "is_worked_example": false, "text": "some instructions don't read both registers, the ST instruction doesn't use its RC field, and we don't want R31 to match since it's always okay to read R31 from the register file."}, {"start": "00:06:39", "is_lecture": true, "end": "00:06:46", "is_worked_example": false, "text": "Stalling will ensure correct pipelined execution, but it does increase the effective CPI."}, {"start": "00:06:46", "is_lecture": true, "end": "00:06:55", "is_worked_example": false, "text": "This will lead to longer execution times if the increase in CPI is larger than the decrease in cycle time afforded by pipelining."}, {"start": "00:06:55", "is_lecture": true, "end": "00:07:01", "is_worked_example": false, "text": "To implement stalling, we only need to make two simple changes to our pipelined data path."}, {"start": "00:07:01", "is_lecture": true, "end": "00:07:14", "is_worked_example": false, "text": "We generate a new control signal, STALL, which, when asserted, disables the loading of the three pipeline registers at the input of the IF and RF stages, which means they'll have the same value next cycle as they do this cycle."}, {"start": "00:07:14", "is_lecture": true, "end": "00:07:20", "is_worked_example": false, "text": "We also introduce a mux to choose the instruction to be sent along to the ALU stage."}, {"start": "00:07:20", "is_lecture": true, "end": "00:07:27", "is_worked_example": false, "text": "If STALL is 1, we choose a NOP instruction, e.g., an ADD with R31 as its destination."}, {"start": "00:07:27", "is_lecture": true, "end": "00:07:33", "is_worked_example": false, "text": "If STALL is 0, the RF stage is not stalled, so we pass its current instruction to the ALU."}, {"start": "00:07:33", "is_lecture": true, "end": "00:07:38", "is_worked_example": false, "text": "And here we see how to compute STALL as described in the previous slide."}, {"start": "00:07:38", "is_lecture": true, "end": "00:07:49", "is_worked_example": false, "text": "The additional logic needed to implement stalling is pretty modest, so the real design tradeoff is about increased CPI due to stalling vs. decreased cycle time due to pipelining."}, {"start": "00:07:49", "is_lecture": true, "end": "00:07:55", "is_worked_example": false, "text": "So we have a solution, although it carries some potential performance costs."}, {"start": "00:07:55", "is_lecture": true, "end": "00:07:56", "is_worked_example": false, "text": "Now let's consider our second strategy:"}, {"start": "00:07:56", "is_lecture": true, "end": "00:08:04", "is_worked_example": false, "text": "bypassing, which is applicable if the data we need in the RF stage is somewhere in the pipelined data path."}, {"start": "00:08:04", "is_lecture": true, "end": "00:08:16", "is_worked_example": false, "text": "In our example, even though ADDC doesn't write R2 until the end of cycle 5, the value that will be written is computed during cycle 3 when the ADDC is in the ALU stage."}, {"start": "00:08:16", "is_lecture": true, "end": "00:08:24", "is_worked_example": false, "text": "In cycle 3, the output of the ALU is the value needed by the SUBC that's in the RF stage in the same cycle."}, {"start": "00:08:24", "is_lecture": true, "end": "00:08:41", "is_worked_example": false, "text": "So, if we detect that the RA field of the instruction in the RF stage is the same as the RC field of the instruction in the ALU stage, we can use the output of the ALU in place of the (stale) RA value being read from the register file."}, {"start": "00:08:41", "is_lecture": true, "end": "00:08:42", "is_worked_example": false, "text": "No stalling necessary!"}, {"start": "00:08:42", "is_lecture": true, "end": "00:08:51", "is_worked_example": false, "text": "In our example, in cycle 3 we want to route the output of the ALU to the RF stage to be used as the value for R2."}, {"start": "00:08:51", "is_lecture": true, "end": "00:08:59", "is_worked_example": false, "text": "We show this with a red \"bypass arrow\" showing data being routed from the ALU stage to the RF stage."}, {"start": "00:08:59", "is_lecture": true, "end": "00:09:09", "is_worked_example": false, "text": "To implement bypassing, we'll add a many-input multiplexer to the read ports of the register file so we can select the appropriate value from other pipeline stages."}, {"start": "00:09:09", "is_lecture": true, "end": "00:09:15", "is_worked_example": false, "text": "Here we show the combinational bypass paths from the ALU, MEM, and WB stages."}, {"start": "00:09:15", "is_lecture": true, "end": "00:09:24", "is_worked_example": false, "text": "For the bypassing example of the previous slides, we use the blue bypass path during cycle 3 to get the correct value for R2."}, {"start": "00:09:24", "is_lecture": true, "end": "00:09:40", "is_worked_example": false, "text": "The bypass muxes are controlled by logic that's matching the number of the source register to the number of the destination registers in the ALU, MEM, and WB stages, with the usual complications of dealing with R31."}, {"start": "00:09:40", "is_lecture": true, "end": "00:09:51", "is_worked_example": false, "text": "What if there are multiple matches, i.e., if the RF stage is trying to read a register that's the destination for, say, the instructions in both the ALU and MEM stages?"}, {"start": "00:09:51", "is_lecture": true, "end": "00:09:52", "is_worked_example": false, "text": "No problem!"}, {"start": "00:09:52", "is_lecture": true, "end": "00:10:05", "is_worked_example": false, "text": "We want to select the result from the most recent instruction, so we'd chose the ALU match if there is one, then the MEM match, then the WB match, then, finally, the output of the register file."}, {"start": "00:10:05", "is_lecture": true, "end": "00:10:09", "is_worked_example": false, "text": "Here's diagram showing all the bypass paths we'll need."}, {"start": "00:10:09", "is_lecture": true, "end": "00:10:20", "is_worked_example": false, "text": "Note that branches and jumps write their PC+4 value into the register file, so we'll need to bypass from the PC+4 values in the various stages as well as the ALU values."}, {"start": "00:10:20", "is_lecture": true, "end": "00:10:27", "is_worked_example": false, "text": "Note that the bypassing is happening at the end of the cycle, e.g., after the ALU has computed its answer."}, {"start": "00:10:27", "is_lecture": true, "end": "00:10:34", "is_worked_example": false, "text": "To accommodate the extra t_PD of the bypass mux, we'll have to extend the clock period by a small amount."}, {"start": "00:10:34", "is_lecture": true, "end": "00:10:43", "is_worked_example": false, "text": "So once again there's a design tradeoff -- the increased CPI of stalling vs the slightly increased cycle time of bypassing."}, {"start": "00:10:43", "is_lecture": true, "end": "00:10:50", "is_worked_example": false, "text": "And, of course, in the case of bypassing there's the extra area needed for the necessary wiring and muxes."}, {"start": "00:10:50", "is_lecture": true, "end": "00:11:03", "is_worked_example": false, "text": "We can cut back on the costs by reducing the amount of bypassing, say, to only bypassing ALU results from the ALU stage and use stalling to deal with all the other data hazards."}, {"start": "00:11:03", "is_lecture": true, "end": "00:11:07", "is_worked_example": false, "text": "If we implement full bypassing, do we still need the STALL logic?"}, {"start": "00:11:07", "is_lecture": true, "end": "00:11:09", "is_worked_example": false, "text": "As it turns out, we do!"}, {"start": "00:11:09", "is_lecture": true, "end": "00:11:13", "is_worked_example": false, "text": "There's one data hazard that bypassing doesn't completely address."}, {"start": "00:11:13", "is_lecture": true, "end": "00:11:18", "is_worked_example": false, "text": "Consider trying to immediately the use the result of a LD instruction."}, {"start": "00:11:18", "is_lecture": true, "end": "00:11:25", "is_worked_example": false, "text": "In the example shown here, the SUBC is trying to use the value the immediately preceding LD is writing to R2."}, {"start": "00:11:25", "is_lecture": true, "end": "00:11:28", "is_worked_example": false, "text": "This is called a load-to-use hazard."}, {"start": "00:11:28", "is_lecture": true, "end": "00:11:44", "is_worked_example": false, "text": "Recalling that LD data isn't available in the data path until the cycle when LD reaches the WB stage, even with full bypassing we'll need to stall SUBC in the RF stage until cycle 5, introducing two NOPs into the pipeline."}, {"start": "00:11:44", "is_lecture": true, "end": "00:11:50", "is_worked_example": false, "text": "Without bypassing from the WB stage, we need to stall until cycle 6."}, {"start": "00:11:50", "is_lecture": true, "end": "00:11:55", "is_worked_example": false, "text": "In summary, we have two strategies for dealing with data hazards."}, {"start": "00:11:55", "is_lecture": true, "end": "00:12:04", "is_worked_example": false, "text": "We can stall the IF and RF stages until the register values needed by the instruction in the RF stage are available in the register file."}, {"start": "00:12:04", "is_lecture": true, "end": "00:12:14", "is_worked_example": false, "text": "The required hardware is simple, but the NOPs introduced into the pipeline waste CPU cycles and result in an higher effective CPI."}, {"start": "00:12:14", "is_lecture": true, "end": "00:12:23", "is_worked_example": false, "text": "Or we can use bypass paths to route the required values to the RF stage assuming they exist somewhere in the pipelined data path."}, {"start": "00:12:23", "is_lecture": true, "end": "00:12:29", "is_worked_example": false, "text": "This approach requires more hardware than stalling, but doesn't reduce the effective CPI."}, {"start": "00:12:29", "is_lecture": true, "end": "00:12:34", "is_worked_example": false, "text": "Even if we implement bypassing, we'll still need stalls to deal with load-to-use hazards."}, {"start": "00:12:34", "is_lecture": true, "end": "00:12:39", "is_worked_example": false, "text": "Can we keep adding pipeline stages in the hopes of further reducing the clock period?"}, {"start": "00:12:39", "is_lecture": true, "end": "00:12:51", "is_worked_example": false, "text": "More pipeline stages mean more instructions in the pipeline at the same time, which in turn increases the chance of a data hazard and the necessity of stalling, thus increasing CPI."}, {"start": "00:12:51", "is_lecture": true, "end": "00:12:57", "is_worked_example": false, "text": "Compilers can help reduce dependencies by reorganizing the assembly language code they produce."}, {"start": "00:12:57", "is_lecture": true, "end": "00:13:01", "is_worked_example": false, "text": "Here's the load-to-use hazard example we saw earlier."}, {"start": "00:13:01", "is_lecture": true, "end": "00:13:05", "is_worked_example": false, "text": "Even with full bypassing, we'd need to stall for 2 cycles."}, {"start": "00:13:05", "is_lecture": true, "end": "00:13:16", "is_worked_example": false, "text": "But if the compiler (or assembly language programmer!) notices that the MUL and XOR instructions are independent of the SUBC instruction and hence can be moved before the SUBC,"}, {"start": "00:13:16", "is_lecture": true, "end": "00:13:25", "is_worked_example": false, "text": "the dependency is now such that the LD is naturally in the WB stage when the SUBC is in the RF stage, so no stalls are needed."}, {"start": "00:13:25", "is_lecture": true, "end": "00:13:31", "is_worked_example": false, "text": "This optimization only works when the compiler can find independent instructions to move around."}, {"start": "00:13:31", "is_lecture": true, "end": "00:13:37", "is_worked_example": false, "text": "Unfortunately there are plenty of programs where such instructions are hard to find."}, {"start": "00:13:37", "is_lecture": true, "end": "00:13:41", "is_worked_example": false, "text": "Then there's one final approach we could take --"}, {"start": "00:13:41", "is_lecture": true, "end": "00:13:54", "is_worked_example": false, "text": "change the ISA so that data hazards are part of the ISA, i.e., just explain that writes to the destination register happen with a 3-instruction delay!"}, {"start": "00:13:54", "is_lecture": true, "end": "00:13:58", "is_worked_example": false, "text": "If NOPs are needed, make the programmer add them to the program."}, {"start": "00:13:58", "is_lecture": true, "end": "00:14:03", "is_worked_example": false, "text": "Simplify the hardware at the \"small\" cost of making the compilers work harder."}, {"start": "00:14:03", "is_lecture": true, "end": "00:14:08", "is_worked_example": false, "text": "You can imagine exactly how much the compiler writers will like this suggestion."}, {"start": "00:14:08", "is_lecture": true, "end": "00:14:10", "is_worked_example": false, "text": "Not to mention assembly language programmers!"}, {"start": "00:14:10", "is_lecture": true, "end": "00:14:16", "is_worked_example": false, "text": "And you can change the ISA again when you add more pipeline stages!"}, {"start": "00:14:16", "is_lecture": true, "end": "00:14:24", "is_worked_example": false, "text": "This is how a compiler writer views CPU architects who unilaterally change the ISA to save a few logic gates :)"}, {"start": "00:14:24", "is_lecture": true, "end": "00:14:34", "is_worked_example": false, "text": "The bottom line is that successful ISAs have very long lifetimes and so shouldn't include tradeoffs driven by short-term implementation considerations."}, {"start": "00:14:34", "is_lecture": true, "end": "00:14:37", "is_worked_example": false, "text": "Best not to go there."}]}, "C08S01B02-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c8/c8s1/2?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c8s1v2", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:07", "is_worked_example": false, "text": "When a user-mode program wants to read a typed character it executes a ReadKey() SVC."}, {"start": "00:00:07", "is_lecture": true, "end": "00:00:20", "is_worked_example": false, "text": "The binary representation of the SVC has an illegal value in the opcode field, so the CPU hardware causes an exception, which starts executing the illegal opcode handler in the OS."}, {"start": "00:00:20", "is_lecture": true, "end": "00:00:31", "is_worked_example": false, "text": "The OS handler recognizes the illegal opcode value as being an SVC and uses the low-order bits of the SVC instruction to determine which sub-handler to call."}, {"start": "00:00:31", "is_lecture": true, "end": "00:00:37", "is_worked_example": false, "text": "Here's our first draft for the ReadKey sub-handler, this time written in C."}, {"start": "00:00:37", "is_lecture": true, "end": "00:00:46", "is_worked_example": false, "text": "The handler starts by looking at the process table entry for the current process to determine which keyboard buffer holds the characters for the process."}, {"start": "00:00:46", "is_lecture": true, "end": "00:00:51", "is_worked_example": false, "text": "Let's assume for the moment the buffer is *not* empty and skip to the last line,"}, {"start": "00:00:51", "is_lecture": true, "end": "00:01:01", "is_worked_example": false, "text": "which reads the character from the buffer and uses it to replace the saved value for the user's R0 in the array holding the saved register values."}, {"start": "00:01:01", "is_lecture": true, "end": "00:01:11", "is_worked_example": false, "text": "When the handler exits, the OS will reload the saved registers and resume execution of the user-mode program with the just-read character in R0."}, {"start": "00:01:11", "is_lecture": true, "end": "00:01:16", "is_worked_example": false, "text": "Now let's figure what to do when the keyboard buffer is empty."}, {"start": "00:01:16", "is_lecture": true, "end": "00:01:21", "is_worked_example": false, "text": "The code shown here simply loops until the buffer is no longer empty."}, {"start": "00:01:21", "is_lecture": true, "end": "00:01:27", "is_worked_example": false, "text": "The theory is that eventually the user will type a character, causing an interrupt,"}, {"start": "00:01:27", "is_lecture": true, "end": "00:01:34", "is_worked_example": false, "text": "which will run the keyboard interrupt handler discussed in the previous section, which will store a new character into the buffer."}, {"start": "00:01:34", "is_lecture": true, "end": "00:01:45", "is_worked_example": false, "text": "This all sounds good until we remember that the SVC handler is running with the supervisor bit (PC[31]) set to 1, disabling interrupts."}, {"start": "00:01:45", "is_lecture": true, "end": "00:01:46", "is_worked_example": false, "text": "Oops!"}, {"start": "00:01:46", "is_lecture": true, "end": "00:01:53", "is_worked_example": false, "text": "Since the keyboard interrupt will never happen, the while loop shown here is actually an infinite loop."}, {"start": "00:01:53", "is_lecture": true, "end": "00:02:04", "is_worked_example": false, "text": "So if the user-mode program tries to read a character from an empty buffer, the system will appear to hang, not responding to any external inputs since interrupts are disabled."}, {"start": "00:02:04", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "Time to reach for the power switch :)"}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:14", "is_worked_example": false, "text": "We'll fix the looping problem by adding code to subtract 4 from the saved value of the XP register before returning."}, {"start": "00:02:14", "is_lecture": true, "end": "00:02:16", "is_worked_example": false, "text": "How does this fix the problem?"}, {"start": "00:02:16", "is_lecture": true, "end": "00:02:28", "is_worked_example": false, "text": "Recall that when the SVC illegal instruction exception happened, the CPU stored the PC+4 value of the illegal instruction in the user's XP register."}, {"start": "00:02:28", "is_lecture": true, "end": "00:02:38", "is_worked_example": false, "text": "When the handler exits, the OS will resume execution of the user-mode program by reloading the registers and then executing a JMP(XP),"}, {"start": "00:02:38", "is_lecture": true, "end": "00:02:42", "is_worked_example": false, "text": "which would normally then execute the instruction *following* the SVC instruction."}, {"start": "00:02:42", "is_lecture": true, "end": "00:02:50", "is_worked_example": false, "text": "By subtracting 4 from the saved XP value, it will be the SVC itself that gets re-executed."}, {"start": "00:02:50", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "That, of course, means we'll go through the same set of steps again, repeating the cycle until the keyboard buffer is no longer empty."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "It's just a more complicated loop!  But with a crucial difference: one of the instructions -- the ReadKey() SVC -- is executed in user-mode with PC[31] = 0."}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:21", "is_worked_example": false, "text": "So during that cycle, if there's a pending interrupt from the keyboard, the device interrupt will supersede the execution of the ReadKey() and the keyboard buffer will be filled."}, {"start": "00:03:21", "is_lecture": true, "end": "00:03:32", "is_worked_example": false, "text": "When the keyboard interrupt handler finishes, the ReadKey() SVC will be executed again, this time finding that the buffer is no longer empty."}, {"start": "00:03:32", "is_lecture": true, "end": "00:03:33", "is_worked_example": false, "text": "Yah!"}, {"start": "00:03:33", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "So this version of the handler actually works, with one small caveat."}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:51", "is_worked_example": false, "text": "If the buffer is empty, the user-mode program will continually re-execute the complicated user-mode/kernel-mode loop until the timer interrupt eventually transfers control to the next process."}, {"start": "00:03:51", "is_lecture": true, "end": "00:03:54", "is_worked_example": false, "text": "This seems pretty inefficient."}, {"start": "00:03:54", "is_lecture": true, "end": "00:04:02", "is_worked_example": false, "text": "Once we've checked and found the buffer empty, it would be better to give other processes a chance to run before we try again."}, {"start": "00:04:02", "is_lecture": true, "end": "00:04:04", "is_worked_example": false, "text": "This problem is easy to fix!"}, {"start": "00:04:04", "is_lecture": true, "end": "00:04:11", "is_worked_example": false, "text": "We'll just add a call to Scheduler() right after arranging for the ReadKey() SVC to be re-executed."}, {"start": "00:04:11", "is_lecture": true, "end": "00:04:19", "is_worked_example": false, "text": "The call to Scheduler() suspends execution of the current process and arranges for the next process to run when the handler exits."}, {"start": "00:04:19", "is_lecture": true, "end": "00:04:27", "is_worked_example": false, "text": "Eventually the round-robin scheduling will come back to the current process and the ReadKey() SVC will try again."}, {"start": "00:04:27", "is_lecture": true, "end": "00:04:39", "is_worked_example": false, "text": "With this simple one-line fix the system will spend much less time wasting cycles checking the empty buffer and instead use those cycles to run other, hopefully more productive, processes."}, {"start": "00:04:39", "is_lecture": true, "end": "00:04:45", "is_worked_example": false, "text": "The cost is a small delay in restarting the program after a character is typed,"}, {"start": "00:04:45", "is_lecture": true, "end": "00:04:57", "is_worked_example": false, "text": "but typically the time slices for each process are small enough that one round of process execution happens more quickly than the time between two typed characters, so the extra delay isn't noticeable."}, {"start": "00:04:57", "is_lecture": true, "end": "00:05:04", "is_worked_example": false, "text": "So now we have some insights into one of the traditional arguments against timesharing."}, {"start": "00:05:04", "is_lecture": true, "end": "00:05:06", "is_worked_example": false, "text": "The argument goes as follows."}, {"start": "00:05:06", "is_lecture": true, "end": "00:05:12", "is_worked_example": false, "text": "Suppose we have 10 processes, each of which takes 1 second to complete its computation."}, {"start": "00:05:12", "is_lecture": true, "end": "00:05:19", "is_worked_example": false, "text": "Without timesharing, the first process would be done after 1 second, the second after 2 seconds, and so on."}, {"start": "00:05:19", "is_lecture": true, "end": "00:05:29", "is_worked_example": false, "text": "With timesharing using, say, a 1/10 second time slice, all the processes will complete sometime after 10 seconds"}, {"start": "00:05:29", "is_lecture": true, "end": "00:05:35", "is_worked_example": false, "text": "since there's a little extra time needed for the hundred or so process switches that would happen before completion."}, {"start": "00:05:35", "is_lecture": true, "end": "00:05:45", "is_worked_example": false, "text": "So in a timesharing system the time-to-completion for *all* processes is as long the worst-case completion time without time sharing!"}, {"start": "00:05:45", "is_lecture": true, "end": "00:05:47", "is_worked_example": false, "text": "So why bother with timesharing?"}, {"start": "00:05:47", "is_lecture": true, "end": "00:05:51", "is_worked_example": false, "text": "We saw one answer to this question earlier in this slide."}, {"start": "00:05:51", "is_lecture": true, "end": "00:05:59", "is_worked_example": false, "text": "If a process can't make productive use of its time slice, it can donate those cycles to completion of some other task."}, {"start": "00:05:59", "is_lecture": true, "end": "00:06:08", "is_worked_example": false, "text": "So in a system where most processes are waiting for some sort of I/O, timesharing is actually a great way of spending cycles where they'll do the most good."}, {"start": "00:06:08", "is_lecture": true, "end": "00:06:20", "is_worked_example": false, "text": "If you open the Task Manager or Activity Monitor on the system you're using now, you'll see there are hundreds of processes, almost all of which are in some sort of I/O wait."}, {"start": "00:06:20", "is_lecture": true, "end": "00:06:32", "is_worked_example": false, "text": "So timesharing does extract a cost when running compute-intensive computations, but in an actual system where there's a mix of I/O and compute tasks, time sharing is the way to go."}, {"start": "00:06:32", "is_lecture": true, "end": "00:06:39", "is_worked_example": false, "text": "We can actually go one step further to ensure we don't run processes waiting for an I/O event that hasn't yet happened."}, {"start": "00:06:39", "is_lecture": true, "end": "00:06:51", "is_worked_example": false, "text": "We'll add a status field to the process state indicating whether the process is ACTIVE (e.g., status is 0) or WAITING (e.g., status is non-zero)."}, {"start": "00:06:51", "is_lecture": true, "end": "00:06:56", "is_worked_example": false, "text": "We'll use different non-zero values to indicate what event the process is waiting for."}, {"start": "00:06:56", "is_lecture": true, "end": "00:07:01", "is_worked_example": false, "text": "Then we'll change the Scheduler() to only run ACTIVE processes."}, {"start": "00:07:01", "is_lecture": true, "end": "00:07:05", "is_worked_example": false, "text": "To see how this works, it's easiest to use a concrete example."}, {"start": "00:07:05", "is_lecture": true, "end": "00:07:13", "is_worked_example": false, "text": "The UNIX OS has two kernel subroutines, sleep() and wakeup(), both of which require a non-zero argument."}, {"start": "00:07:13", "is_lecture": true, "end": "00:07:17", "is_worked_example": false, "text": "The argument will be used as the value of the status field."}, {"start": "00:07:17", "is_lecture": true, "end": "00:07:19", "is_worked_example": false, "text": "Let's see this in action."}, {"start": "00:07:19", "is_lecture": true, "end": "00:07:31", "is_worked_example": false, "text": "When the ReadKey() SVC detects the buffer is empty, it calls sleep() with an argument that uniquely identifies the I/O event it's waiting for, in this case the arrival of a character in a particular buffer."}, {"start": "00:07:31", "is_lecture": true, "end": "00:07:37", "is_worked_example": false, "text": "sleep() sets the process status to this unique identifier, then calls Scheduler()."}, {"start": "00:07:37", "is_lecture": true, "end": "00:07:45", "is_worked_example": false, "text": "Scheduler() has been modified to skip over processes with a non-zero status, not giving them a chance to run."}, {"start": "00:07:45", "is_lecture": true, "end": "00:07:57", "is_worked_example": false, "text": "Meanwhile, a keyboard interrupt will cause the interrupt handler to add a character to the keyboard buffer and call wakeup() to signal any process waiting on that buffer."}, {"start": "00:07:57", "is_lecture": true, "end": "00:08:03", "is_worked_example": false, "text": "Watch what happens when the kbdnum in the interrupt handler matches the kbdnum in the ReadKey() handler."}, {"start": "00:08:03", "is_lecture": true, "end": "00:08:11", "is_worked_example": false, "text": "wakeup() loops through all processes, looking for ones that are waiting for this particular I/O event."}, {"start": "00:08:11", "is_lecture": true, "end": "00:08:16", "is_worked_example": false, "text": "When it finds one, it sets the status for the process to zero, marking it as ACTIVE."}, {"start": "00:08:16", "is_lecture": true, "end": "00:08:24", "is_worked_example": false, "text": "The zero status will cause the process to run again next time the Scheduler() reaches it in its round-robin search for things to do."}, {"start": "00:08:24", "is_lecture": true, "end": "00:08:37", "is_worked_example": false, "text": "The effect is that once a process goes to sleep() WAITING for an event, it's not considered for execution again until the event occurs and wakeup() marks the process as ACTIVE."}, {"start": "00:08:37", "is_lecture": true, "end": "00:08:38", "is_worked_example": false, "text": "Pretty neat!"}, {"start": "00:08:38", "is_lecture": true, "end": "00:08:44", "is_worked_example": false, "text": "Another elegant fix to ensure that no CPU cycles are wasted on useless activity."}, {"start": "00:08:44", "is_lecture": true, "end": "00:08:52", "is_worked_example": false, "text": "I can remember how impressed I was when I first saw this many years ago in a (very) early version of the UNIX code :)"}]}, "C03S01B07-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c3/c3s1/7?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c3s1v7", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:04", "is_worked_example": false, "text": "Now let's figure out how exceptions impact pipelined execution."}, {"start": "00:00:04", "is_lecture": true, "end": "00:00:17", "is_worked_example": false, "text": "When an exception occurs because of an illegal instruction or an external interrupt, we need to store the current PC+4 value in the XP register and load the program counter with the address of the appropriate exception handler."}, {"start": "00:00:17", "is_lecture": true, "end": "00:00:22", "is_worked_example": false, "text": "Exceptions cause control flow hazards since they are effectively implicit branches."}, {"start": "00:00:22", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "In an unpipelined implementation, exceptions affect the execution of the current instruction."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:33", "is_worked_example": false, "text": "We want to achieve exactly the same effect in our pipelined implementation."}, {"start": "00:00:33", "is_lecture": true, "end": "00:00:49", "is_worked_example": false, "text": "So first we have to identify which one of the instructions in our pipeline is affected, then ensure that instructions that came earlier in the code complete correctly and that we annul the affected instruction and any following instructions that are in the pipeline."}, {"start": "00:00:49", "is_lecture": true, "end": "00:00:53", "is_worked_example": false, "text": "Since there are multiple instructions in the pipeline, we have a bit of sorting out to do."}, {"start": "00:00:53", "is_lecture": true, "end": "00:00:59", "is_worked_example": false, "text": "When, during pipelined execution, do we determine that an instruction will cause an exception?"}, {"start": "00:00:59", "is_lecture": true, "end": "00:01:06", "is_worked_example": false, "text": "An obvious example is detecting an illegal opcode when we decode the instruction in the RF stage."}, {"start": "00:01:06", "is_lecture": true, "end": "00:01:10", "is_worked_example": false, "text": "But we can also generate exceptions in other pipeline stages."}, {"start": "00:01:10", "is_lecture": true, "end": "00:01:16", "is_worked_example": false, "text": "For example, the ALU stage can generate an exception if the second operand of a DIV instruction is 0."}, {"start": "00:01:16", "is_lecture": true, "end": "00:01:23", "is_worked_example": false, "text": "Or the MEM stage may detect that the instruction is attempting to access memory with an illegal address."}, {"start": "00:01:23", "is_lecture": true, "end": "00:01:28", "is_worked_example": false, "text": "Similarly the IF stage can generate a memory exception when fetching the next instruction."}, {"start": "00:01:28", "is_lecture": true, "end": "00:01:36", "is_worked_example": false, "text": "In each case, instructions that follow the one that caused the exception may already be in the pipeline and will need to be annulled."}, {"start": "00:01:36", "is_lecture": true, "end": "00:01:44", "is_worked_example": false, "text": "The good news is that since register values are only updated in the WB stage, annulling an instruction only requires replacing it with a NOP."}, {"start": "00:01:44", "is_lecture": true, "end": "00:01:50", "is_worked_example": false, "text": "We won't have to restore any changed values in the register file or main memory."}, {"start": "00:01:50", "is_lecture": true, "end": "00:01:52", "is_worked_example": false, "text": "Here's our plan."}, {"start": "00:01:52", "is_lecture": true, "end": "00:02:04", "is_worked_example": false, "text": "If an instruction causes an exception in stage i, replace that instruction with this BNE instruction, whose only side effect is writing the PC+4 value into the XP register."}, {"start": "00:02:04", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "Then flush the pipeline by annulling instructions in earlier pipeline stages."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:15", "is_worked_example": false, "text": "And, finally, load the program counter with the address of the exception handler."}, {"start": "00:02:15", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "In this example, assume that LD will generate a memory exception in the MEM stage, which occurs in cycle 4."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:34", "is_worked_example": false, "text": "The arrows show how the instructions in the pipeline are rewritten for cycle 5, at which point the IF stage is working on fetching the first instruction in the exception handler."}, {"start": "00:02:34", "is_lecture": true, "end": "00:02:38", "is_worked_example": false, "text": "Here are the changes required to the execution pipeline."}, {"start": "00:02:38", "is_lecture": true, "end": "00:02:51", "is_worked_example": false, "text": "We modify the muxes in the instruction path so that they can replace an actual instruction with either NOP if the instruction is to be annulled, or BNE if the instruction caused the exception."}, {"start": "00:02:51", "is_lecture": true, "end": "00:03:00", "is_worked_example": false, "text": "Since the pipeline is executing multiple instructions at the same time, we have to worry about what happens if multiple exceptions are detected during execution."}, {"start": "00:03:00", "is_lecture": true, "end": "00:03:10", "is_worked_example": false, "text": "In this example assume that LD will cause a memory exception in the MEM stage and note that it is followed by an instruction with an illegal opcode."}, {"start": "00:03:10", "is_lecture": true, "end": "00:03:24", "is_worked_example": false, "text": "Looking at the pipeline diagram, the invalid opcode is detected in the RF stage during cycle 3, causing the illegal instruction exception process to begin in cycle 4."}, {"start": "00:03:24", "is_lecture": true, "end": "00:03:35", "is_worked_example": false, "text": "But during that cycle, the MEM stage detects the illegal memory access from the LD instruction and so causes the memory exception process to begin in cycle 5."}, {"start": "00:03:35", "is_lecture": true, "end": "00:03:59", "is_worked_example": false, "text": "Note that the exception caused by the earlier instruction (LD) overrides the exception caused by the later illegal opcode even though the illegal opcode exception was detected first. .348 That's the correct behavior since once the execution of LD is abandoned, the pipeline should behave as if none of the instructions that come after the LD were executed."}, {"start": "00:03:59", "is_lecture": true, "end": "00:04:07", "is_worked_example": false, "text": "If multiple exceptions are detected in the *same* cycle, the exception from the instruction furthest down the pipeline should be given precedence."}, {"start": "00:04:07", "is_lecture": true, "end": "00:04:17", "is_worked_example": false, "text": "External interrupts also behave as implicit branches, but it turns out they are a bit easier to handle in our pipeline."}, {"start": "00:04:17", "is_lecture": true, "end": "00:04:22", "is_worked_example": false, "text": "We'll treat external interrupts as if they were an exception that affected the IF stage."}, {"start": "00:04:22", "is_lecture": true, "end": "00:04:26", "is_worked_example": false, "text": "Let's assume the external interrupt occurs in cycle 2."}, {"start": "00:04:26", "is_lecture": true, "end": "00:04:26", "is_worked_example": false, "text": "This means that the SUB instruction will be replaced by our magic BNE to capture the PC+4 value and we'll force the next PC to be the address of the interrupt handler."}, {"start": "00:04:26", "is_lecture": true, "end": "00:04:51", "is_worked_example": false, "text": "After the interrupt handler completes, we'll want to resume execution of the interrupted program at the SUB instruction, so we'll code the handler to correct the value saved in the XP register so that it points to the SUB instruction."}, {"start": "00:04:51", "is_lecture": true, "end": "00:04:55", "is_worked_example": false, "text": "This is all shown in the pipeline diagram."}, {"start": "00:04:55", "is_lecture": true, "end": "00:05:01", "is_worked_example": false, "text": "Note that the ADD, LD, and other instructions that came before SUB in the program are unaffected by the interrupt."}, {"start": "00:05:01", "is_lecture": true, "end": "00:05:10", "is_worked_example": false, "text": "We can use the existing instruction-path muxes to deal with interrupts, since we're treating them as IF-stage exceptions."}, {"start": "00:05:10", "is_lecture": true, "end": "00:05:18", "is_worked_example": false, "text": "We simply have to adjust the logic for IRSrc_IF to also make it 1 when an interrupt is requested."}]}, "C05S01B06-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c5/c5s1/6?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c5s1v6", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:07", "is_worked_example": false, "text": "There are three architectural parameters that characterize a virtual memory system and hence the architecture of the MMU."}, {"start": "00:00:07", "is_lecture": true, "end": "00:00:14", "is_worked_example": false, "text": "P is the number of address bits used for the page offset in both virtual and physical addresses."}, {"start": "00:00:14", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "V is the number of address bits used for the virtual page number."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "And M is the number of address bits used for the physical page number."}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "All the other parameters, listed on the right, are derived from these three parameters."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "As mentioned earlier, the typical page size is between 4KB and 16KB, the sweet spot in the tradeoff between the downside of using physical memory to hold unwanted locations"}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:52", "is_worked_example": false, "text": "and the upside of reading as much as possible from secondary storage so as to amortize the high cost of accessing the initial word over as many words as possible."}, {"start": "00:00:52", "is_lecture": true, "end": "00:00:57", "is_worked_example": false, "text": "The size of the virtual address is determined by the ISA."}, {"start": "00:00:57", "is_lecture": true, "end": "00:01:10", "is_worked_example": false, "text": "We're now making the transition from 32-bit architectures, which support a 4 gigabyte virtual address space, to 64-bit architectures, which support a 16 exabyte virtual address space."}, {"start": "00:01:10", "is_lecture": true, "end": "00:01:18", "is_worked_example": false, "text": "\"Exa\" is the SI prefix for 10^18 -- a 64-bit address can access a *lot* of memory!"}, {"start": "00:01:18", "is_lecture": true, "end": "00:01:26", "is_worked_example": false, "text": "The limitations of a small virtual address have been the main cause for the extinction of many ISAs."}, {"start": "00:01:26", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "Of course, each generation of engineers thinks that the transition they make will be the final one!"}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:38", "is_worked_example": false, "text": "I can remember when we all thought that 32 bits was an unimaginably large address."}, {"start": "00:01:38", "is_lecture": true, "end": "00:01:47", "is_worked_example": false, "text": "Back then we're buying memory by the megabyte and only in our fantasies did we think one could have a system with several thousand megabytes."}, {"start": "00:01:47", "is_lecture": true, "end": "00:01:55", "is_worked_example": false, "text": "Today's CPU architects are feeling pretty smug about 64 bits -- we'll see how they feel in a couple of decades!"}, {"start": "00:01:55", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "The size of physical addresses is currently between 30 bits (for embedded processors with modest memory needs) and 40+ bits (for servers that handle large data sets)."}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:17", "is_worked_example": false, "text": "Since CPU implementations are expected to change every couple of years, the choice of physical memory size can be adjusted to match current technologies."}, {"start": "00:02:17", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "Since programmers use virtual addresses, they're insulated from this implementation choice."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:29", "is_worked_example": false, "text": "The MMU ensures that existing software will continue to function correctly with different sizes of physical memory."}, {"start": "00:02:29", "is_lecture": true, "end": "00:02:35", "is_worked_example": false, "text": "The programmer may notice differences in performance, but not in basic functionality."}, {"start": "00:02:35", "is_lecture": true, "end": "00:02:44", "is_worked_example": false, "text": "For example, suppose our system supported a 32-bit virtual address, a 30-bit physical address and a 4KB page size."}, {"start": "00:02:44", "is_lecture": true, "end": "00:02:54", "is_worked_example": false, "text": "So p = 12, v = 32-12 = 20, and m = 30 - 12 = 18."}, {"start": "00:02:54", "is_lecture": true, "end": "00:03:00", "is_worked_example": false, "text": "There are 2^m physical pages, which is 2^18 in our example."}, {"start": "00:03:00", "is_lecture": true, "end": "00:03:05", "is_worked_example": false, "text": "There are 2^v virtual pages, which is 2^20 in our example."}, {"start": "00:03:05", "is_lecture": true, "end": "00:03:14", "is_worked_example": false, "text": "And since there is one entry in the page map for each virtual page, there are 2^20 (approximately one million) page map entries."}, {"start": "00:03:14", "is_lecture": true, "end": "00:03:26", "is_worked_example": false, "text": "Each page map entry contains a PPN, an R bit and a D bit, for a total of m+2 bits, which is 20 bits in our example."}, {"start": "00:03:26", "is_lecture": true, "end": "00:03:30", "is_worked_example": false, "text": "So there are approximately 20 million bits in the page map."}, {"start": "00:03:30", "is_lecture": true, "end": "00:03:38", "is_worked_example": false, "text": "If we were thinking of using a large special-purpose static RAM to hold the page map, this would get pretty expensive!"}, {"start": "00:03:38", "is_lecture": true, "end": "00:03:42", "is_worked_example": false, "text": "But why use a special-purpose memory for the page map?"}, {"start": "00:03:42", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "Why not use a portion of main memory, which we have a lot of and have already bought and paid for?"}, {"start": "00:03:47", "is_lecture": true, "end": "00:03:54", "is_worked_example": false, "text": "We could use a register, called the page map pointer, to hold the address of the page map array in main memory."}, {"start": "00:03:54", "is_lecture": true, "end": "00:03:59", "is_worked_example": false, "text": "In other words, the page map would occupy some number of dedicated physical pages."}, {"start": "00:03:59", "is_lecture": true, "end": "00:04:10", "is_worked_example": false, "text": "Using the desired virtual page number as an index, the hardware could perform the usual array access calculation to fetch the needed page map entry from main memory."}, {"start": "00:04:10", "is_lecture": true, "end": "00:04:19", "is_worked_example": false, "text": "The downside of this proposed implementation is that it now takes two accesses to physical memory to perform one virtual access:"}, {"start": "00:04:19", "is_lecture": true, "end": "00:04:28", "is_worked_example": false, "text": "the first to retrieve the page table entry needed for the virtual-to-physical address translation, and the second to actually access the requested location."}, {"start": "00:04:28", "is_lecture": true, "end": "00:04:31", "is_worked_example": false, "text": "Once again, caches to the rescue."}, {"start": "00:04:31", "is_lecture": true, "end": "00:04:41", "is_worked_example": false, "text": "Most systems incorporate a special-purpose cache, called a translation look-aside buffer (TLB), that maps virtual page numbers to physical page numbers."}, {"start": "00:04:41", "is_lecture": true, "end": "00:04:45", "is_worked_example": false, "text": "The TLB is usually small and quite fast."}, {"start": "00:04:45", "is_lecture": true, "end": "00:04:51", "is_worked_example": false, "text": "It's usually fully-associative to ensure the best possible hit ratio by avoiding collisions."}, {"start": "00:04:51", "is_lecture": true, "end": "00:05:03", "is_worked_example": false, "text": "If the PPN is found by using the TLB, the access to main memory for the page table entry can be avoided, and we're back to a single physical access for each virtual access."}, {"start": "00:05:03", "is_lecture": true, "end": "00:05:09", "is_worked_example": false, "text": "The hit ratio of a TLB is quite high, usually better than 99%."}, {"start": "00:05:09", "is_lecture": true, "end": "00:05:19", "is_worked_example": false, "text": "This isn't too surprising since locality and the notion of a working set suggest that only a small number of pages are in active use over short periods of time."}, {"start": "00:05:19", "is_lecture": true, "end": "00:05:28", "is_worked_example": false, "text": "As we'll see in a few slides, there are interesting variations to this simple TLB page-map-in-main-memory architecture."}, {"start": "00:05:28", "is_lecture": true, "end": "00:05:31", "is_worked_example": false, "text": "But the basic strategy will remain the same."}, {"start": "00:05:31", "is_lecture": true, "end": "00:05:33", "is_worked_example": false, "text": "Putting it all together:"}, {"start": "00:05:33", "is_lecture": true, "end": "00:05:43", "is_worked_example": false, "text": "the virtual address generated by the CPU is first processed by the TLB to see if the appropriate translation from VPN to PPN has been cached."}, {"start": "00:05:43", "is_lecture": true, "end": "00:05:47", "is_worked_example": false, "text": "If so, the main memory access can proceed directly."}, {"start": "00:05:47", "is_lecture": true, "end": "00:05:55", "is_worked_example": false, "text": "If the desired mapping is not in the TLB, the appropriate entry in the page map is accessed in main memory."}, {"start": "00:05:55", "is_lecture": true, "end": "00:06:02", "is_worked_example": false, "text": "If the page is resident, the PPN field of the page map entry is used to complete the address translation."}, {"start": "00:06:02", "is_lecture": true, "end": "00:06:11", "is_worked_example": false, "text": "And, of course, the translation is cached in the TLB so that subsequent accesses to this page can avoid the access to the page map."}, {"start": "00:06:11", "is_lecture": true, "end": "00:06:20", "is_worked_example": false, "text": "If the desired page is not resident, the MMU triggers a page fault exception and the page fault handler code will deal with the problem."}, {"start": "00:06:20", "is_lecture": true, "end": "00:06:24", "is_worked_example": false, "text": "Here's a final example showing all the pieces in action."}, {"start": "00:06:24", "is_lecture": true, "end": "00:06:30", "is_worked_example": false, "text": "In this example, p = 10, v = 22, and m = 14."}, {"start": "00:06:30", "is_lecture": true, "end": "00:06:35", "is_worked_example": false, "text": "How many pages can reside in physical memory at one time?"}, {"start": "00:06:35", "is_lecture": true, "end": "00:06:40", "is_worked_example": false, "text": "There are 2^m physical pages, so 2^14."}, {"start": "00:06:40", "is_lecture": true, "end": "00:06:44", "is_worked_example": false, "text": "How many entries are there in the page table?"}, {"start": "00:06:44", "is_lecture": true, "end": "00:06:52", "is_worked_example": false, "text": "There's one entry for each virtual page and there are 2^v virtual pages, so there are 2^22 entries in the page table."}, {"start": "00:06:52", "is_lecture": true, "end": "00:06:56", "is_worked_example": false, "text": "How many bits per entry in the page table?"}, {"start": "00:06:56", "is_lecture": true, "end": "00:07:00", "is_worked_example": false, "text": "Assume each entry holds the PPN, the resident bit, and the dirty bit."}, {"start": "00:07:00", "is_lecture": true, "end": "00:07:07", "is_worked_example": false, "text": "Since the PPN is m bits, there are m+2 bits in each entry, so 16 bits."}, {"start": "00:07:07", "is_lecture": true, "end": "00:07:10", "is_worked_example": false, "text": "How many pages does the page table occupy?"}, {"start": "00:07:10", "is_lecture": true, "end": "00:07:23", "is_worked_example": false, "text": "There are 2^v page table entries, each occupying (m+2)/8 bytes, so the total size of the page table in this example is 2^23 bytes."}, {"start": "00:07:23", "is_lecture": true, "end": "00:07:35", "is_worked_example": false, "text": "Each page holds 2^p = 2^10 bytes, so the page table occupies 2^23/2^10 = 2^13 pages."}, {"start": "00:07:35", "is_lecture": true, "end": "00:07:40", "is_worked_example": false, "text": "What fraction of virtual memory can be resident at any given time?"}, {"start": "00:07:40", "is_lecture": true, "end": "00:07:45", "is_worked_example": false, "text": "There are 2^v virtual pages, of which 2^m can be resident."}, {"start": "00:07:45", "is_lecture": true, "end": "00:07:56", "is_worked_example": false, "text": "So the fraction of resident pages is 2^m/2^v = 2^14/2^22 = 1/2^8."}, {"start": "00:07:56", "is_lecture": true, "end": "00:08:03", "is_worked_example": false, "text": "What is the physical address for virtual address 0x1804?"}, {"start": "00:08:03", "is_lecture": true, "end": "00:08:07", "is_worked_example": false, "text": "Which MMU components are involved in the translation?"}, {"start": "00:08:07", "is_lecture": true, "end": "00:08:12", "is_worked_example": false, "text": "First we have to decompose the virtual address into VPN and offset."}, {"start": "00:08:12", "is_lecture": true, "end": "00:08:17", "is_worked_example": false, "text": "The offset is the low-order 10 bits, so is 0x004 in this example."}, {"start": "00:08:17", "is_lecture": true, "end": "00:08:22", "is_worked_example": false, "text": "The VPN is the remaining address bits, so the VPN is 0x6."}, {"start": "00:08:22", "is_lecture": true, "end": "00:08:28", "is_worked_example": false, "text": "Looking first in the TLB, we that the VPN-to-PPN mapping for VPN 0x6 is cached,"}, {"start": "00:08:28", "is_lecture": true, "end": "00:08:40", "is_worked_example": false, "text": "so we can construct the physical address by concatenating the PPN (0x2) with the 10-bit offset (0x4) to get a physical address of 0x804."}, {"start": "00:08:40", "is_lecture": true, "end": "00:08:41", "is_worked_example": false, "text": "You're right!"}, {"start": "00:08:41", "is_lecture": true, "end": "00:08:46", "is_worked_example": false, "text": "It's a bit of pain to do all the bit manipulations when p is not a multiple of 4."}, {"start": "00:08:46", "is_lecture": true, "end": "00:08:50", "is_worked_example": false, "text": "How about virtual address 0x1080?"}, {"start": "00:08:50", "is_lecture": true, "end": "00:08:55", "is_worked_example": false, "text": "For this address the VPN is 0x4 and the offset is 0x80."}, {"start": "00:08:55", "is_lecture": true, "end": "00:09:05", "is_worked_example": false, "text": "The translation for VPN 0x4 is not cached in the TLB, so we have to check the page map, which tells us that the page is resident in physical page 5."}, {"start": "00:09:05", "is_lecture": true, "end": "00:09:11", "is_worked_example": false, "text": "Concatenating the PPN and offset, we get 0x1480 as the physical address."}, {"start": "00:09:11", "is_lecture": true, "end": "00:09:16", "is_worked_example": false, "text": "Finally, how about virtual address 0x0FC?"}, {"start": "00:09:16", "is_lecture": true, "end": "00:09:20", "is_worked_example": false, "text": "Here the VPN is 0 and the offset 0xFC."}, {"start": "00:09:20", "is_lecture": true, "end": "00:09:32", "is_worked_example": false, "text": "The mapping for VPN 0 is not found in the TLB and checking the page map reveals that VPN 0 is not resident in main memory, so a page fault exception is triggered."}, {"start": "00:09:32", "is_lecture": true, "end": "00:09:40", "is_worked_example": false, "text": "There are a few things to note about the example TLB and page map contents."}, {"start": "00:09:40", "is_lecture": true, "end": "00:09:44", "is_worked_example": false, "text": "Note that a TLB entry can be invalid (it's R bit is 0)."}, {"start": "00:09:44", "is_lecture": true, "end": "00:09:53", "is_worked_example": false, "text": "This can happen when a virtual page is replaced, so when we change the R bit to 0 in the page map, we have to do the same in the TLB."}, {"start": "00:09:53", "is_lecture": true, "end": "00:09:58", "is_worked_example": false, "text": "And should we be concerned that PPN 0x5 appears twice in the page table?"}, {"start": "00:09:58", "is_lecture": true, "end": "00:10:04", "is_worked_example": false, "text": "Note that the entry for VPN 0x3 doesn't matter since it's R bit is 0."}, {"start": "00:10:04", "is_lecture": true, "end": "00:10:12", "is_worked_example": false, "text": "Typically when marking a page not resident, we don't bother to clear out the other fields in the entry since they won't be used when R=0."}, {"start": "00:10:12", "is_lecture": true, "end": "00:10:16", "is_worked_example": false, "text": "So there's only one *valid* mapping to PPN 5."}]}, "C05S01B04-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c5/c5s1/4?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c5s1v4", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:09", "is_worked_example": false, "text": "Let's review what happens when the CPU accesses a non-resident virtual page, i.e., a page with its resident bit set to 0."}, {"start": "00:00:09", "is_lecture": true, "end": "00:00:15", "is_worked_example": false, "text": "In the example shown here, the CPU is trying to access virtual page 5."}, {"start": "00:00:15", "is_lecture": true, "end": "00:00:27", "is_worked_example": false, "text": "In this case, the MMU signals a page fault exception, causing the CPU to suspend execution of the program and switch to the page fault handler, which is code that deals with the page fault."}, {"start": "00:00:27", "is_lecture": true, "end": "00:00:39", "is_worked_example": false, "text": "The handler starts by either finding an unused physical page or, if necessary, creating an unused page by selecting an in-use page and making it available."}, {"start": "00:00:39", "is_lecture": true, "end": "00:00:44", "is_worked_example": false, "text": "In our example, the handler has chosen virtual page 1 for reuse."}, {"start": "00:00:44", "is_lecture": true, "end": "00:00:55", "is_worked_example": false, "text": "If the selected page is dirty, i.e., its D bit is 1 indicating that its contents have changed since being read from secondary storage, write it back to secondary storage."}, {"start": "00:00:55", "is_lecture": true, "end": "00:01:00", "is_worked_example": false, "text": "Finally, mark the selected virtual page as no longer resident."}, {"start": "00:01:00", "is_lecture": true, "end": "00:01:07", "is_worked_example": false, "text": "In the \"after\" figure, we see that the R bit for virtual page 1 has been set to 0."}, {"start": "00:01:07", "is_lecture": true, "end": "00:01:10", "is_worked_example": false, "text": "Now physical page 4 is available for re-use."}, {"start": "00:01:10", "is_lecture": true, "end": "00:01:15", "is_worked_example": false, "text": "Are there any restrictions on which page we can select?"}, {"start": "00:01:15", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "Obviously, we can't select the page that holds the code for the page fault handler."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:24", "is_worked_example": false, "text": "Pages immune from selection are called \"wired\" pages."}, {"start": "00:01:24", "is_lecture": true, "end": "00:01:36", "is_worked_example": false, "text": "And it would very inefficient to choose the page that holds the code that made the initial memory access, since we expect to start executing that code as soon as we finish handling the page fault."}, {"start": "00:01:36", "is_lecture": true, "end": "00:01:42", "is_worked_example": false, "text": "The optimal strategy would be to choose the page whose next use will occur farthest in the future."}, {"start": "00:01:42", "is_lecture": true, "end": "00:01:49", "is_worked_example": false, "text": "But, of course, this involves knowledge of future execution paths and so isn't a realizable strategy."}, {"start": "00:01:49", "is_lecture": true, "end": "00:02:00", "is_worked_example": false, "text": "Wikipedia provides a nice description of the many strategies for choosing a replacement page, with their various tradeoffs between ease of implementation and impact on the rate of page faults --"}, {"start": "00:02:00", "is_lecture": true, "end": "00:02:03", "is_worked_example": false, "text": "see the URL given at the bottom of the slide."}, {"start": "00:02:03", "is_lecture": true, "end": "00:02:13", "is_worked_example": false, "text": "The aging algorithm they describe is frequently used since it offers near optimal performance at a moderate implementation cost."}, {"start": "00:02:13", "is_lecture": true, "end": "00:02:20", "is_worked_example": false, "text": "Next, the desired virtual page is read from secondary storage into the selected physical page."}, {"start": "00:02:20", "is_lecture": true, "end": "00:02:26", "is_worked_example": false, "text": "In our example, virtual page 5 is now loaded into physical page 4."}, {"start": "00:02:26", "is_lecture": true, "end": "00:02:37", "is_worked_example": false, "text": "Then the R bit and PPN fields in the page table entry for virtual page 5 are updated to indicate that the contents of that virtual page now reside in physical page 4."}, {"start": "00:02:37", "is_lecture": true, "end": "00:02:46", "is_worked_example": false, "text": "Finally the handler is finished and execution of the original program is resumed, re-executing the instruction that caused the page fault."}, {"start": "00:02:46", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "Since the page map has been updated, this time the access succeeds and execution continues."}, {"start": "00:02:53", "is_lecture": true, "end": "00:02:58", "is_worked_example": false, "text": "To double-check our understanding of page faults, let's run through an example."}, {"start": "00:02:58", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "Here's the same setup as in our previous example, but this time consider a store instruction that's making an access to virtual address 0x600, which is located on virtual page 6."}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:23", "is_worked_example": false, "text": "Checking the page table entry for VPN 6, we see that its R bit 0 indicating that it is NOT resident in main memory, which causes a page fault exception."}, {"start": "00:03:23", "is_lecture": true, "end": "00:03:30", "is_worked_example": false, "text": "The page fault handler selects VPN 0xE for replacement since we've been told in the setup that it's the least-recently-used page."}, {"start": "00:03:30", "is_lecture": true, "end": "00:03:42", "is_worked_example": false, "text": "The page table entry for VPN 0xE has D=1 so the handler writes the contents of VPN 0xE, which is found in PPN 0x5, to secondary storage."}, {"start": "00:03:42", "is_lecture": true, "end": "00:03:48", "is_worked_example": false, "text": "Then it updates the page table to indicate that VPN 0xE is no longer resident."}, {"start": "00:03:48", "is_lecture": true, "end": "00:03:55", "is_worked_example": false, "text": "Next, the contents of VPN 0x6 are read from secondary storage into the now available PPN 0x5."}, {"start": "00:03:55", "is_lecture": true, "end": "00:04:04", "is_worked_example": false, "text": "Now the handler updates the page table entry for VPN 0x6 to indicate that it's resident in PPN 0x5."}, {"start": "00:04:04", "is_lecture": true, "end": "00:04:12", "is_worked_example": false, "text": "The page fault handler has completed its work, so program execution resumes and the ST instruction is re-executed."}, {"start": "00:04:12", "is_lecture": true, "end": "00:04:21", "is_worked_example": false, "text": "This time the MMU is able to translate virtual address 0x600 to physical address 0x500."}, {"start": "00:04:21", "is_lecture": true, "end": "00:04:27", "is_worked_example": false, "text": "And since the ST instruction modifies the contents of VPN 0x6, its D bit is set to 1."}, {"start": "00:04:27", "is_lecture": true, "end": "00:04:29", "is_worked_example": false, "text": "Whew!  We're done :)"}, {"start": "00:04:29", "is_lecture": true, "end": "00:04:38", "is_worked_example": false, "text": "We can think of the work of the MMU as being divided into two tasks, which as computer scientists, we would think of as two procedures."}, {"start": "00:04:38", "is_lecture": true, "end": "00:04:45", "is_worked_example": false, "text": "In this formulation the information in the page map is held in several arrays: the R array holds the resident bits,"}, {"start": "00:04:45", "is_lecture": true, "end": "00:04:50", "is_worked_example": false, "text": "the D array holds the dirty bits, the PPN array holds the physical page numbers,"}, {"start": "00:04:50", "is_lecture": true, "end": "00:04:57", "is_worked_example": false, "text": "and the DiskAdr array holds the location in secondary storage for each virtual page."}, {"start": "00:04:57", "is_lecture": true, "end": "00:05:05", "is_worked_example": false, "text": "The VtoP procedure is invoked on each memory access to translate the virtual address into a physical address."}, {"start": "00:05:05", "is_lecture": true, "end": "00:05:11", "is_worked_example": false, "text": "If the requested virtual page is not resident, the PageFault procedure is invoked to make the page resident."}, {"start": "00:05:11", "is_lecture": true, "end": "00:05:23", "is_worked_example": false, "text": "Once the requested page is resident, the VPN is used as an index to lookup the corresponding PPN, which is then concatenated with the page offset to form the physical address."}, {"start": "00:05:23", "is_lecture": true, "end": "00:05:30", "is_worked_example": false, "text": "The PageFault routine starts by selecting a virtual page to be replaced, writing out its contents if it's dirty."}, {"start": "00:05:30", "is_lecture": true, "end": "00:05:34", "is_worked_example": false, "text": "The selected page is then marked as not resident."}, {"start": "00:05:34", "is_lecture": true, "end": "00:05:45", "is_worked_example": false, "text": "Finally the desired virtual page is read from secondary storage and the page map information updated to reflect that it's now resident in the newly filled physical page."}, {"start": "00:05:45", "is_lecture": true, "end": "00:05:52", "is_worked_example": false, "text": "We'll use hardware to implement the VtoP functionality since it's needed for every memory access."}, {"start": "00:05:52", "is_lecture": true, "end": "00:06:04", "is_worked_example": false, "text": "The call to the PageFault procedure is accomplished via a page fault exception, which directs the CPU to execute the appropriate handler software that contains the PageFault procedure."}, {"start": "00:06:04", "is_lecture": true, "end": "00:06:17", "is_worked_example": false, "text": "This is a good strategy to pursue in all our implementation choices: use hardware for the operations that need to be fast, but use exceptions to handle the (hopefully infrequent) exceptional cases in software."}, {"start": "00:06:17", "is_lecture": true, "end": "00:06:33", "is_worked_example": false, "text": "Since the software is executed by the CPU, which is itself a piece of hardware, what we're really doing is making the tradeoff between using special-purpose hardware (e.g., the MMU) or using general-purpose hardware (e.g., the CPU)."}, {"start": "00:06:33", "is_lecture": true, "end": "00:06:47", "is_worked_example": false, "text": "In general, one should be skeptical of proposals to use special-purpose hardware, reserving that choice for operations that truly are commonplace and whose performance is critical to the overall performance of the system."}]}, "C05S01B09-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c5/c5s1/9?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c5s1v9", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:06", "is_worked_example": false, "text": "There are a few MMU implementation details we can tweak for more efficiency or functionality."}, {"start": "00:00:06", "is_lecture": true, "end": "00:00:13", "is_worked_example": false, "text": "In our simple page-map implementation, the full page map occupies some number of physical pages."}, {"start": "00:00:13", "is_lecture": true, "end": "00:00:23", "is_worked_example": false, "text": "Using the numbers shown here, if each page map occupies one word of main memory, we'd need 2^20 words (or 2^12 pages) to hold the page table."}, {"start": "00:00:23", "is_lecture": true, "end": "00:00:33", "is_worked_example": false, "text": "If we have multiple contexts, we would need multiple page tables, and the demands on our physical memory resources would start to get large."}, {"start": "00:00:33", "is_lecture": true, "end": "00:00:39", "is_worked_example": false, "text": "The MMU implementation shown here uses a hierarchical page map."}, {"start": "00:00:39", "is_lecture": true, "end": "00:00:50", "is_worked_example": false, "text": "The top 10 bits of virtual address are used to access a \"page directory\", which indicates the physical page that holds the page map for that segment of the virtual address space."}, {"start": "00:00:50", "is_lecture": true, "end": "00:00:59", "is_worked_example": false, "text": "The key idea is that the page map segments are in virtual memory, i.e., they don't all have to be resident at any given time."}, {"start": "00:00:59", "is_lecture": true, "end": "00:01:12", "is_worked_example": false, "text": "If the running application is only actively using a small portion of its virtual address space, we may only need a handful of pages to hold the page directory and the necessary page map segments."}, {"start": "00:01:12", "is_lecture": true, "end": "00:01:18", "is_worked_example": false, "text": "The resultant savings really add up when there are many applications, each with their own context."}, {"start": "00:01:18", "is_lecture": true, "end": "00:01:40", "is_worked_example": false, "text": "In this example, note that the middle entries in the page directory, i.e., the entries corresponding to the as-yet unallocated virtual memory between the stack and heap, are all marked as not resident. .133 So no page map resources need be devoted to holding a zillion page map entries all marked \"not resident\"."}, {"start": "00:01:40", "is_lecture": true, "end": "00:01:50", "is_worked_example": false, "text": "Accessing the page map now requires two access to main memory (first to the page directory, then to the appropriate segment of the page map),"}, {"start": "00:01:50", "is_lecture": true, "end": "00:01:54", "is_worked_example": false, "text": "but the TLB makes the impact of that additional access negligible."}, {"start": "00:01:54", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "Normally when changing contexts, the OS would reload the page-table pointer to point to the appropriate page table (or page table directory if we adopt the scheme from the previous slide)."}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:17", "is_worked_example": false, "text": "Since this context switch in effect changes all the entries in the page table, the OS would also have to invalidate all the entries in the TLB cache."}, {"start": "00:02:17", "is_lecture": true, "end": "00:02:30", "is_worked_example": false, "text": "This naturally has a huge impact on the TLB hit ratio and the average memory access time takes a huge hit because of the all page map accesses that are now necessary until the TLB is refilled."}, {"start": "00:02:30", "is_lecture": true, "end": "00:02:43", "is_worked_example": false, "text": "To reduce the impact of context switches, some MMUs include a context-number register whose contents are concatenated with the virtual page number to form the query to the TLB."}, {"start": "00:02:43", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "Essentially this means that the tag field in the TLB cache entries will expand to include the context number provided at the time the TLB entry was filled."}, {"start": "00:02:53", "is_lecture": true, "end": "00:03:01", "is_worked_example": false, "text": "To switch contexts, the OS would now reload both the context-number register and the page-table pointer."}, {"start": "00:03:01", "is_lecture": true, "end": "00:03:10", "is_worked_example": false, "text": "With a new context number, entries in the TLB for other contexts would no longer match, so no need to flush the TLB on a context switch."}, {"start": "00:03:10", "is_lecture": true, "end": "00:03:24", "is_worked_example": false, "text": "If the TLB has sufficient capacity to cache the VPN-to-PPN mappings for several contexts, context switches would no longer have a substantial impact on average memory access time."}, {"start": "00:03:24", "is_lecture": true, "end": "00:03:32", "is_worked_example": false, "text": "Finally, let's return to the question about how to incorporate both a cache and an MMU into our memory system."}, {"start": "00:03:32", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "The first choice is to place the cache between the CPU and the MMU, i.e., the cache would work on virtual addresses."}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "This seems good: the cost of the VPN-to-PPN translation is only incurred on a cache miss."}, {"start": "00:03:47", "is_lecture": true, "end": "00:03:54", "is_worked_example": false, "text": "The difficulty comes when there's a context switch, which changes the effective contents of virtual memory."}, {"start": "00:03:54", "is_lecture": true, "end": "00:04:00", "is_worked_example": false, "text": "After all that was the point of the context switch, since we want to switch execution to another program."}, {"start": "00:04:00", "is_lecture": true, "end": "00:04:13", "is_worked_example": false, "text": "But that means the OS would have to invalidate all the entries in the cache when performing a context switch, which makes the cache miss ratio quite large until the cache is refilled."}, {"start": "00:04:13", "is_lecture": true, "end": "00:04:18", "is_worked_example": false, "text": "So once again the performance impact of a context switch would be quite high."}, {"start": "00:04:18", "is_lecture": true, "end": "00:04:27", "is_worked_example": false, "text": "We can solve this problem by caching physical addresses, i.e., placing the cache between the MMU and main memory."}, {"start": "00:04:27", "is_lecture": true, "end": "00:04:31", "is_worked_example": false, "text": "Thus the contents of the cache are unaffected by context switches --"}, {"start": "00:04:31", "is_lecture": true, "end": "00:04:36", "is_worked_example": false, "text": "the requested physical addresses will be different, but the cache handles that in due course."}, {"start": "00:04:36", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "The downside of this approach is that we have to incur the cost of the MMU translation before we can start the cache access, slightly increasing the average memory access time."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:55", "is_worked_example": false, "text": "But if we're clever we don't have to wait for the MMU to finish before starting the access to the cache."}, {"start": "00:04:55", "is_lecture": true, "end": "00:05:02", "is_worked_example": false, "text": "To get started, the cache needs the line number from the virtual address in order to fetch the appropriate cache line."}, {"start": "00:05:02", "is_lecture": true, "end": "00:05:13", "is_worked_example": false, "text": "If the address bits used for the line number are completely contained in the page offset of the virtual address, these bits are unaffected by the MMU translation,"}, {"start": "00:05:13", "is_lecture": true, "end": "00:05:19", "is_worked_example": false, "text": "and so the cache lookup can happen in parallel with the MMU operation."}, {"start": "00:05:19", "is_lecture": true, "end": "00:05:29", "is_worked_example": false, "text": "Once the cache lookup is complete, the tag field of the cache line can be compared with the appropriate bits of the physical address produced by the MMU."}, {"start": "00:05:29", "is_lecture": true, "end": "00:05:38", "is_worked_example": false, "text": "If there was a TLB hit in the MMU, the physical address should be available at about the same time as the tag field produced by the cache lookup."}, {"start": "00:05:38", "is_lecture": true, "end": "00:05:46", "is_worked_example": false, "text": "By performing the MMU translation and cache lookup in parallel, there's usually no impact on the average memory access time!"}, {"start": "00:05:46", "is_lecture": true, "end": "00:05:54", "is_worked_example": false, "text": "Voila, the best of both worlds: a physically addressed cache that incurs no time penalty for MMU translation."}, {"start": "00:05:54", "is_lecture": true, "end": "00:06:06", "is_worked_example": false, "text": "One final detail: one way to increase the capacity of the cache is to increase the number of cache lines and hence the number of bits of address used as the line number."}, {"start": "00:06:06", "is_lecture": true, "end": "00:06:14", "is_worked_example": false, "text": "Since we want the line number to fit into the page offset field of the virtual address, we're limited in how many cache lines we can have."}, {"start": "00:06:14", "is_lecture": true, "end": "00:06:19", "is_worked_example": false, "text": "The same argument applies to increasing the block size."}, {"start": "00:06:19", "is_lecture": true, "end": "00:06:31", "is_worked_example": false, "text": "So to increase the capacity of the cache our only option is to increase the cache associativity, which adds capacity without affecting the address bits used for the line number."}, {"start": "00:06:31", "is_lecture": true, "end": "00:06:35", "is_worked_example": false, "text": "That's it for our discussion of virtual memory."}, {"start": "00:06:35", "is_lecture": true, "end": "00:06:41", "is_worked_example": false, "text": "We use the MMU to provide the context for mapping virtual addresses to physical addresses."}, {"start": "00:06:41", "is_lecture": true, "end": "00:06:53", "is_worked_example": false, "text": "By switching contexts we can create the illusion of many virtual address spaces, so many programs can share a single CPU and physical memory without interfering with each other."}, {"start": "00:06:53", "is_lecture": true, "end": "00:06:59", "is_worked_example": false, "text": "We discussed using a page map to translate virtual page numbers to physical page numbers."}, {"start": "00:06:59", "is_lecture": true, "end": "00:07:11", "is_worked_example": false, "text": "To save costs, we located the page map in physical memory and used a TLB to eliminate the cost of accessing the page map for most virtual memory accesses."}, {"start": "00:07:11", "is_lecture": true, "end": "00:07:23", "is_worked_example": false, "text": "Access to a non-resident page causes a page fault exception, allowing the OS to manage the complexities of equitably sharing physical memory across many applications."}, {"start": "00:07:23", "is_lecture": true, "end": "00:07:33", "is_worked_example": false, "text": "We saw that providing contexts was the first step towards creating virtual machines, which is the topic of our next lecture."}]}, "C09S01B04-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c9/c9s1/4?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c9s1v4", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:03", "is_worked_example": false, "text": "Let's take a moment to look at a different example."}, {"start": "00:00:03", "is_lecture": true, "end": "00:00:10", "is_worked_example": false, "text": "Automated teller machines allow bank customers to perform a variety of transactions: deposits, withdrawals, transfers, etc."}, {"start": "00:00:10", "is_lecture": true, "end": "00:00:17", "is_worked_example": false, "text": "Let's consider what happens when two customers try to withdraw $50 from the same account at the same time."}, {"start": "00:00:17", "is_lecture": true, "end": "00:00:22", "is_worked_example": false, "text": "A portion of the bank's code for a withdrawal transaction is shown in the upper right."}, {"start": "00:00:22", "is_lecture": true, "end": "00:00:27", "is_worked_example": false, "text": "This code is responsible for adjusting the account balance to reflect the amount of the withdrawal."}, {"start": "00:00:27", "is_lecture": true, "end": "00:00:32", "is_worked_example": false, "text": "Presumably the check to see if there is sufficient funds has already happened."}, {"start": "00:00:32", "is_lecture": true, "end": "00:00:34", "is_worked_example": false, "text": "What's supposed to happen?"}, {"start": "00:00:34", "is_lecture": true, "end": "00:00:47", "is_worked_example": false, "text": "Let's assume that the bank is using a separate process to handle each transaction, so the two withdrawal transactions cause two different processes to be created, each of which will run the Debit code."}, {"start": "00:00:47", "is_lecture": true, "end": "00:01:00", "is_worked_example": false, "text": "If each of the calls to Debit run to completion without interruption, we get the desired outcome: the first transaction debits the account by $50, then the second transaction does the same."}, {"start": "00:01:00", "is_lecture": true, "end": "00:01:06", "is_worked_example": false, "text": "The net result is that you and your friend have $100 and the balance is $100 less."}, {"start": "00:01:06", "is_lecture": true, "end": "00:01:08", "is_worked_example": false, "text": "So far, so good."}, {"start": "00:01:08", "is_lecture": true, "end": "00:01:14", "is_worked_example": false, "text": "But what if the process for the first transaction is interrupted just after it's read the balance?"}, {"start": "00:01:14", "is_lecture": true, "end": "00:01:19", "is_worked_example": false, "text": "The second process subtracts $50 from the balance, completing that transaction."}, {"start": "00:01:19", "is_lecture": true, "end": "00:01:27", "is_worked_example": false, "text": "Now the first process resumes, using the now out-of-date balance it loaded just before being interrupted."}, {"start": "00:01:27", "is_lecture": true, "end": "00:01:33", "is_worked_example": false, "text": "The net result is that you and your friend have $100, but the balance has only been debited by $50."}, {"start": "00:01:33", "is_lecture": true, "end": "00:01:44", "is_worked_example": false, "text": "The moral of the story is that we need to be careful when writing code that reads and writes shared data since other processes might modify the data in the middle of our execution."}, {"start": "00:01:44", "is_lecture": true, "end": "00:01:53", "is_worked_example": false, "text": "When, say, updating a shared memory location, we'll need to LD the current value, modify it, then ST the updated value."}, {"start": "00:01:53", "is_lecture": true, "end": "00:02:02", "is_worked_example": false, "text": "We would like to ensure that no other processes access the shared location between the start of the LD and the completion of the ST."}, {"start": "00:02:02", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "The LD/modify/ST code sequence is what we call a \"critical section\"."}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:15", "is_worked_example": false, "text": "We need to arrange that other processes attempting to execute the same critical section are delayed until our execution is complete."}, {"start": "00:02:15", "is_lecture": true, "end": "00:02:24", "is_worked_example": false, "text": "This constraint is called \"mutual exclusion\", i.e., only one process at a time can be executing code in the same critical section."}, {"start": "00:02:24", "is_lecture": true, "end": "00:02:32", "is_worked_example": false, "text": "Once we've identified critical sections, we'll use semaphores to guarantee they execute atomically,"}, {"start": "00:02:32", "is_lecture": true, "end": "00:02:41", "is_worked_example": false, "text": "i.e., that once execution of the critical section begins, no other process will be able to enter the critical section until the execution is complete."}, {"start": "00:02:41", "is_lecture": true, "end": "00:02:51", "is_worked_example": false, "text": "The combination of the semaphore to enforce the mutual exclusion constraint and the critical section of code implement what's called a \"transaction\"."}, {"start": "00:02:51", "is_lecture": true, "end": "00:03:01", "is_worked_example": false, "text": "A transaction can perform multiple reads and writes of shared data with the guarantee that none of the data will be read or written by other processes while the transaction is in progress."}, {"start": "00:03:01", "is_lecture": true, "end": "00:03:06", "is_worked_example": false, "text": "Here's the original code to Debit, which we'll modify by adding a LOCK semaphore."}, {"start": "00:03:06", "is_lecture": true, "end": "00:03:12", "is_worked_example": false, "text": "In this case, the resource controlled by the semaphore is the right to run the code in the critical section."}, {"start": "00:03:12", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "By initializing LOCK to 1, we're saying that at most one process can execute the critical section at a time."}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:25", "is_worked_example": false, "text": "A process running the Debit code WAITs on the LOCK semaphore."}, {"start": "00:03:25", "is_lecture": true, "end": "00:03:34", "is_worked_example": false, "text": "If the value of LOCK is 1, the WAIT will decrement value of LOCK to 0 and let the process enter the critical section."}, {"start": "00:03:34", "is_lecture": true, "end": "00:03:36", "is_worked_example": false, "text": "This is called acquiring the lock."}, {"start": "00:03:36", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "If the value of LOCK is 0, some other process has acquired the lock and is executing the critical section and our execution is suspended until the LOCK value is non-zero."}, {"start": "00:03:47", "is_lecture": true, "end": "00:03:58", "is_worked_example": false, "text": "When the process completes execution of the critical section, it releases the LOCK with a call to SIGNAL, which will allow other processes to enter the critical section."}, {"start": "00:03:58", "is_lecture": true, "end": "00:04:05", "is_worked_example": false, "text": "If there are multiple WAITing processes, only one will be able to acquire the lock, and the others will still have to wait their turn."}, {"start": "00:04:05", "is_lecture": true, "end": "00:04:16", "is_worked_example": false, "text": "Used in this manner, semaphores are implementing a mutual exclusion constraint, i.e., there's a guarantee that two executions of the critical section cannot overlap."}, {"start": "00:04:16", "is_lecture": true, "end": "00:04:25", "is_worked_example": false, "text": "Note that if multiple processes need to execute the critical section, they may run in any order and the only guarantee is that their executions will not overlap."}, {"start": "00:04:25", "is_lecture": true, "end": "00:04:30", "is_worked_example": false, "text": "There are some interesting engineering issues to consider."}, {"start": "00:04:30", "is_lecture": true, "end": "00:04:36", "is_worked_example": false, "text": "There's the question of the granularity of the lock, i.e., what shared data is controlled by the lock?"}, {"start": "00:04:36", "is_lecture": true, "end": "00:04:43", "is_worked_example": false, "text": "In our bank example, should there be one lock controlling access to the balance for all accounts?"}, {"start": "00:04:43", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "That would mean that no one could access any balance while a transaction was in progress."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:56", "is_worked_example": false, "text": "That would mean that transactions accessing different accounts would have to run one after the other even though they're accessing different data."}, {"start": "00:04:56", "is_lecture": true, "end": "00:05:05", "is_worked_example": false, "text": "So one lock for all the balances would introduce unnecessary precedence constraints, greatly slowing the rate at which transactions could be processed."}, {"start": "00:05:05", "is_lecture": true, "end": "00:05:12", "is_worked_example": false, "text": "Since the guarantee we need is that we shouldn't permit multiple simultaneous transactions on the same account,"}, {"start": "00:05:12", "is_lecture": true, "end": "00:05:20", "is_worked_example": false, "text": "it would make more sense to have a separate lock for each account, and change the Debit code to acquire the account's lock before proceeding."}, {"start": "00:05:20", "is_lecture": true, "end": "00:05:32", "is_worked_example": false, "text": "That will only delay transactions that truly overlap, an important efficiency consideration for a large system processing many thousands of mostly non-overlapping transactions each second."}, {"start": "00:05:32", "is_lecture": true, "end": "00:05:37", "is_worked_example": false, "text": "Of course, having per-account locks would mean a lot of locks!"}, {"start": "00:05:37", "is_lecture": true, "end": "00:05:47", "is_worked_example": false, "text": "If that's a concern, we can adopt a compromise strategy of having locks that protect groups of accounts, e.g., accounts with same last three digits in the account number."}, {"start": "00:05:47", "is_lecture": true, "end": "00:05:55", "is_worked_example": false, "text": "That would mean we'd only need 1000 locks, which would allow up to 1000 transactions to happen simultaneously."}, {"start": "00:05:55", "is_lecture": true, "end": "00:06:04", "is_worked_example": false, "text": "The notion of transactions on shared data is so useful that we often use a separate system called a database that provides the desired functionality."}, {"start": "00:06:04", "is_lecture": true, "end": "00:06:12", "is_worked_example": false, "text": "Database systems are engineered to provide low-latency access to shared data, providing the appropriate transactional semantics."}, {"start": "00:06:12", "is_lecture": true, "end": "00:06:17", "is_worked_example": false, "text": "The design and implementation of databases and transactions is pretty interesting."}, {"start": "00:06:17", "is_lecture": true, "end": "00:06:21", "is_worked_example": false, "text": "To follow up, I recommend reading about databases on the web."}, {"start": "00:06:21", "is_lecture": true, "end": "00:06:30", "is_worked_example": false, "text": "Returning to our producer/consumer example, we see that if multiple producers are trying to insert characters into the buffer at the same time,"}, {"start": "00:06:30", "is_lecture": true, "end": "00:06:39", "is_worked_example": false, "text": "it's possible that their execution may overlap in a way that causes characters to be overwritten and/or the index to be improperly incremented."}, {"start": "00:06:39", "is_lecture": true, "end": "00:06:42", "is_worked_example": false, "text": "We just saw this bug in the bank example:"}, {"start": "00:06:42", "is_lecture": true, "end": "00:06:52", "is_worked_example": false, "text": "the producer code contains a critical section of code that accesses the FIFO buffer and we need to ensure that the critical section is executed atomically."}, {"start": "00:06:52", "is_lecture": true, "end": "00:07:03", "is_worked_example": false, "text": "Here we've added a third semaphore, called LOCK, to implement the necessary mutual exclusion constraint for the critical section of code that inserts characters into the FIFO buffer."}, {"start": "00:07:03", "is_lecture": true, "end": "00:07:10", "is_worked_example": false, "text": "With this modification, the system will now work correctly when there are multiple producer processes."}, {"start": "00:07:10", "is_lecture": true, "end": "00:07:18", "is_worked_example": false, "text": "There's a similar issue with multiple consumers, so we've used the same LOCK to protect the critical section for reading from the buffer in the RCV code."}, {"start": "00:07:18", "is_lecture": true, "end": "00:07:29", "is_worked_example": false, "text": "Using the same LOCK for producers and consumers will work, but does introduce unnecessary precedence constraints since producers and consumers use different indices,"}, {"start": "00:07:29", "is_lecture": true, "end": "00:07:33", "is_worked_example": false, "text": "i.e., IN for producers and OUT for consumers."}, {"start": "00:07:33", "is_lecture": true, "end": "00:07:39", "is_worked_example": false, "text": "To solve this problem we could use two locks: one for producers and one for consumers."}, {"start": "00:07:39", "is_lecture": true, "end": "00:07:45", "is_worked_example": false, "text": "Semaphores are a pretty handy swiss army knife when it comes to dealing with synchronization issues."}, {"start": "00:07:45", "is_lecture": true, "end": "00:07:53", "is_worked_example": false, "text": "When WAIT and SIGNAL appear in different processes, the semaphore ensures the correct execution timing between processes."}, {"start": "00:07:53", "is_lecture": true, "end": "00:08:02", "is_worked_example": false, "text": "In our example, we used two semaphores to ensure that consumers can't read from an empty buffer and that producers can't write into a full buffer."}, {"start": "00:08:02", "is_lecture": true, "end": "00:08:13", "is_worked_example": false, "text": "We also used semaphores to ensure that execution of critical sections -- in our example, updates of the indices IN and OUT -- were guaranteed to be atomic."}, {"start": "00:08:13", "is_lecture": true, "end": "00:08:25", "is_worked_example": false, "text": "In other words, that the sequence of reads and writes needed to increment a shared index would not be interrupted by another process between the initial read of the index and the final write."}]}, "C12S01B06-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c12/c12s1/6?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c12s1v6", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:03", "is_worked_example": false, "text": "And now we've reached the end of 6.004."}, {"start": "00:00:03", "is_lecture": true, "end": "00:00:11", "is_worked_example": false, "text": "Looking back, there are two ways of thinking about the material we've discussed, the skills we've practiced, and the designs we've completed."}, {"start": "00:00:11", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "Starting at devices, we've worked our way up the design hierarchy, each level serving as building blocks for the next."}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "Along the way we thought about design tradeoffs, choosing the alternatives that would make our systems reliable, efficient and easy to understand and hence easy to maintain."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:40", "is_worked_example": false, "text": "In the other view of 6.004, we created and then used a hierarchy of engineering abstractions that are reasonably independent of the technologies they encapsulate."}, {"start": "00:00:40", "is_lecture": true, "end": "00:00:47", "is_worked_example": false, "text": "Even though technologies change at a rapid pace, these abstractions embody principles that are timeless."}, {"start": "00:00:47", "is_lecture": true, "end": "00:00:57", "is_worked_example": false, "text": "For example, the symbolic logic described by George Boole in 1847 is still used to reason about the operation of digital circuits you and I design today."}, {"start": "00:00:57", "is_lecture": true, "end": "00:01:06", "is_worked_example": false, "text": "The power of engineering abstractions is that they allow us to reason about the behavior of a system based on the behavior of the components"}, {"start": "00:01:06", "is_lecture": true, "end": "00:01:10", "is_worked_example": false, "text": "without having to understand the implementation details of each component."}, {"start": "00:01:10", "is_lecture": true, "end": "00:01:21", "is_worked_example": false, "text": "The advantage of viewing components as \"black boxes\" implementing some specified function is that the implementation can change as long as the same specification is satisfied."}, {"start": "00:01:21", "is_lecture": true, "end": "00:01:34", "is_worked_example": false, "text": "In my lifetime, the size of a 2-input NAND gate has shrunk by 10 orders of magnitude, yet a 50-year-old logic design would still work as intended if implemented in today's technologies."}, {"start": "00:01:34", "is_lecture": true, "end": "00:01:44", "is_worked_example": false, "text": "Imagine trying to build a circuit that added two binary numbers if you had to reason about the electrical properties of doped silicon and conducting metals."}, {"start": "00:01:44", "is_lecture": true, "end": "00:01:53", "is_worked_example": false, "text": "Using abstractions lets us limit the design complexity at each level, shortening design time and making it easier to verify that the specifications have been met."}, {"start": "00:01:53", "is_lecture": true, "end": "00:02:01", "is_worked_example": false, "text": "And once we've created a useful repertoire of building blocks, we can use them again and again to assemble many different systems."}, {"start": "00:02:01", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "Our goal in 6.004 is to demystify how computers work, starting with MOSFETs and working our way up to operating systems."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:18", "is_worked_example": false, "text": "We hope you've understood the engineering abstractions we've introduced and had a chance to practice using them when completing the design problems offered in the labs."}, {"start": "00:02:18", "is_lecture": true, "end": "00:02:27", "is_worked_example": false, "text": "We also hope that you'll also understand their limitations and have the confidence to create new abstractions when tackling new engineering tasks."}, {"start": "00:02:27", "is_lecture": true, "end": "00:02:32", "is_worked_example": false, "text": "Good engineers use abstractions, but great engineers create them."}, {"start": "00:06:04", "is_lecture": true, "end": "00:02:40", "is_worked_example": false, "text": "principles used at each level of the design hierarchy."}, {"start": "00:02:40", "is_lecture": true, "end": "00:02:49", "is_worked_example": false, "text": "If a particular topic struck you as especially interesting, we hope you'll seek out a more advanced course that will let you dig deeper into that engineering discipline."}, {"start": "00:02:49", "is_lecture": true, "end": "00:02:56", "is_worked_example": false, "text": "Hundreds of thousands of engineers have worked to create the digital systems that are the engines of today's information society."}, {"start": "00:02:56", "is_lecture": true, "end": "00:03:05", "is_worked_example": false, "text": "As you can imagine, there's no end of interesting engineering to explore and master -- so roll up your sleeves and come join in the fun!"}, {"start": "00:03:05", "is_lecture": true, "end": "00:03:08", "is_worked_example": false, "text": "What will be the engineering challenges of tomorrow?"}, {"start": "00:03:08", "is_lecture": true, "end": "00:03:13", "is_worked_example": false, "text": "Here are a few thoughts about how the future of computing may be very different than the present."}, {"start": "00:03:13", "is_lecture": true, "end": "00:03:17", "is_worked_example": false, "text": "The systems we build today have a well-defined notion of state:"}, {"start": "00:03:17", "is_lecture": true, "end": "00:03:23", "is_worked_example": false, "text": "the exact digital values stored in their memories, produced by their logic components, and traveling along their interconnect."}, {"start": "00:03:23", "is_lecture": true, "end": "00:03:34", "is_worked_example": false, "text": "But computation based on the principles of quantum mechanics may allow us to solve what are now intractable problems using states described not as collections of 1's and 0's,"}, {"start": "00:03:34", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "but as interrelated probabilities that describe the superposition of many states."}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:50", "is_worked_example": false, "text": "We've built our systems using voltages to encode information and voltage-controlled switches to perform computation, using silicon-based electrical devices."}, {"start": "00:03:50", "is_lecture": true, "end": "00:03:59", "is_worked_example": false, "text": "But the chemistry of life has been carrying out detailed manufacturing operations for millennia using information encoded as sequences of amino acids."}, {"start": "00:03:59", "is_lecture": true, "end": "00:04:07", "is_worked_example": false, "text": "Some of the information encoded in our DNA has been around for millions of years, a truly long-lived information system!"}, {"start": "00:04:07", "is_lecture": true, "end": "00:04:13", "is_worked_example": false, "text": "Today biologists are starting to build computational components from biological materials."}, {"start": "00:04:13", "is_lecture": true, "end": "00:04:18", "is_worked_example": false, "text": "Maybe in 50 years instead of plugging in your laptop, you'll have to feed it :)"}, {"start": "00:04:18", "is_lecture": true, "end": "00:04:28", "is_worked_example": false, "text": "Instead of using truth tables and logic functions, some computations are best performed neural networks that operate by forming appropriately weighted combinations of analog inputs,"}, {"start": "00:04:28", "is_lecture": true, "end": "00:04:34", "is_worked_example": false, "text": "where the weights are learned by the system as it is trained using example inputs that should produce known outputs."}, {"start": "00:04:34", "is_lecture": true, "end": "00:04:40", "is_worked_example": false, "text": "Artificial neural nets are thought to model the operation of the synapses and neurons in our brains."}, {"start": "00:04:40", "is_lecture": true, "end": "00:04:49", "is_worked_example": false, "text": "As we learn more about how the brain operates, we may get many new insights into how to implement systems that are good at recognition and reasoning."}, {"start": "00:04:49", "is_lecture": true, "end": "00:05:00", "is_worked_example": false, "text": "Again using living organisms as useful models, programming may be replaced by learning, where stimulus and feedback are used to evolve system behavior."}, {"start": "00:05:00", "is_lecture": true, "end": "00:05:08", "is_worked_example": false, "text": "In other words, systems will use adaptation mechanisms to evolve the desired functionality rather than have it explicitly programmed."}, {"start": "00:05:08", "is_lecture": true, "end": "00:05:17", "is_worked_example": false, "text": "This all seems the stuff of science fiction, but I suspect our parents feel the same way about having conversations with Siri about tomorrow's weather."}, {"start": "00:05:17", "is_lecture": true, "end": "00:05:20", "is_worked_example": false, "text": "Thanks for joining us here in 6.004."}, {"start": "00:05:20", "is_lecture": true, "end": "00:05:26", "is_worked_example": false, "text": "We've enjoyed presenting the material and challenging you with design tasks to exercise your new skills and understanding."}, {"start": "00:05:26", "is_lecture": true, "end": "00:05:33", "is_worked_example": false, "text": "There are interesting times ahead in the world of digital systems and we can certainly use your help in inventing the future!"}, {"start": "00:05:33", "is_lecture": true, "end": "00:05:39", "is_worked_example": false, "text": "We'd welcome any feedback you have about the course so please feel free leave comments in the forum."}, {"start": "00:05:39", "is_lecture": true, "end": "00:05:43", "is_worked_example": false, "text": "Good bye for now... and good luck in your future studies."}]}, "C06S01B01-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c6/c6s1/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c6s1v1", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:13", "is_worked_example": false, "text": "In the last lecture we introduced the notion of virtual memory and added a Memory Management Unit (MMU) to translate the virtual addresses generated by the CPU to the physical addresses sent to main memory."}, {"start": "00:00:13", "is_lecture": true, "end": "00:00:23", "is_worked_example": false, "text": "This gave us the ability to share physical memory between many running programs while still giving each program the illusion of having its own large address space."}, {"start": "00:00:23", "is_lecture": true, "end": "00:00:32", "is_worked_example": false, "text": "Both the virtual and physical address spaces are divided into a sequence of pages, each holding some fixed number of locations."}, {"start": "00:00:32", "is_lecture": true, "end": "00:00:43", "is_worked_example": false, "text": "For example if each page holds 2^12 bytes, a 32-bit address would have 2^32/2^12 = 2^20 pages."}, {"start": "00:00:43", "is_lecture": true, "end": "00:00:57", "is_worked_example": false, "text": "In this example the 32-bit address can be thought of as having two fields: a 20-bit page number formed from the high-order address bits and a 12-bit page offset formed from the low-order address bits."}, {"start": "00:00:57", "is_lecture": true, "end": "00:01:02", "is_worked_example": false, "text": "This arrangement ensures that nearby data will be located on the same page."}, {"start": "00:01:02", "is_lecture": true, "end": "00:01:10", "is_worked_example": false, "text": "The MMU translates virtual page numbers into physical page numbers using a page map."}, {"start": "00:01:10", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "Conceptually the page map is an array where each entry in the array contains a physical page number along with a couple of bits indicating the page status."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:30", "is_worked_example": false, "text": "The translation process is simple: the virtual page number is used as an index into the array to fetch the corresponding physical page number."}, {"start": "00:01:30", "is_lecture": true, "end": "00:01:37", "is_worked_example": false, "text": "The physical page number is then combined with the page offset to form the complete physical address."}, {"start": "00:01:37", "is_lecture": true, "end": "00:01:48", "is_worked_example": false, "text": "In the actual implementation the page map is usually organized into multiple levels, which permits us to have resident only the portion of the page map we're actively using."}, {"start": "00:01:48", "is_lecture": true, "end": "00:02:03", "is_worked_example": false, "text": "And to avoid the costs of accessing the page map on each address translation, we use a cache (called the translation look-aside buffer) to remember the results of recent vpn-to-ppn translations."}, {"start": "00:02:03", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "All allocated locations of each virtual address space can be found on secondary storage."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:14", "is_worked_example": false, "text": "Note that they may not necessarily be resident in main memory."}, {"start": "00:02:14", "is_lecture": true, "end": "00:02:27", "is_worked_example": false, "text": "If the CPU attempts to access a virtual address that's not resident in main memory, a page fault is signaled and the operating system will arrange to move the desired page from secondary storage into main memory."}, {"start": "00:02:27", "is_lecture": true, "end": "00:02:34", "is_worked_example": false, "text": "In practice, only the active pages for each program are resident in main memory at any given time."}, {"start": "00:02:34", "is_lecture": true, "end": "00:02:39", "is_worked_example": false, "text": "Here's a diagram showing the translation process."}, {"start": "00:02:39", "is_lecture": true, "end": "00:02:44", "is_worked_example": false, "text": "First we check to see if the required vpn-to-ppn mapping is cached in the TLB."}, {"start": "00:02:44", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "If not, we have to access the hierarchical page map to see if the page is resident and, if so, lookup its physical page number."}, {"start": "00:02:53", "is_lecture": true, "end": "00:03:03", "is_worked_example": false, "text": "If we discover that the page is not resident, a page fault exception is signaled to the CPU so that it can run a handler to load the page from secondary storage."}, {"start": "00:03:03", "is_lecture": true, "end": "00:03:09", "is_worked_example": false, "text": "Note that access to a particular mapping context is controlled by two registers."}, {"start": "00:03:09", "is_lecture": true, "end": "00:03:13", "is_worked_example": false, "text": "The context-number register controls which mappings are accessible in the TLB."}, {"start": "00:03:13", "is_lecture": true, "end": "00:03:21", "is_worked_example": false, "text": "And the page-directory register indicates which physical page holds the top tier of the hierarchical page map."}, {"start": "00:03:21", "is_lecture": true, "end": "00:03:26", "is_worked_example": false, "text": "We can switch to another context by simply reloading these two registers."}, {"start": "00:03:26", "is_lecture": true, "end": "00:03:37", "is_worked_example": false, "text": "To effectively accommodate multiple contexts we'll need to have sufficient TLB capacity to simultaneously cache the most frequent mappings for all the processes."}, {"start": "00:03:37", "is_lecture": true, "end": "00:03:43", "is_worked_example": false, "text": "And we'll need some number of physical pages to hold the required page directories and segments of the page tables."}, {"start": "00:03:43", "is_lecture": true, "end": "00:03:55", "is_worked_example": false, "text": "For example, for a particular process, three pages will suffice hold the resident two-level page map for 1024 pages at each end of the virtual address space,"}, {"start": "00:03:55", "is_lecture": true, "end": "00:04:03", "is_worked_example": false, "text": "providing access to up to 8MB of code, stack, and heap, more than enough for many simple programs."}, {"start": "00:04:03", "is_lecture": true, "end": "00:04:09", "is_worked_example": false, "text": "The page map creates the context needed to translate virtual addresses to physical addresses."}, {"start": "00:04:09", "is_lecture": true, "end": "00:04:19", "is_worked_example": false, "text": "In a computer system that's working on multiple tasks at the same time, we would like to support multiple contexts and to be able to quickly switch from one context to another."}, {"start": "00:04:19", "is_lecture": true, "end": "00:04:26", "is_worked_example": false, "text": "Multiple contexts would allow us to share physical memory between multiple programs."}, {"start": "00:04:26", "is_lecture": true, "end": "00:04:41", "is_worked_example": false, "text": "Each program would have an independent virtual address space, e.g., two programs could both access virtual address 0 as the address of their first instruction and would end up accessing different physical locations in main memory."}, {"start": "00:04:41", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "When switching between programs, we'd perform a \"context switch\" to move to the appropriate MMU context."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "The ability to share the CPU between many programs seems like a great idea!"}, {"start": "00:04:53", "is_lecture": true, "end": "00:04:57", "is_worked_example": false, "text": "Let's figure out the details of how that might work..."}]}, "C11S01B02-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c11/c11s1/2?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c11s1v2", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:04", "is_worked_example": false, "text": "So, how hard can it be to build a communication channel?"}, {"start": "00:00:04", "is_lecture": true, "end": "00:00:10", "is_worked_example": false, "text": "Aren't they just logic circuits with a long wire that runs from one component to another?"}, {"start": "00:00:10", "is_lecture": true, "end": "00:00:21", "is_worked_example": false, "text": "A circuit theorist would tell you that wires in a schematic diagram are intended to represent the equipotential nodes of the circuit, which are used to connect component terminals."}, {"start": "00:00:21", "is_lecture": true, "end": "00:00:34", "is_worked_example": false, "text": "In this simple model, a wire has the same voltage at all points and any changes in the voltage or current at one component terminal is instantly propagated to the other component terminals connected to the same wire."}, {"start": "00:00:34", "is_lecture": true, "end": "00:00:43", "is_worked_example": false, "text": "The notion of distance is abstracted out of our circuit models: terminals are either connected by a wire, or they're not."}, {"start": "00:00:43", "is_lecture": true, "end": "00:00:52", "is_worked_example": false, "text": "If there are resistances, capacitances, and inductances that need to be accounted for, the necessary components can be added to the circuit model."}, {"start": "00:00:52", "is_lecture": true, "end": "00:00:53", "is_worked_example": false, "text": "Wires are timeless."}, {"start": "00:00:53", "is_lecture": true, "end": "00:00:59", "is_worked_example": false, "text": "They are used to show how components connect, but they aren't themselves components."}, {"start": "00:00:59", "is_lecture": true, "end": "00:01:13", "is_worked_example": false, "text": "In fact, thinking of wires as equipotential nodes is a very workable model when the rate of change of the voltage on the wire is slow compared to the time it takes for an electromagnetic wave to propagate down the wire."}, {"start": "00:01:13", "is_lecture": true, "end": "00:01:26", "is_worked_example": false, "text": "Only as circuit speeds have increased with advances in integrated circuit technologies did this rule-of-thumb start to be violated in logic circuits where the components were separated by at most 10's of inches."}, {"start": "00:01:26", "is_lecture": true, "end": "00:01:36", "is_worked_example": false, "text": "In fact, it has been known since the late 1800s that changes in voltage levels take finite time to propagate down a wire."}, {"start": "00:01:36", "is_lecture": true, "end": "00:01:47", "is_worked_example": false, "text": "Oliver Heaviside was a self-taught English electrical engineer who, in the 1880's, developed a set of \"telegrapher's equations\" that described how signals propagate down wires."}, {"start": "00:01:47", "is_lecture": true, "end": "00:01:56", "is_worked_example": false, "text": "Using these, he was able to show how to improve the rate of transmission on then new transatlantic telegraph cable by a factor of 10."}, {"start": "00:01:56", "is_lecture": true, "end": "00:02:05", "is_worked_example": false, "text": "We now know that for high-speed signaling we have to treat wires as transmission lines, which we'll say more about in the next few slides."}, {"start": "00:02:05", "is_lecture": true, "end": "00:02:15", "is_worked_example": false, "text": "In this domain, the distance between components and hence the lengths of wires is of critical concern if we want to correctly predict the performance of our circuits."}, {"start": "00:02:15", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "Distance and signal propagation matter -- real-world wires are, in fact, fairly complex components!"}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:29", "is_worked_example": false, "text": "Here's an electrical model for an infinitesimally small segment of a real-world wire."}, {"start": "00:02:29", "is_lecture": true, "end": "00:02:36", "is_worked_example": false, "text": "An actual wire is correctly modeled by imagining a many copies of the model shown here connected end-to-end."}, {"start": "00:02:36", "is_lecture": true, "end": "00:02:44", "is_worked_example": false, "text": "The signal, i.e., the voltage on the wire, is measured with respect to the reference node which is also shown in the model."}, {"start": "00:02:44", "is_lecture": true, "end": "00:02:48", "is_worked_example": false, "text": "There are 4 parameters that characterize the behavior of the wire."}, {"start": "00:02:48", "is_lecture": true, "end": "00:02:51", "is_worked_example": false, "text": "R tells us the resistance of the conductor."}, {"start": "00:02:51", "is_lecture": true, "end": "00:02:58", "is_worked_example": false, "text": "It's usually negligible for the wiring on printed circuit boards, but it can be significant for long wires in integrated circuits."}, {"start": "00:02:58", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "L represents the self-inductance of the conductor, which characterizes how much energy will be absorbed by the wire's magnetic fields when the current flowing through the wire changes."}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:22", "is_worked_example": false, "text": "The conductor and reference node are separated by some sort insulator (which might be just air!) and hence form a capacitor with capacitance C."}, {"start": "00:03:22", "is_lecture": true, "end": "00:03:27", "is_worked_example": false, "text": "Finally, the conductance G represents the current that leaks through the insulator."}, {"start": "00:03:27", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "Usually this is quite small."}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:36", "is_worked_example": false, "text": "The table shows the parameter values we might measure for wires inside an integrated circuit and on printed circuit boards."}, {"start": "00:03:36", "is_lecture": true, "end": "00:03:52", "is_worked_example": false, "text": "If we analyze what happens when sending signals down the wire, we can describe the behavior of the wires using a single component called a transmission line, which has a characteristic complex-valued impedance Z_0."}, {"start": "00:03:52", "is_lecture": true, "end": "00:04:03", "is_worked_example": false, "text": "At high signaling frequencies and over the distances found on-chip or on circuit boards, such as one might find in a modern digital system, the transmission line is lossless,"}, {"start": "00:04:03", "is_lecture": true, "end": "00:04:10", "is_worked_example": false, "text": "and voltage changes (\"steps\") propagate down the wire at the rate of 1/sqrt(LC) meters per second."}, {"start": "00:04:10", "is_lecture": true, "end": "00:04:24", "is_worked_example": false, "text": "Using the values given here for a printed circuit board, the characteristic impedance is approximately 50 ohms and the speed of propagation is about 18 cm (7\") per ns."}, {"start": "00:04:24", "is_lecture": true, "end": "00:04:35", "is_worked_example": false, "text": "To send digital information from one component to another, we change the voltage on the connecting wire, and that voltage step propagates from the sender to the receiver."}, {"start": "00:04:35", "is_lecture": true, "end": "00:04:40", "is_worked_example": false, "text": "We have to pay some attention to what happens to that energy front when it gets to the end of the wire!"}, {"start": "00:04:40", "is_lecture": true, "end": "00:04:52", "is_worked_example": false, "text": "If we do nothing to absorb that energy, conservation laws tell us that it reflects off the end of the wire as an \"echo\" and soon our wire will be full of echoes from previous voltage steps!"}, {"start": "00:04:52", "is_lecture": true, "end": "00:05:02", "is_worked_example": false, "text": "To prevent these echoes we have to terminate the wire with a resistance to ground that matches the characteristic impedance of the transmission line."}, {"start": "00:05:02", "is_lecture": true, "end": "00:05:07", "is_worked_example": false, "text": "If the signal can propagate in both directions, we'll need to terminate at both ends."}, {"start": "00:05:07", "is_lecture": true, "end": "00:05:21", "is_worked_example": false, "text": "What this model is telling is the time it takes to transmit information from one component to another and that we have to be careful to absorb the energy associated with the transmission when the information has reached its destination."}, {"start": "00:05:21", "is_lecture": true, "end": "00:05:30", "is_worked_example": false, "text": "With that little bit of theory as background, we're in a position to describe the real-world consequences of poor engineering of our signal wires."}, {"start": "00:05:30", "is_lecture": true, "end": "00:05:40", "is_worked_example": false, "text": "The key observation is that unless we're careful there can still be energy left over from previous transmissions that might corrupt the current transmission."}, {"start": "00:05:40", "is_lecture": true, "end": "00:05:48", "is_worked_example": false, "text": "The general fix to this problem is time, i.e., giving the transmitted value a longer time to settle to an interference-free value."}, {"start": "00:05:48", "is_lecture": true, "end": "00:05:57", "is_worked_example": false, "text": "But slowing down isn't usually acceptable in high-performance systems, so we have to do our best to minimize these energy-storage effects."}, {"start": "00:05:57", "is_lecture": true, "end": "00:06:08", "is_worked_example": false, "text": "If the termination isn't exactly right, we'll get some reflections from any voltage step reaching the end of the wire, and it will take a while for these echoes to die out."}, {"start": "00:06:08", "is_lecture": true, "end": "00:06:17", "is_worked_example": false, "text": "In fact, as we'll see, energy will reflect off of any impedance discontinuity, which means we'll want to minimize the number of such discontinuities."}, {"start": "00:06:17", "is_lecture": true, "end": "00:06:23", "is_worked_example": false, "text": "We need to be careful to allow sufficient time for signals to reach valid logic levels."}, {"start": "00:06:23", "is_lecture": true, "end": "00:06:30", "is_worked_example": false, "text": "The shaded region shows a transition of the wire A from 1 to 0 to 1."}, {"start": "00:06:30", "is_lecture": true, "end": "00:06:42", "is_worked_example": false, "text": "The first inverter is trying to produce a 1-output from the initial input transition to 0, but doesn't have sufficient time to complete the transition on wire B before the input changes again."}, {"start": "00:06:42", "is_lecture": true, "end": "00:06:54", "is_worked_example": false, "text": "This leads to a runt pulse on wire C, the output of the second inverter, and we see that the sequence of bits on A has been corrupted by the time the signal reaches C."}, {"start": "00:06:54", "is_lecture": true, "end": "00:07:04", "is_worked_example": false, "text": "This problem was caused by the energy storage in the capacitance of the wire between the inverters, which will limit the speed at which we can run the logic."}, {"start": "00:07:04", "is_lecture": true, "end": "00:07:15", "is_worked_example": false, "text": "And here see we what happens when a large voltage step triggers oscillations in a wire, called ringing, due to the wire's inductance and capacitance."}, {"start": "00:07:15", "is_lecture": true, "end": "00:07:22", "is_worked_example": false, "text": "The graph shows it takes some time before the ringing dampens to the point that we have a reliable digital signal."}, {"start": "00:07:22", "is_lecture": true, "end": "00:07:27", "is_worked_example": false, "text": "The ringing can be diminished by spreading the voltage step over a longer time."}, {"start": "00:07:27", "is_lecture": true, "end": "00:07:41", "is_worked_example": false, "text": "The key idea here is that by paying close attention to the design of our wiring and the drivers that put information onto the wire, we can minimize the performance implications of these energy-storage effects."}, {"start": "00:07:41", "is_lecture": true, "end": "00:07:44", "is_worked_example": false, "text": "Okay, enough electrical engineering!"}, {"start": "00:07:44", "is_lecture": true, "end": "00:07:47", "is_worked_example": false, "text": "Suppose we have some information in our system."}, {"start": "00:07:47", "is_lecture": true, "end": "00:07:51", "is_worked_example": false, "text": "If we preserve that information over time, we call that storage."}, {"start": "00:07:51", "is_lecture": true, "end": "00:07:57", "is_worked_example": false, "text": "If we send that information to another component, we call that communication."}, {"start": "00:07:57", "is_lecture": true, "end": "00:08:05", "is_worked_example": false, "text": "In the real world, we've seen that communication takes time and we have to budget for that time in our system timing."}, {"start": "00:08:05", "is_lecture": true, "end": "00:08:18", "is_worked_example": false, "text": "Our engineering has to accommodate the fundamental bounds on propagating speeds, distances between components, and how fast we can change wire voltages without triggering the effects we saw on the previous slide."}, {"start": "00:08:18", "is_lecture": true, "end": "00:08:23", "is_worked_example": false, "text": "The upshot: our timing models will have to account for wire delays."}, {"start": "00:08:23", "is_lecture": true, "end": "00:08:35", "is_worked_example": false, "text": "In Part 1 of this course, we had a simple timing model that assigned a fixed propagation delay, t_PD, to the time it took for the output of a logic gate to reflect a change to the gate's input."}, {"start": "00:08:35", "is_lecture": true, "end": "00:08:43", "is_worked_example": false, "text": "We'll need to change our timing model to account for delay of transmitting the output of a logic gate to the next components."}, {"start": "00:08:43", "is_lecture": true, "end": "00:08:53", "is_worked_example": false, "text": "The timing will be load dependent, so signals that connect to the inputs of many other gates will be slower than signals that connect to only one other gate."}, {"start": "00:08:53", "is_lecture": true, "end": "00:09:02", "is_worked_example": false, "text": "The Jade simulator takes the loading of a gate's output signal into account when calculating the effective propagation delay of the gate."}, {"start": "00:09:02", "is_lecture": true, "end": "00:09:17", "is_worked_example": false, "text": "We can improve propagation delays by reducing the number of loads on output signals or by using specially-design gates called buffers (the component shown in red) to drive signals that have very large loads."}, {"start": "00:09:17", "is_lecture": true, "end": "00:09:28", "is_worked_example": false, "text": "A common task when optimizing the performance of a circuit is to track down heavily-loaded and hence slow wires and re-engineering the circuit to make them faster."}, {"start": "00:09:28", "is_lecture": true, "end": "00:09:24", "is_worked_example": false, "text": "Today our concern is wires used to connect components at the system level."}, {"start": "00:09:24", "is_lecture": true, "end": "00:09:42", "is_worked_example": false, "text": "So next we'll turn our attention to possible designs for system-level interconnect and the issues that might arise."}]}, "C06S01B03-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c6/c6s1/3?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c6s1v3", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:05", "is_worked_example": false, "text": "A key technology for timesharing is the periodic interrupt from the external timer device."}, {"start": "00:00:05", "is_lecture": true, "end": "00:00:09", "is_worked_example": false, "text": "Let's remind ourselves how the interrupt hardware in the Beta works."}, {"start": "00:00:09", "is_lecture": true, "end": "00:00:16", "is_worked_example": false, "text": "External devices request an interrupt by asserting the Beta's interrupt request (IRQ) input."}, {"start": "00:00:16", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "If the Beta is running in user mode, i.e., the supervisor bit stored in the PC is 0, asserting IRQ will trigger the following actions on the clock cycle the interrupt is recognized."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:40", "is_worked_example": false, "text": "The goal is to save the current PC+4 value in the XP register and force the program counter (PC) to a particular kernel-mode instruction, which starts the execution of the interrupt handler code."}, {"start": "00:00:40", "is_lecture": true, "end": "00:00:50", "is_worked_example": false, "text": "The normal process of generating control signals based on the current instruction is superseded by forcing particular values for some of the control signals."}, {"start": "00:00:50", "is_lecture": true, "end": "00:00:58", "is_worked_example": false, "text": "PCSEL is set to 4, which selects a specified kernel-mode address as the next value of the program counter."}, {"start": "00:00:58", "is_lecture": true, "end": "00:01:02", "is_worked_example": false, "text": "The address chosen depends on the type of external interrupt."}, {"start": "00:01:02", "is_lecture": true, "end": "00:01:08", "is_worked_example": false, "text": "In the case of the timer interrupt, the address is 0x80000008."}, {"start": "00:01:08", "is_lecture": true, "end": "00:01:18", "is_worked_example": false, "text": "Note that PC[31], the supervisor bit, is being set to 1 and the CPU will be in kernel-mode as it starts executing the code of the interrupt handler."}, {"start": "00:01:18", "is_lecture": true, "end": "00:01:30", "is_worked_example": false, "text": "The WASEL, WDSEL, and WERF control signals are set so that PC+4 is written into the XP register (i.e., R30) in the register file."}, {"start": "00:01:30", "is_lecture": true, "end": "00:01:39", "is_worked_example": false, "text": "And, finally, MWR is set to 0 to ensure that if we're interrupting a ST instruction that its execution is aborted correctly."}, {"start": "00:01:39", "is_lecture": true, "end": "00:01:45", "is_worked_example": false, "text": "So in the next clock cycle, execution starts with the first instruction of the kernel-mode interrupt handler,"}, {"start": "00:01:45", "is_lecture": true, "end": "00:01:52", "is_worked_example": false, "text": "which can find the PC+4 of the interrupted instruction in the XP register of the CPU."}, {"start": "00:01:52", "is_lecture": true, "end": "00:01:56", "is_worked_example": false, "text": "As we can see the interrupt hardware is pretty minimal:"}, {"start": "00:01:56", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "it saves the PC+4 of the interrupted user-mode program in the XP register and sets the program counter to some predetermined value that depends on which external interrupt happened."}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:13", "is_worked_example": false, "text": "The remainder of the work to handle the interrupt request is performed in software."}, {"start": "00:02:13", "is_lecture": true, "end": "00:02:24", "is_worked_example": false, "text": "The state of the interrupted process, e.g., the values in the CPU registers R0 through R30, is stored in main memory in an OS data structure called UserMState."}, {"start": "00:02:24", "is_lecture": true, "end": "00:02:31", "is_worked_example": false, "text": "Then the appropriate handler code, usually a procedure written in C, is invoked to do the heavy lifting."}, {"start": "00:02:31", "is_lecture": true, "end": "00:02:36", "is_worked_example": false, "text": "When that procedure returns, the process state is reloaded from UserMState."}, {"start": "00:02:36", "is_lecture": true, "end": "00:02:47", "is_worked_example": false, "text": "The OS subtracts 4 from the value in XP, making it point to the interrupted instruction and then resumes user-mode execution with a JMP(XP)."}, {"start": "00:02:47", "is_lecture": true, "end": "00:02:57", "is_worked_example": false, "text": "Note that in our simple Beta implementation the first instructions for the various interrupt handlers occupy consecutive locations in main memory."}, {"start": "00:02:57", "is_lecture": true, "end": "00:03:05", "is_worked_example": false, "text": "Since interrupt handlers are longer than one instruction, this first instruction is invariably a branch to the actual interrupt code."}, {"start": "00:03:05", "is_lecture": true, "end": "00:03:21", "is_worked_example": false, "text": "Here we see that the reset interrupt (asserted when the CPU first starts running) sets the PC to 0, the illegal instruction interrupt sets the PC to 4, the timer interrupt sets the PC to 8, and so on."}, {"start": "00:03:21", "is_lecture": true, "end": "00:03:33", "is_worked_example": false, "text": "In all cases, bit 31 of the new PC value is set to 1 so that handlers execute in supervisor or kernel mode, giving them access to the kernel context."}, {"start": "00:03:33", "is_lecture": true, "end": "00:03:45", "is_worked_example": false, "text": "A common alternative is provide a table of new PC values at a known location and have the interrupt hardware access that table to fetch the PC for the appropriate handler routine."}, {"start": "00:03:45", "is_lecture": true, "end": "00:03:50", "is_worked_example": false, "text": "This provides the same functionality as our simple Beta implementation."}, {"start": "00:03:50", "is_lecture": true, "end": "00:03:59", "is_worked_example": false, "text": "Since the process state is saved and restored during an interrupt, interrupts are transparent to the running user-mode program."}, {"start": "00:03:59", "is_lecture": true, "end": "00:04:07", "is_worked_example": false, "text": "In essence, we borrow a few CPU cycles to deal with the interrupt, then it's back to normal program execution."}, {"start": "00:04:07", "is_lecture": true, "end": "00:04:10", "is_worked_example": false, "text": "Here's how the timer interrupt handler would work."}, {"start": "00:04:10", "is_lecture": true, "end": "00:04:17", "is_worked_example": false, "text": "Our initial goal is to use the timer interrupt to update a data value in the OS that records the current time of day (TOD)."}, {"start": "00:04:17", "is_lecture": true, "end": "00:04:21", "is_worked_example": false, "text": "Let's assume the timer interrupt is triggered every 1/60th of a second."}, {"start": "00:04:21", "is_lecture": true, "end": "00:04:28", "is_worked_example": false, "text": "A user-mode program executes normally, not needing to make any special provision to deal with timer interrupts."}, {"start": "00:04:28", "is_lecture": true, "end": "00:04:38", "is_worked_example": false, "text": "Periodically the timer interrupts the user-mode program to run the clock interrupt handler code in the OS, then resumes execution of the user-mode program."}, {"start": "00:04:38", "is_lecture": true, "end": "00:04:43", "is_worked_example": false, "text": "The program continues execution just as if the interrupt had not occurred."}, {"start": "00:04:43", "is_lecture": true, "end": "00:04:50", "is_worked_example": false, "text": "If the program needs access to the TOD, it makes the appropriate service request to the OS."}, {"start": "00:04:50", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "The clock handler code in the OS starts and ends with a small amount of assembly-language code to save and restore the state."}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:04", "is_worked_example": false, "text": "In the middle, the assembly code makes a C procedure call to actually handle the interrupt."}, {"start": "00:05:04", "is_lecture": true, "end": "00:05:07", "is_worked_example": false, "text": "Here's what the handler code might look like."}, {"start": "00:05:07", "is_lecture": true, "end": "00:05:17", "is_worked_example": false, "text": "In C, we find the declarations for the TOD data value and the structure, called UserMState, that temporarily holds the saved process state."}, {"start": "00:05:17", "is_lecture": true, "end": "00:05:21", "is_worked_example": false, "text": "There's also the C procedure for incrementing the TOD value."}, {"start": "00:05:21", "is_lecture": true, "end": "00:05:31", "is_worked_example": false, "text": "A timer interrupt executes the BR() instruction at location 8, which branches to the actual interrupt handler code at CLOCK_H."}, {"start": "00:05:31", "is_lecture": true, "end": "00:05:37", "is_worked_example": false, "text": "The code first saves the values of all the CPU registers into the UserMState data structure."}, {"start": "00:05:37", "is_lecture": true, "end": "00:05:42", "is_worked_example": false, "text": "Note that we don't save the value of R31 since its value is always 0."}, {"start": "00:05:42", "is_lecture": true, "end": "00:05:49", "is_worked_example": false, "text": "After setting up the kernel-mode stack, the assembly-language stub calls the C procedure above to do the hard work."}, {"start": "00:05:49", "is_lecture": true, "end": "00:06:01", "is_worked_example": false, "text": "When the procedure returns, the CPU registers are reloaded from the saved process state and the XP register value decremented by 4 so that it will point to the interrupted instruction."}, {"start": "00:06:01", "is_lecture": true, "end": "00:06:05", "is_worked_example": false, "text": "Then a JMP(XP) resumes user-mode execution."}, {"start": "00:06:05", "is_lecture": true, "end": "00:06:08", "is_worked_example": false, "text": "Okay, that was simple enough."}, {"start": "00:06:08", "is_lecture": true, "end": "00:06:12", "is_worked_example": false, "text": "But what does this all have to do with timesharing?"}, {"start": "00:06:12", "is_lecture": true, "end": "00:06:16", "is_worked_example": false, "text": "Wasn't our goal to arrange to periodically switch which process was running?"}, {"start": "00:06:16", "is_lecture": true, "end": "00:06:18", "is_worked_example": false, "text": "Aha!"}, {"start": "00:06:18", "is_lecture": true, "end": "00:06:27", "is_worked_example": false, "text": "We have code that runs on every timer interrupt, so let's modify it so that every so often we arrange to call the OS' Scheduler() routine."}, {"start": "00:06:27", "is_lecture": true, "end": "00:06:35", "is_worked_example": false, "text": "In this example, we'd set the constant QUANTUM to 2 if we wanted to call Scheduler() every second timer interrupt."}, {"start": "00:06:35", "is_lecture": true, "end": "00:06:40", "is_worked_example": false, "text": "The Scheduler() subroutine is where the time sharing magic happens!"}, {"start": "00:06:40", "is_lecture": true, "end": "00:06:48", "is_worked_example": false, "text": "Here we see the UserMState data structure from the previous slide where the user-mode process state is stored during interrupts."}, {"start": "00:06:48", "is_lecture": true, "end": "00:06:55", "is_worked_example": false, "text": "And here's an array of process control block (PCB) data structures, one for each process in the system."}, {"start": "00:06:55", "is_lecture": true, "end": "00:07:05", "is_worked_example": false, "text": "The PCB holds the complete state of a process when some other process is currently executing -- it's the long-term storage for processor state!"}, {"start": "00:07:05", "is_lecture": true, "end": "00:07:17", "is_worked_example": false, "text": "As you can see, it includes a copy of MState with the process' register values, the MMU state, and various state associated with the process' input/output activities,"}, {"start": "00:07:17", "is_lecture": true, "end": "00:07:23", "is_worked_example": false, "text": "represented here by a number indicating which virtual user-interface console is attached to the process."}, {"start": "00:07:23", "is_lecture": true, "end": "00:07:27", "is_worked_example": false, "text": "There are N processes altogether."}, {"start": "00:07:27", "is_lecture": true, "end": "00:07:33", "is_worked_example": false, "text": "The variable CUR gives the index into ProcTable for the currently running process."}, {"start": "00:07:33", "is_lecture": true, "end": "00:07:39", "is_worked_example": false, "text": "And here's the surprisingly simple code for implementing timesharing."}, {"start": "00:07:39", "is_lecture": true, "end": "00:07:48", "is_worked_example": false, "text": "Whenever the Scheduler() routine is called, it starts by moving the temporary saved state into the PCB for the current process."}, {"start": "00:07:48", "is_lecture": true, "end": "00:07:58", "is_worked_example": false, "text": "It then increments CUR to move to the next process, making sure it wraps back around to 0 when we've just finished running the last of the N processes."}, {"start": "00:07:58", "is_lecture": true, "end": "00:08:07", "is_worked_example": false, "text": "It then loads reloads the temporary state from the PCB of the new process and sets up the MMU appropriately."}, {"start": "00:08:07", "is_lecture": true, "end": "00:08:17", "is_worked_example": false, "text": "At this point Scheduler() returns and the clock interrupt handler reloads the CPU registers from the updated temporary saved state and resumes execution."}, {"start": "00:08:17", "is_lecture": true, "end": "00:08:19", "is_worked_example": false, "text": "Voila!"}, {"start": "00:08:19", "is_lecture": true, "end": "00:08:21", "is_worked_example": false, "text": "We're now running a new process."}, {"start": "00:08:21", "is_lecture": true, "end": "00:08:26", "is_worked_example": false, "text": "Let's use this diagram to once again walk through how time sharing works."}, {"start": "00:08:26", "is_lecture": true, "end": "00:08:24", "is_worked_example": false, "text": "At the top of the diagram you'll see the code for the user-mode processes, and below the OS code along with its data structures."}, {"start": "00:08:24", "is_lecture": true, "end": "00:08:42", "is_worked_example": false, "text": "The timer interrupts the currently running user-mode program and starts execution of the OS' clock handler code."}, {"start": "00:08:42", "is_lecture": true, "end": "00:08:48", "is_worked_example": false, "text": "The first thing the handler does is save all the registers into the UserMState data structure."}, {"start": "00:08:48", "is_lecture": true, "end": "00:09:00", "is_worked_example": false, "text": "If the Scheduler() routine is called, it moves the temporarily saved state into the PCB, which provides the long-term storage for a process' state."}, {"start": "00:09:00", "is_lecture": true, "end": "00:09:06", "is_worked_example": false, "text": "Next, Scheduler() copies the saved state for the next process into the temporary holding area."}, {"start": "00:09:06", "is_lecture": true, "end": "00:09:15", "is_worked_example": false, "text": "Then the clock handler reloads the updated state into the CPU registers and resumes execution, this time running code in the new process."}, {"start": "00:09:15", "is_lecture": true, "end": "00:09:25", "is_worked_example": false, "text": "While we're looking at the OS, note that since its code runs with the supervisor mode bit set to 1, interrupts are disabled while in the OS."}, {"start": "00:09:25", "is_lecture": true, "end": "00:09:37", "is_worked_example": false, "text": "This prevents the awkward problem of getting a second interrupt while still in the middle of handling a first interrupt, a situation that might accidentally overwrite the state in UserMState."}, {"start": "00:09:37", "is_lecture": true, "end": "00:09:42", "is_worked_example": false, "text": "But that means one has to be very careful when writing OS code."}, {"start": "00:09:42", "is_lecture": true, "end": "00:09:46", "is_worked_example": false, "text": "Any sort of infinite loop can never be interrupted."}, {"start": "00:09:46", "is_lecture": true, "end": "00:09:53", "is_worked_example": false, "text": "You may have experienced this when your machine appears to freeze, accepting no inputs and just sitting there like a lump."}, {"start": "00:09:53", "is_lecture": true, "end": "00:10:00", "is_worked_example": false, "text": "At this point, your only choice is to power-cycle the hardware (the ultimate interrupt!) and start afresh."}, {"start": "00:10:00", "is_lecture": true, "end": "00:10:12", "is_worked_example": false, "text": "Interrupts are allowed during execution of user-mode programs, so if they run amok and need to be interrupted, that's always possible since the OS is still responding to, say, keyboard interrupts."}, {"start": "00:10:12", "is_lecture": true, "end": "00:10:22", "is_worked_example": false, "text": "Every OS has a magic combination of keystrokes that is guaranteed to suspend execution of the current process, sometimes arranging to make a copy of the process state for later debugging."}, {"start": "00:10:22", "is_lecture": true, "end": "00:10:24", "is_worked_example": false, "text": "Very handy!"}]}, "C12S01B05-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c12/c12s1/5?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c12s1v5", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:06", "is_worked_example": false, "text": "The problem with our simple multicore system is that there is no communication when the value of a shared variable is changed."}, {"start": "00:00:06", "is_lecture": true, "end": "00:00:13", "is_worked_example": false, "text": "The fix is to provide the necessary communications over a shared bus that's watched by all the caches."}, {"start": "00:00:13", "is_lecture": true, "end": "00:00:20", "is_worked_example": false, "text": "A cache can then \"snoop\" on what's happening in other caches and then update its local state to be consistent."}, {"start": "00:00:20", "is_lecture": true, "end": "00:00:25", "is_worked_example": false, "text": "The required communications protocol is called a \"cache coherence protocol\"."}, {"start": "00:00:25", "is_lecture": true, "end": "00:00:37", "is_worked_example": false, "text": "In designing the protocol, we'd like to incur the communications overhead only when there's actual sharing in progress, i.e., when multiple caches have local copies of a shared variable."}, {"start": "00:00:37", "is_lecture": true, "end": "00:00:42", "is_worked_example": false, "text": "To implement a cache coherence protocol, we'll change the state maintained for each cache line."}, {"start": "00:00:42", "is_lecture": true, "end": "00:00:50", "is_worked_example": false, "text": "The initial state for all cache lines is INVALID indicating that the tag and data fields do not contain up-to-date information."}, {"start": "00:00:50", "is_lecture": true, "end": "00:00:55", "is_worked_example": false, "text": "This corresponds to setting the valid bit to 0 in our original cache implementation."}, {"start": "00:00:55", "is_lecture": true, "end": "00:01:05", "is_worked_example": false, "text": "When the cache line state is EXCLUSIVE, this cache has the only copy of those memory locations and indicates that the local data is the same as that in main memory."}, {"start": "00:01:05", "is_lecture": true, "end": "00:01:10", "is_worked_example": false, "text": "This corresponds to setting the valid bit to 1 in our original cache implementation."}, {"start": "00:01:10", "is_lecture": true, "end": "00:01:24", "is_worked_example": false, "text": "If the cache line state is MODIFIED, that means the cache line data is the sole valid copy of the data. This corresponds to setting both the dirty and valid bits to 1 in our original cache implementation."}, {"start": "00:01:24", "is_lecture": true, "end": "00:01:33", "is_worked_example": false, "text": "To deal with sharing issues, there's a fourth state called SHARED that indicates when other caches may also have a copy of the same unmodified memory data."}, {"start": "00:01:33", "is_lecture": true, "end": "00:01:41", "is_worked_example": false, "text": "When filling a cache from main memory, other caches can snoop on the read request and participate if fulfilling the read request."}, {"start": "00:01:41", "is_lecture": true, "end": "00:01:51", "is_worked_example": false, "text": "If no other cache has the requested data, the data is fetched from main memory and the requesting cache sets the state of that cache line to EXCLUSIVE."}, {"start": "00:01:51", "is_lecture": true, "end": "00:02:05", "is_worked_example": false, "text": "If some other cache has the requested in line in the EXCLUSIVE or SHARED state, it supplies the data and asserts the SHARED signal on the snoopy bus to indicate that more than one cache now has a copy of the data."}, {"start": "00:02:05", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "All caches will mark the state of the cache line as SHARED."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:21", "is_worked_example": false, "text": "If another cache has a MODIFIED copy of the cache line, it supplies the changed data, providing the correct values for the requesting cache as well as updating the values in main memory."}, {"start": "00:02:21", "is_lecture": true, "end": "00:02:28", "is_worked_example": false, "text": "Again the SHARED signal is asserted and both the reading and responding cache will set the state for that cache line to SHARED."}, {"start": "00:02:28", "is_lecture": true, "end": "00:02:37", "is_worked_example": false, "text": "So, at the end of the read request, if there are multiple copies of the cache line, they will all be in the SHARED state."}, {"start": "00:02:37", "is_lecture": true, "end": "00:02:42", "is_worked_example": false, "text": "If there's only one copy of the cache line it will be in the EXCLUSIVE state."}, {"start": "00:02:42", "is_lecture": true, "end": "00:02:46", "is_worked_example": false, "text": "Writing to a cache line is when the sharing magic happens."}, {"start": "00:02:46", "is_lecture": true, "end": "00:02:51", "is_worked_example": false, "text": "If there's a cache miss, the first cache performs a cache line read as described above."}, {"start": "00:02:51", "is_lecture": true, "end": "00:02:58", "is_worked_example": false, "text": "If the cache line is now in the SHARED state, a write will cause the cache to send an INVALIDATE message on the snoopy bus,"}, {"start": "00:02:58", "is_lecture": true, "end": "00:03:07", "is_worked_example": false, "text": "telling all other caches to invalidate their copy of the cache line, guaranteeing the local cache now has EXCLUSIVE access to the cache line."}, {"start": "00:03:07", "is_lecture": true, "end": "00:03:13", "is_worked_example": false, "text": "If the cache line is in the EXCLUSIVE state when the write happens, no communication is necessary."}, {"start": "00:03:13", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "Now the cache data can be changed and the cache line state set to MODIFIED, completing the write."}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:25", "is_worked_example": false, "text": "This protocol is called \"MESI\" after the first initials of the possible states."}, {"start": "00:03:25", "is_lecture": true, "end": "00:03:33", "is_worked_example": false, "text": "Note that the the valid and dirty state bits in our original cache implementation have been repurposed to encode one of the four MESI states."}, {"start": "00:03:33", "is_lecture": true, "end": "00:03:45", "is_worked_example": false, "text": "The key to success is that each cache now knows when a cache line may be shared by another cache, prompting the necessary communication when the value of a shared location is changed."}, {"start": "00:03:45", "is_lecture": true, "end": "00:03:56", "is_worked_example": false, "text": "No attempt is made to update shared values, they're simply invalidated and the other caches will issue read requests if they need the value of the shared variable at some future time."}, {"start": "00:03:56", "is_lecture": true, "end": "00:04:06", "is_worked_example": false, "text": "To support cache coherence, the cache hardware has to be modified to support two request streams: one from the CPU and one from the snoopy bus."}, {"start": "00:04:06", "is_lecture": true, "end": "00:04:12", "is_worked_example": false, "text": "The CPU side includes a queue of store requests that were delayed by cache misses."}, {"start": "00:04:12", "is_lecture": true, "end": "00:04:18", "is_worked_example": false, "text": "This allows the CPU to proceed without having to wait for the cache refill operation to complete."}, {"start": "00:04:18", "is_lecture": true, "end": "00:04:27", "is_worked_example": false, "text": "Note that CPU read requests will need to check the store queue before they check the cache to ensure the most-recent value is supplied to the CPU."}, {"start": "00:04:27", "is_lecture": true, "end": "00:04:39", "is_worked_example": false, "text": "Usually there's a STORE_BARRIER instruction that stalls the CPU until the store queue is empty, guaranteeing that all processors have seen the effect of the writes before execution resumes."}, {"start": "00:04:39", "is_lecture": true, "end": "00:04:51", "is_worked_example": false, "text": "On the snoopy side, the cache has to snoop on the transactions from other caches, invalidating or supplying cache line data as appropriate, and then updating the local cache line state."}, {"start": "00:04:51", "is_lecture": true, "end": "00:04:57", "is_worked_example": false, "text": "If the cache is busy with, say, a refill operation, INVALIDATE requests may be queued until they can be processed."}, {"start": "00:04:57", "is_lecture": true, "end": "00:05:11", "is_worked_example": false, "text": "Usually there's a READ_BARRIER instruction that stalls the CPU until the invalidate queue is empty, guaranteeing that updates from other processors have been applied to the local cache state before execution resumes."}, {"start": "00:05:11", "is_lecture": true, "end": "00:05:19", "is_worked_example": false, "text": "Note that the \"read with intent to modify\" transaction shown here is just protocol shorthand for a READ immediately followed by an INVALIDATE,"}, {"start": "00:05:19", "is_lecture": true, "end": "00:05:23", "is_worked_example": false, "text": "indicating that the requester will be changing the contents of the cache line."}, {"start": "00:05:23", "is_lecture": true, "end": "00:05:28", "is_worked_example": false, "text": "How do the CPU and snoopy cache requests affect the cache state?"}, {"start": "00:05:28", "is_lecture": true, "end": "00:05:32", "is_worked_example": false, "text": "Here in micro type is a flow chart showing what happens when."}, {"start": "00:05:32", "is_lecture": true, "end": "00:05:37", "is_worked_example": false, "text": "If you're interested, try following the actions required to complete various transactions."}, {"start": "00:05:37", "is_lecture": true, "end": "00:05:47", "is_worked_example": false, "text": "Intel, in its wisdom, adds a fifth \"F\" state, used to determine which cache will respond to read requests when the requested cache line is shared by multiple caches --"}, {"start": "00:05:47", "is_lecture": true, "end": "00:05:52", "is_worked_example": false, "text": "basically it selects which of the SHARED cache lines gets to be the responder."}, {"start": "00:05:52", "is_lecture": true, "end": "00:05:54", "is_worked_example": false, "text": "But this is a bit abstract."}, {"start": "00:05:54", "is_lecture": true, "end": "00:05:59", "is_worked_example": false, "text": "Let's try the MESI cache coherence protocol on our earlier example!"}, {"start": "00:05:59", "is_lecture": true, "end": "00:06:07", "is_worked_example": false, "text": "Here are our two threads and their local cache states indicating that values of locations X and Y are shared by both caches."}, {"start": "00:06:07", "is_lecture": true, "end": "00:06:13", "is_worked_example": false, "text": "Let's see what happens when the operations happen in the order (1 through 4) shown here."}, {"start": "00:06:13", "is_lecture": true, "end": "00:06:18", "is_worked_example": false, "text": "You can check what happens when the transactions are in a different order or happen concurrently."}, {"start": "00:06:18", "is_lecture": true, "end": "00:06:22", "is_worked_example": false, "text": "First, Thread A changes X to 3."}, {"start": "00:06:22", "is_lecture": true, "end": "00:06:31", "is_worked_example": false, "text": "Since this location is marked as SHARED [S] in the local cache, the cache for core 0 ($_0) issues an INVALIDATE transaction for location X to the other caches,"}, {"start": "00:06:31", "is_lecture": true, "end": "00:06:36", "is_worked_example": false, "text": "giving it exclusive access to location X, which it changes to have the value 3."}, {"start": "00:06:36", "is_lecture": true, "end": "00:06:43", "is_worked_example": false, "text": "At the end of this step, the cache for core 1 ($_1) no longer has a copy of the value for location X."}, {"start": "00:06:43", "is_lecture": true, "end": "00:06:48", "is_worked_example": false, "text": "In step 2, Thread B changes Y to 4."}, {"start": "00:06:48", "is_lecture": true, "end": "00:06:55", "is_worked_example": false, "text": "Since this location is marked as SHARED in the local cache, cache 1 issues an INVALIDATE transaction for location Y to the other caches,"}, {"start": "00:06:55", "is_lecture": true, "end": "00:07:01", "is_worked_example": false, "text": "giving it exclusive access to location Y, which it changes to have the value 4."}, {"start": "00:07:01", "is_lecture": true, "end": "00:07:08", "is_worked_example": false, "text": "In step 3, execution continues in Thread B, which needs the value of location X."}, {"start": "00:07:08", "is_lecture": true, "end": "00:07:20", "is_worked_example": false, "text": "That's a cache miss, so it issues a read request on the snoopy bus, and cache 0 responds with its updated value, and both caches mark the location X as SHARED."}, {"start": "00:07:20", "is_lecture": true, "end": "00:07:26", "is_worked_example": false, "text": "Main memory, which is also watching the snoopy bus, also updates its copy of the X value."}, {"start": "00:07:26", "is_lecture": true, "end": "00:07:33", "is_worked_example": false, "text": "Finally, in step 4, Thread A needs the value for Y, which results in a similar transaction on the snoopy bus."}, {"start": "00:07:33", "is_lecture": true, "end": "00:07:40", "is_worked_example": false, "text": "Note the outcome corresponds exactly to that produced by the same execution sequence on a timeshared core"}, {"start": "00:07:40", "is_lecture": true, "end": "00:07:47", "is_worked_example": false, "text": "since the coherence protocol guarantees that no cache has an out-of-date copy of a shared memory location."}, {"start": "00:07:47", "is_lecture": true, "end": "00:07:52", "is_worked_example": false, "text": "And both caches agree on the ending values for the shared variables X and Y."}, {"start": "00:07:52", "is_lecture": true, "end": "00:08:00", "is_worked_example": false, "text": "If you try other execution orders, you'll see that sequential consistency and shared memory semantics are maintained in each case."}, {"start": "00:08:00", "is_lecture": true, "end": "00:08:03", "is_worked_example": false, "text": "The cache coherency protocol has done it's job!"}, {"start": "00:08:03", "is_lecture": true, "end": "00:08:07", "is_worked_example": false, "text": "Let's summarize our discussion of parallel processing."}, {"start": "00:08:07", "is_lecture": true, "end": "00:08:12", "is_worked_example": false, "text": "At the moment, it seems that the architecture of a single core has reached a stable point."}, {"start": "00:08:12", "is_lecture": true, "end": "00:08:23", "is_worked_example": false, "text": "At least with the current ISAs, pipeline depths are unlikely to increase and out-of-order, superscalar instruction execution has reached the point of diminishing performance returns."}, {"start": "00:08:23", "is_lecture": true, "end": "00:06:30", "is_worked_example": false, "text": "So it seems unlikely there will be dramatic performance improvements due to architectural changes inside the CPU core."}, {"start": "00:06:30", "is_lecture": true, "end": "00:08:40", "is_worked_example": false, "text": "GPU architectures continue to evolve as they adapt to new uses in specific application areas, but they are unlikely to impact general-purpose computing."}, {"start": "00:08:40", "is_lecture": true, "end": "00:08:49", "is_worked_example": false, "text": "At the system level, the trend is toward increasing the number of cores and figuring out how to best exploit parallelism with new algorithms."}, {"start": "00:08:49", "is_lecture": true, "end": "00:08:57", "is_worked_example": false, "text": "Looking further ahead, notice that the brain is able to accomplish remarkable results using fairly slow mechanisms"}, {"start": "00:08:57", "is_lecture": true, "end": "00:09:05", "is_worked_example": false, "text": "It takes ~.01 seconds to get a message to the brain and synapses fire somewhere between 0.3 to 1.8 times per second."}, {"start": "00:09:05", "is_lecture": true, "end": "00:09:10", "is_worked_example": false, "text": "Is it massive parallelism that gives the brain its \"computational\" power?"}, {"start": "00:09:10", "is_lecture": true, "end": "00:09:18", "is_worked_example": false, "text": "Or is it that the brain uses a different computation model, e.g., neural nets, to decide upon new actions given new inputs?"}, {"start": "00:09:18", "is_lecture": true, "end": "00:09:25", "is_worked_example": false, "text": "At least for applications involving cognition there are new architectural and technology frontiers to explore."}, {"start": "00:09:25", "is_lecture": true, "end": "00:09:30", "is_worked_example": false, "text": "You have some interesting challenges ahead if you get interested in the future of parallel processing!"}]}, "C08S01B09-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c8/c8s1/9?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c8s1v9", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:07", "is_worked_example": false, "text": "In a weak priority system the currently-running task will always run to completion before considering what to run next."}, {"start": "00:00:07", "is_lecture": true, "end": "00:00:14", "is_worked_example": false, "text": "This means the worst-case latency for a device always includes the worst-case service time across all the other devices,"}, {"start": "00:00:14", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "i.e., the maximum time we have to wait for the currently-running task to complete."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:26", "is_worked_example": false, "text": "If there's a long-running task that usually means it will be impossible to meet tight deadlines for other tasks."}, {"start": "00:00:26", "is_lecture": true, "end": "00:00:34", "is_worked_example": false, "text": "For example, suppose disk requests have a 800 us deadline in order to guarantee the best throughput from the disk subsystem."}, {"start": "00:00:34", "is_lecture": true, "end": "00:00:46", "is_worked_example": false, "text": "Since the disk handler service time is 500 us, the maximum allowable latency between a disk request and starting to execute the disk service routine is 300 us."}, {"start": "00:00:46", "is_lecture": true, "end": "00:00:48", "is_worked_example": false, "text": "Oops!"}, {"start": "00:00:48", "is_lecture": true, "end": "00:00:55", "is_worked_example": false, "text": "The weak priority scheme can only guarantee a maximum latency of 800 us, not nearly fast enough to meet the disk deadline."}, {"start": "00:00:55", "is_lecture": true, "end": "00:00:59", "is_worked_example": false, "text": "We can't meet the disk deadline using weak priorities."}, {"start": "00:00:59", "is_lecture": true, "end": "00:01:08", "is_worked_example": false, "text": "We need to introduce a preemptive priority system that allows lower-priority handlers to be interrupted by higher-priority requests."}, {"start": "00:01:08", "is_lecture": true, "end": "00:01:11", "is_worked_example": false, "text": "We'll refer to this as a \"strong\" priority system."}, {"start": "00:01:11", "is_lecture": true, "end": "00:01:19", "is_worked_example": false, "text": "Suppose we gave the disk the highest priority, the printer second priority, and keyboard the lowest priority, just like we did before."}, {"start": "00:01:19", "is_lecture": true, "end": "00:01:29", "is_worked_example": false, "text": "Now when a disk request arrives, it will start executing immediately without having to wait for the completion of the lower-priority printer or keyboard handlers."}, {"start": "00:01:29", "is_lecture": true, "end": "00:01:33", "is_worked_example": false, "text": "The worst-case latency for the disk has dropped to 0."}, {"start": "00:01:33", "is_lecture": true, "end": "00:01:39", "is_worked_example": false, "text": "The printer can only be preempted by the disk, so it's worst-case latency is 500 us."}, {"start": "00:01:39", "is_lecture": true, "end": "00:01:49", "is_worked_example": false, "text": "Since it has the lowest priority, the worst-case latency for the keyboard is unchanged at 900 us since it might still have to wait on the disk and printer."}, {"start": "00:01:49", "is_lecture": true, "end": "00:01:59", "is_worked_example": false, "text": "The good news: with the proper assignment of priorities, the strong priority system can guarantee that disk requests will be serviced by the 800 us deadline."}, {"start": "00:01:59", "is_lecture": true, "end": "00:02:05", "is_worked_example": false, "text": "We'll need to make a small tweak to our Beta hardware to implement a strong priority system."}, {"start": "00:02:05", "is_lecture": true, "end": "00:02:20", "is_worked_example": false, "text": "We'll replace the single supervisor mode bit in PC[31] with, say, a three-bit field (PRI) in PC[31:29] that indicates which of the eight priority levels the processor is currently running at."}, {"start": "00:02:20", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "Next, we'll modify the interrupt mechanism as follows."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:32", "is_worked_example": false, "text": "In addition to requesting an interrupt, the requesting device also specifies the 3-bit priority it was assigned by the system architect."}, {"start": "00:02:32", "is_lecture": true, "end": "00:02:44", "is_worked_example": false, "text": "We'll add a priority encoder circuit to the interrupt hardware to select the highest-priority request and compare the priority of that request (PDEV) to the 3-bit PRI value in the PC."}, {"start": "00:02:44", "is_lecture": true, "end": "00:02:55", "is_worked_example": false, "text": "The system will take the interrupt request only if PDEV > PRI, i.e., if the priority of the request is *higher* than the priority the system is running at."}, {"start": "00:02:55", "is_lecture": true, "end": "00:03:08", "is_worked_example": false, "text": "When the interrupt is taken, the old PC and PRI information is saved in XP, and the new PC is determined by the type of interrupt and the new PRI field is set to PDEV."}, {"start": "00:03:08", "is_lecture": true, "end": "00:03:12", "is_worked_example": false, "text": "So the processor will now be running at the higher priority specified by the device."}, {"start": "00:03:12", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "A strong priority system allows low-priority handlers to be interrupted by higher-priority requests,"}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:28", "is_worked_example": false, "text": "so the worst-case latency seen at high priorities is unaffected by the service times of lower-priority handlers."}, {"start": "00:03:28", "is_lecture": true, "end": "00:03:36", "is_worked_example": false, "text": "Using strong priorities allows us to assign a high priority to devices with tight deadlines and thus guarantee their deadlines are met."}, {"start": "00:03:36", "is_lecture": true, "end": "00:03:43", "is_worked_example": false, "text": "Now let's consider the impact of recurring interrupts, i.e., multiple interrupt requests from each device."}, {"start": "00:03:43", "is_lecture": true, "end": "00:03:52", "is_worked_example": false, "text": "We've added a \"maximum frequency\" column to our table, which gives the maximum rate at which requests will be generated by each device."}, {"start": "00:03:52", "is_lecture": true, "end": "00:03:57", "is_worked_example": false, "text": "The execution diagram for a strong priority system is shown below the table."}, {"start": "00:03:57", "is_lecture": true, "end": "00:04:05", "is_worked_example": false, "text": "Here we see there are multiple requests from each device, in this case shown at their maximum possible rate of request."}, {"start": "00:04:05", "is_lecture": true, "end": "00:04:10", "is_worked_example": false, "text": "Each tick on the timeline represent 100 us of real time."}, {"start": "00:04:10", "is_lecture": true, "end": "00:04:22", "is_worked_example": false, "text": "Printer requests occur every 1 ms (10 ticks), disk requests every 2 ms (20 ticks), and keyboard requests every 10 ms (100 ticks)."}, {"start": "00:04:22", "is_lecture": true, "end": "00:04:29", "is_worked_example": false, "text": "In the diagram you can see that the high-priority disk requests are serviced as soon as they're received."}, {"start": "00:04:29", "is_lecture": true, "end": "00:04:35", "is_worked_example": false, "text": "And that medium-priority printer requests preempt lower-priority execution of the keyboard handler."}, {"start": "00:04:35", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "Printer requests would be preempted by disk requests, but given their request patterns, there's never a printer request in progress when a disk request arrives, so we don't see that happening here."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "The maximum latency before a keyboard requests starts is indeed 900 us."}, {"start": "00:04:53", "is_lecture": true, "end": "00:04:56", "is_worked_example": false, "text": "But that doesn't tell the whole story!"}, {"start": "00:04:56", "is_lecture": true, "end": "00:05:09", "is_worked_example": false, "text": "As you can see, the poor keyboard handler is continually preempted by higher-priority disk and printer requests and so the keyboard handler doesn't complete until 3 ms after its request was received!"}, {"start": "00:05:09", "is_lecture": true, "end": "00:05:15", "is_worked_example": false, "text": "This illustrates why real-time constraints are best expressed in terms of deadlines and not latencies."}, {"start": "00:05:15", "is_lecture": true, "end": "00:05:24", "is_worked_example": false, "text": "If the keyboard deadline had been less that 3 ms, even the strong priority system would have failed to meet the hard real-time constraints."}, {"start": "00:05:24", "is_lecture": true, "end": "00:05:32", "is_worked_example": false, "text": "The reason would be that there simply aren't enough CPU cycles to meet the recurring demands of the devices in the face of tight deadlines."}, {"start": "00:05:32", "is_lecture": true, "end": "00:05:40", "is_worked_example": false, "text": "Speaking of having enough CPU cycles, there are several calculations we need to do when thinking about recurring interrupts."}, {"start": "00:05:40", "is_lecture": true, "end": "00:05:45", "is_worked_example": false, "text": "The first is to consider how much load each periodic request places on the system."}, {"start": "00:05:45", "is_lecture": true, "end": "00:05:57", "is_worked_example": false, "text": "There's one keyboard request every 10 ms and servicing each request takes 800 us, which consumes 800us/10ms = 8% of the CPU."}, {"start": "00:05:57", "is_lecture": true, "end": "00:06:05", "is_worked_example": false, "text": "A similar calculation shows that servicing the disk takes 25% of the CPU and servicing the printer takes 40% of the CPU."}, {"start": "00:06:05", "is_lecture": true, "end": "00:06:15", "is_worked_example": false, "text": "Collectively servicing all the devices takes 73% of the CPU cycles, leaving 27% for running user-mode programs."}, {"start": "00:06:15", "is_lecture": true, "end": "00:06:21", "is_worked_example": false, "text": "Obviously we'd be in trouble if takes more than 100% of the available cycles to service the devices."}, {"start": "00:06:21", "is_lecture": true, "end": "00:06:28", "is_worked_example": false, "text": "Another way to get in trouble is to not have enough CPU cycles to meet each of the deadlines."}, {"start": "00:06:28", "is_lecture": true, "end": "00:06:36", "is_worked_example": false, "text": "We need 500/800 = 67.5% of the cycles to service the disk in the time between the disk request and disk deadline."}, {"start": "00:06:36", "is_lecture": true, "end": "00:06:45", "is_worked_example": false, "text": "If we assume we want to finish serving one printer request before receiving the next, the effective printer deadline is 1000 us."}, {"start": "00:06:45", "is_lecture": true, "end": "00:06:55", "is_worked_example": false, "text": "In 1000 us we need to be able to service one higher-priority disk request (500 us) and, obviously, the printer request (400 us)."}, {"start": "00:06:55", "is_lecture": true, "end": "00:07:01", "is_worked_example": false, "text": "So we'll need to use 900 us of the CPU in that 1000 us interval."}, {"start": "00:07:01", "is_lecture": true, "end": "00:07:04", "is_worked_example": false, "text": "Whew, just barely made it!"}, {"start": "00:07:04", "is_lecture": true, "end": "00:07:08", "is_worked_example": false, "text": "Suppose we tried setting the keyboard deadline to 2000 us."}, {"start": "00:07:08", "is_lecture": true, "end": "00:07:14", "is_worked_example": false, "text": "In that time interval we'd also need to service 1 disk request and 2 printer requests."}, {"start": "00:07:14", "is_lecture": true, "end": "00:07:22", "is_worked_example": false, "text": "So the total service time needed is 500 + 2*400 + 800 = 2100 us."}, {"start": "00:07:22", "is_lecture": true, "end": "00:07:32", "is_worked_example": false, "text": "Oops, that exceeds the 2000 us window we were given, so we can't meet the 2000 us deadline with the available CPU resources."}, {"start": "00:07:32", "is_lecture": true, "end": "00:07:38", "is_worked_example": false, "text": "But if the keyboard deadline is 3000 us, let's see what happens."}, {"start": "00:07:38", "is_lecture": true, "end": "00:07:53", "is_worked_example": false, "text": "In a 3000 us interval we need to service 2 disk requests, 3 printer requests, and, of course, 1 keyboard request, for a total service time of 2*500 + 3*400 + 800 = 3000 us."}, {"start": "00:07:53", "is_lecture": true, "end": "00:07:56", "is_worked_example": false, "text": "Whew! Just made it!"}]}, "C08S01B07-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c8/c8s1/7?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c8s1v7", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:03", "is_worked_example": false, "text": "Suppose we have a real-time system supporting three devices:"}, {"start": "00:00:03", "is_lecture": true, "end": "00:00:07", "is_worked_example": false, "text": "a keyboard whose interrupt handler has a service time of 800 us,"}, {"start": "00:00:07", "is_lecture": true, "end": "00:00:10", "is_worked_example": false, "text": "a disk with a service time of 500 us,"}, {"start": "00:00:10", "is_lecture": true, "end": "00:00:14", "is_worked_example": false, "text": "and a printer with a service time of 400 us."}, {"start": "00:00:14", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "What is the worst-case latency seen by each device?"}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:25", "is_worked_example": false, "text": "For now we'll assume that requests are infrequent, i.e., that each request only happens once in each scenario."}, {"start": "00:00:25", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "Requests can arrive at any time and in any order."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:36", "is_worked_example": false, "text": "If we serve the requests in first-come-first-served order, each device might be delayed by the service of all other devices."}, {"start": "00:00:36", "is_lecture": true, "end": "00:00:45", "is_worked_example": false, "text": "So the start of the keyboard handler might be delayed by the execution of the disk and printer handlers, a worst-case latency of 900 us."}, {"start": "00:00:45", "is_lecture": true, "end": "00:00:53", "is_worked_example": false, "text": "The start of the disk handler might be delayed by the keyboard and printer handlers, a worst-case latency of 1200 us."}, {"start": "00:00:53", "is_lecture": true, "end": "00:01:00", "is_worked_example": false, "text": "And the printer handler might be delayed by the keyboard and disk handlers, a worst-case latency of 1300 us."}, {"start": "00:01:00", "is_lecture": true, "end": "00:01:08", "is_worked_example": false, "text": "In this scenario we see that long-running handlers have a huge impact on the worst-case latency seen by the other devices."}, {"start": "00:01:08", "is_lecture": true, "end": "00:01:13", "is_worked_example": false, "text": "What are the possibilities for reducing the worst-case latencies?"}, {"start": "00:01:13", "is_lecture": true, "end": "00:01:18", "is_worked_example": false, "text": "Is there a better scheduling algorithm than first-come-first-served?"}, {"start": "00:01:18", "is_lecture": true, "end": "00:01:25", "is_worked_example": false, "text": "One strategy is to assign priorities to the pending requests and to serve the requests in priority order."}, {"start": "00:01:25", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "If the handlers are uninterruptible, the priorities will be used to select the *next* task to be run at the completion of the current task."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:40", "is_worked_example": false, "text": "Note that under this strategy, the current task always runs to completion even if a higher-priority request arrives while it's executing."}, {"start": "00:01:40", "is_lecture": true, "end": "00:01:44", "is_worked_example": false, "text": "This is called a \"nonpreemptive\" or \"weak\" priority system."}, {"start": "00:01:44", "is_lecture": true, "end": "00:01:53", "is_worked_example": false, "text": "Using a weak priority system, the worst-case latency seen by each device is the worst-case service time of all the other devices"}, {"start": "00:01:53", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "(since that handler may have just started running when the new request arrives),"}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:02", "is_worked_example": false, "text": "plus the service time of all higher-priority devices (since they'll be run first)."}, {"start": "00:02:02", "is_lecture": true, "end": "00:02:10", "is_worked_example": false, "text": "In our example, suppose we assigned the highest priority to the disk, the next priority to the printer, and the lowest priority to the keyboard."}, {"start": "00:02:10", "is_lecture": true, "end": "00:02:19", "is_worked_example": false, "text": "The worst-case latency of the keyboard is unchanged since it has the lowest priority and hence can be delayed by the higher-priority disk and printer handlers."}, {"start": "00:02:19", "is_lecture": true, "end": "00:02:27", "is_worked_example": false, "text": "The disk handler has the highest priority and so will always be selected for execution after the current handler completes."}, {"start": "00:02:27", "is_lecture": true, "end": "00:02:34", "is_worked_example": false, "text": "So its worst-case latency is the worst-case service time for the currently-running handler, which in this case is the keyboard."}, {"start": "00:02:34", "is_lecture": true, "end": "00:02:38", "is_worked_example": false, "text": "So the worst-case latency for the disk is 800 us."}, {"start": "00:02:38", "is_lecture": true, "end": "00:02:43", "is_worked_example": false, "text": "This is a considerable improvement over the first-come-first-served scenario."}, {"start": "00:02:43", "is_lecture": true, "end": "00:02:53", "is_worked_example": false, "text": "Finally the worst-case scenario for the printer is 1300 us since it may have to wait for the keyboard handler to finish (which can take up to 800 us)"}, {"start": "00:02:53", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "and then for a higher-priority disk request to be serviced (which takes 500 us)."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:04", "is_worked_example": false, "text": "How should priorities be assigned given hard real-time constraints?"}, {"start": "00:03:04", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "We'll assume each device has a service deadline D after the arrival of its service request."}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:17", "is_worked_example": false, "text": "If not otherwise specified, assume D is the time until the *next* request for the same device."}, {"start": "00:03:17", "is_lecture": true, "end": "00:03:23", "is_worked_example": false, "text": "This is a reasonably conservative assumption that prevents the system from falling further and further behind."}, {"start": "00:03:23", "is_lecture": true, "end": "00:03:31", "is_worked_example": false, "text": "For example, it makes sense that the keyboard handler should finish processing one character before the next arrives."}, {"start": "00:03:31", "is_lecture": true, "end": "00:03:40", "is_worked_example": false, "text": "\"Earliest Deadline\" is a strategy for assigning priorities that is guaranteed to meet the deadlines if any priority assignment can meet the deadlines."}, {"start": "00:03:40", "is_lecture": true, "end": "00:03:44", "is_worked_example": false, "text": "It's very simple: Sort the requests by their deadlines."}, {"start": "00:03:44", "is_lecture": true, "end": "00:03:50", "is_worked_example": false, "text": "Assign the highest priority to the earliest deadline, second priority to the next deadline, and so on."}, {"start": "00:03:50", "is_lecture": true, "end": "00:03:48", "is_worked_example": false, "text": "A weak priority system will choose the pending request with the highest priority, i.e., the request that has the earliest deadline."}, {"start": "00:03:48", "is_lecture": true, "end": "00:04:02", "is_worked_example": false, "text": "Earliest Deadline has an intuitive appeal."}, {"start": "00:04:02", "is_lecture": true, "end": "00:04:05", "is_worked_example": false, "text": "Imagine standing in a long security line at the airport."}, {"start": "00:04:05", "is_lecture": true, "end": "00:04:15", "is_worked_example": false, "text": "It would make sense to prioritize the processing of passengers who have the earliest flights assuming that there's enough time to process everyone before their flight leaves."}, {"start": "00:04:15", "is_lecture": true, "end": "00:04:24", "is_worked_example": false, "text": "Processing 10 people whose flights leave in 30 minutes before someone whose flight leaves in 5 min will cause that last person to miss their flight."}, {"start": "00:04:24", "is_lecture": true, "end": "00:04:31", "is_worked_example": false, "text": "But if that person is processed first, the other passengers may be slightly delayed but everyone will make their flight."}, {"start": "00:04:31", "is_lecture": true, "end": "00:04:37", "is_worked_example": false, "text": "This is the sort of scheduling problem that Earliest Deadline and a weak priority system can solve."}, {"start": "00:04:37", "is_lecture": true, "end": "00:04:45", "is_worked_example": false, "text": "It's outside the scope of our discussion, but it's interesting to think about what should happen if some flights are going to be missed."}, {"start": "00:04:45", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "If the system is overloaded, prioritizing by earliest deadline may mean that everyone will miss their flights!"}, {"start": "00:04:53", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "In this scenario it might be better to assign priorities to the minimize the total number of missed flights."}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:08", "is_worked_example": false, "text": "This gets complicated in a hurry since the assignment of priorities now depends on exactly what requests are pending and how long it will take them to be serviced."}, {"start": "00:05:08", "is_lecture": true, "end": "00:05:10", "is_worked_example": false, "text": "An intriguing problem to think about!"}]}, "C09S01B01-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c9/c9s1/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c9s1v1", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:06", "is_worked_example": false, "text": "It's not unusual to find that an application is organized as multiple communicating processes."}, {"start": "00:00:06", "is_lecture": true, "end": "00:00:10", "is_worked_example": false, "text": "What's the advantage of using multiple processes instead of just a single process?"}, {"start": "00:00:10", "is_lecture": true, "end": "00:00:18", "is_worked_example": false, "text": "Many applications exhibit concurrency, i.e., some of the required computations can be performed in parallel."}, {"start": "00:00:18", "is_lecture": true, "end": "00:00:26", "is_worked_example": false, "text": "For example, video compression algorithms represent each video frame as an array of 8-pixel by 8-pixel macroblocks."}, {"start": "00:00:26", "is_lecture": true, "end": "00:00:39", "is_worked_example": false, "text": "Each macroblock is individually compressed by converting the 64 intensity and color values from the spatial domain to the frequency domain and then quantizing and Huffman encoding the frequency coefficients."}, {"start": "00:00:39", "is_lecture": true, "end": "00:00:46", "is_worked_example": false, "text": "If you're using a multi-core processor to do the compression, you can perform the macroblock compressions concurrently."}, {"start": "00:00:46", "is_lecture": true, "end": "00:00:56", "is_worked_example": false, "text": "Applications like video games are naturally divided into the \"front-end\" user interface and \"back-end\" simulation and rendering engines."}, {"start": "00:00:56", "is_lecture": true, "end": "00:01:07", "is_worked_example": false, "text": "Inputs from the user arrive asynchronously with respect to the simulation and it's easiest to organize the processing of user events separately from the backend processing."}, {"start": "00:01:07", "is_lecture": true, "end": "00:01:17", "is_worked_example": false, "text": "Processes are an effective way to encapsulate the state and computation for what are logically independent components of an application,"}, {"start": "00:01:17", "is_lecture": true, "end": "00:01:21", "is_worked_example": false, "text": "which communicate with one another when they need to share information."}, {"start": "00:01:21", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "These sorts of applications are often data- or event-driven, i.e., the processing required is determined by the data to be processed or the arrival of external events."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:36", "is_worked_example": false, "text": "How should the processes communicate with each other?"}, {"start": "00:01:36", "is_lecture": true, "end": "00:01:49", "is_worked_example": false, "text": "If the processes are running out of the same physical memory, it would be easy to arrange to share memory data by mapping the same physical page into the contexts for both processes."}, {"start": "00:01:49", "is_lecture": true, "end": "00:01:55", "is_worked_example": false, "text": "Any data written to that page by one process will be able to be read by the other process."}, {"start": "00:01:55", "is_lecture": true, "end": "00:02:03", "is_worked_example": false, "text": "To make it easier to coordinate the processes' communicating via shared memory, we'll see it's convenient to provide synchronization primitives."}, {"start": "00:02:03", "is_lecture": true, "end": "00:02:08", "is_worked_example": false, "text": "Some ISAs include instructions that make it easy to do the required synchronization."}, {"start": "00:02:08", "is_lecture": true, "end": "00:02:16", "is_worked_example": false, "text": "Another approach is to add OS supervisor calls to pass messages from one process to another."}, {"start": "00:02:16", "is_lecture": true, "end": "00:02:28", "is_worked_example": false, "text": "Message passing involves more overhead than shared memory, but makes the application programming independent of whether the communicating processes are running on the same physical processor."}, {"start": "00:02:28", "is_lecture": true, "end": "00:02:36", "is_worked_example": false, "text": "In this lecture, we'll use the classic producer-consumer problem as our example of concurrent processes that need to communicate and synchronize."}, {"start": "00:02:36", "is_lecture": true, "end": "00:02:40", "is_worked_example": false, "text": "There are two processes: a producer and a consumer."}, {"start": "00:02:40", "is_lecture": true, "end": "00:02:50", "is_worked_example": false, "text": "The producer is running in a loop, which performs some computation <xxx> to generate information, in this case, a single character C."}, {"start": "00:02:50", "is_lecture": true, "end": "00:03:00", "is_worked_example": false, "text": "The consumer is also running a loop, which waits for the next character to arrive from the producer, then performs some computation <yyy>."}, {"start": "00:03:00", "is_lecture": true, "end": "00:03:06", "is_worked_example": false, "text": "The information passing between the producer and consumer could obviously be much more complicated than a single character."}, {"start": "00:03:06", "is_lecture": true, "end": "00:03:16", "is_worked_example": false, "text": "For example, a compiler might produce a sequence of assembly language statements that are passed to the assembler to be converted into the appropriate binary representation."}, {"start": "00:03:16", "is_lecture": true, "end": "00:03:24", "is_worked_example": false, "text": "The user interface front-end for a video game might pass a sequence of player actions to the simulation and rendering back-end."}, {"start": "00:03:24", "is_lecture": true, "end": "00:03:36", "is_worked_example": false, "text": "In fact, the notion of hooking multiple processes together in a processing pipeline is so useful that the Unix and Linux operating systems provide a PIPE primitive in the operating system"}, {"start": "00:03:36", "is_lecture": true, "end": "00:03:42", "is_worked_example": false, "text": "that connects the output channel of the upstream process to the input channel of the downstream process."}, {"start": "00:03:42", "is_lecture": true, "end": "00:03:48", "is_worked_example": false, "text": "Let's look at a timing diagram for the actions of our simple producer/consumer example."}, {"start": "00:03:48", "is_lecture": true, "end": "00:03:52", "is_worked_example": false, "text": "We'll use arrows to indicate when one action happens before another."}, {"start": "00:03:52", "is_lecture": true, "end": "00:04:00", "is_worked_example": false, "text": "Inside a single process, e.g., the producer, the order of execution implies a particular ordering in time:"}, {"start": "00:04:00", "is_lecture": true, "end": "00:04:05", "is_worked_example": false, "text": "the first execution of <xxx> is followed by the sending of the first character."}, {"start": "00:04:05", "is_lecture": true, "end": "00:04:11", "is_worked_example": false, "text": "Then there's the second execution of <xxx>, followed by the sending of the second character, and so on."}, {"start": "00:04:11", "is_lecture": true, "end": "00:04:16", "is_worked_example": false, "text": "In later examples, we'll omit the timing arrows between successive statements in the same program."}, {"start": "00:04:16", "is_lecture": true, "end": "00:04:27", "is_worked_example": false, "text": "We see a similar order of execution in the consumer: the first character is received, then the computation <yyy> is performed for the first time, etc."}, {"start": "00:04:27", "is_lecture": true, "end": "00:04:34", "is_worked_example": false, "text": "Inside of each process, the process' program counter is determining the order in which the computations are performed."}, {"start": "00:04:34", "is_lecture": true, "end": "00:04:38", "is_worked_example": false, "text": "So far, so good -- each process is running as expected."}, {"start": "00:04:38", "is_lecture": true, "end": "00:04:47", "is_worked_example": false, "text": "However, for the producer/consumer system to function correctly as a whole, we'll need to introduce some additional constraints on the order of execution."}, {"start": "00:04:47", "is_lecture": true, "end": "00:04:59", "is_worked_example": false, "text": "These are called \"precedence constraints\" and we'll use this stylized less-than sign to indicate that computation A must precede, i.e., come before, computation B."}, {"start": "00:04:59", "is_lecture": true, "end": "00:05:12", "is_worked_example": false, "text": "In the producer/consumer system we can't consume data before it's been produced, a constraint we can formalize as requiring that the i_th send operation has to precede the i_th receive operation."}, {"start": "00:05:12", "is_lecture": true, "end": "00:05:18", "is_worked_example": false, "text": "This timing constraint is shown as the solid red arrow in the timing diagram."}, {"start": "00:05:18", "is_lecture": true, "end": "00:05:25", "is_worked_example": false, "text": "Assuming we're using, say, a shared memory location to hold the character being transmitted from the producer to the consumer,"}, {"start": "00:05:25", "is_lecture": true, "end": "00:05:32", "is_worked_example": false, "text": "we need to ensure that the producer doesn't overwrite the previous character before it's been read by the consumer."}, {"start": "00:05:32", "is_lecture": true, "end": "00:05:39", "is_worked_example": false, "text": "In other words, we require the i_th receive to precede the i+1_st send."}, {"start": "00:05:39", "is_lecture": true, "end": "00:05:44", "is_worked_example": false, "text": "These timing constraints are shown as the dotted red arrows in the timing diagram."}, {"start": "00:05:44", "is_lecture": true, "end": "00:05:55", "is_worked_example": false, "text": "Together these precedence constraints mean that the producer and consumer are tightly coupled in the sense that a character has to be read by the consumer before the next character can be sent by the producer,"}, {"start": "00:05:55", "is_lecture": true, "end": "00:06:03", "is_worked_example": false, "text": "which might be less than optimal if the <xxx> and <yyy> computations take a variable amount of time."}, {"start": "00:06:03", "is_lecture": true, "end": "00:06:12", "is_worked_example": false, "text": "So let's see how we can relax the constraints to allow for more independence between the producer and consumer."}, {"start": "00:06:12", "is_lecture": true, "end": "00:06:23", "is_worked_example": false, "text": "We can relax the execution constraints on the producer and consumer by having them communicate via N-character first-in-first-out (FIFO) buffer."}, {"start": "00:06:23", "is_lecture": true, "end": "00:06:27", "is_worked_example": false, "text": "As the producer produces characters it inserts them into the buffer."}, {"start": "00:06:27", "is_lecture": true, "end": "00:06:31", "is_worked_example": false, "text": "The consumer reads characters from the buffer in the same order as they were produced."}, {"start": "00:06:31", "is_lecture": true, "end": "00:06:35", "is_worked_example": false, "text": "The buffer can hold between 0 and N characters."}, {"start": "00:06:35", "is_lecture": true, "end": "00:06:41", "is_worked_example": false, "text": "If the buffer holds 0 characters, it's empty; if it holds N characters, it's full."}, {"start": "00:06:41", "is_lecture": true, "end": "00:06:47", "is_worked_example": false, "text": "The producer should wait if the buffer is full, the consumer should wait if the buffer is empty."}, {"start": "00:06:47", "is_lecture": true, "end": "00:06:58", "is_worked_example": false, "text": "Using the N-character FIFO buffer relaxes our second overwrite constraint to the requirement that the i_th receive must happen before i+N_th send."}, {"start": "00:06:58", "is_lecture": true, "end": "00:07:03", "is_worked_example": false, "text": "In other words, the producer can get up to N characters ahead of the consumer."}, {"start": "00:07:03", "is_lecture": true, "end": "00:07:09", "is_worked_example": false, "text": "FIFO buffers are implemented as an N-element character array with two indices:"}, {"start": "00:07:09", "is_lecture": true, "end": "00:07:16", "is_worked_example": false, "text": "the read index indicates the next character to be read, the write index indicates the next character to be written."}, {"start": "00:07:16", "is_lecture": true, "end": "00:07:23", "is_worked_example": false, "text": "We'll also need a counter to keep track of the number of characters held by the buffer, but that's been omitted from this diagram."}, {"start": "00:07:23", "is_lecture": true, "end": "00:07:34", "is_worked_example": false, "text": "The indices are incremented modulo N, i.e., the next element to be accessed after the N-1_st element is the 0_th element, hence the name \"circular buffer\"."}, {"start": "00:07:34", "is_lecture": true, "end": "00:07:36", "is_worked_example": false, "text": "Here's how it works."}, {"start": "00:07:36", "is_lecture": true, "end": "00:07:41", "is_worked_example": false, "text": "The producer runs, using the write index to add the first character to the buffer."}, {"start": "00:07:41", "is_lecture": true, "end": "00:07:47", "is_worked_example": false, "text": "The producer can produce additional characters, but must wait once the buffer is full."}, {"start": "00:07:47", "is_lecture": true, "end": "00:07:55", "is_worked_example": false, "text": "The consumer can receive a character anytime the buffer is not empty, using the read index to keep track of the next character to be read."}, {"start": "00:07:55", "is_lecture": true, "end": "00:08:07", "is_worked_example": false, "text": "Execution of the producer and consumer can proceed in any order so long as the producer doesn't write into a full buffer and the consumer doesn't read from an empty buffer."}, {"start": "00:08:07", "is_lecture": true, "end": "00:08:11", "is_worked_example": false, "text": "Here's what the code for the producer and consumer might look like."}, {"start": "00:08:11", "is_lecture": true, "end": "00:08:17", "is_worked_example": false, "text": "The array and indices for the circular buffer live in shared memory where they can be accessed by both processes."}, {"start": "00:08:17", "is_lecture": true, "end": "00:08:24", "is_worked_example": false, "text": "The SEND routine in the producer uses the write index IN to keep track of where to write the next character."}, {"start": "00:08:24", "is_lecture": true, "end": "00:08:31", "is_worked_example": false, "text": "Similarly the RCV routine in the consumer uses the read index OUT to keep track of the next character to be read."}, {"start": "00:08:31", "is_lecture": true, "end": "00:08:35", "is_worked_example": false, "text": "After each use, each index is incremented modulo N."}, {"start": "00:08:35", "is_lecture": true, "end": "00:08:42", "is_worked_example": false, "text": "The problem with this code is that, as currently written, neither of the two precedence constraints is enforced."}, {"start": "00:08:42", "is_lecture": true, "end": "00:08:48", "is_worked_example": false, "text": "The consumer can read from an empty buffer and the producer can overwrite entries when the buffer is full."}, {"start": "00:08:48", "is_lecture": true, "end": "00:09:00", "is_worked_example": false, "text": "We'll need to modify this code to enforce the constraints and for that we'll introduce a new programming construct that we'll use to provide the appropriate inter-process synchronization."}]}, "C09S02B01-WE.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c9/c9s2/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c9s2v1", "items": [{"start": "00:00:00", "is_lecture": false, "end": "00:00:08", "is_worked_example": true, "text": "In this exercise, we will learn how semaphores can be used to ensure that different precedence constraints in our programs can be satisfied."}, {"start": "00:00:08", "is_lecture": false, "end": "00:00:16", "is_worked_example": true, "text": "Before diving into the use of semaphores to enforce our precedence requirements, let's review what tools we have available to us."}, {"start": "00:00:16", "is_lecture": false, "end": "00:00:21", "is_worked_example": true, "text": "You can think of a semaphore as a shared resource that is limited in quantity."}, {"start": "00:00:21", "is_lecture": false, "end": "00:00:29", "is_worked_example": true, "text": "If we have a semaphore S that is initialized to 0 then it represents the fact that currently resource S is not available."}, {"start": "00:00:29", "is_lecture": false, "end": "00:00:36", "is_worked_example": true, "text": "If S equals 1, then that means that exactly one S resource is available for use."}, {"start": "00:00:36", "is_lecture": false, "end": "00:00:41", "is_worked_example": true, "text": "If S equals 2, then there are 2 S resources available, and so on."}, {"start": "00:00:41", "is_lecture": false, "end": "00:00:46", "is_worked_example": true, "text": "In order to make use of the shared resource, a process must first grab that resource."}, {"start": "00:00:46", "is_lecture": false, "end": "00:00:52", "is_worked_example": true, "text": "This is achieved by adding a wait(S) call before the code that requires the resource."}, {"start": "00:00:52", "is_lecture": false, "end": "00:01:02", "is_worked_example": true, "text": "As long as the value of S equals 0, the code that is waiting for this resource is stalled meaning that it can't get past the wait(S) command."}, {"start": "00:01:02", "is_lecture": false, "end": "00:01:11", "is_worked_example": true, "text": "To get past the wait(S) call, the value of semaphore S must be greater than 0, indicating that the resource is available."}, {"start": "00:01:11", "is_lecture": false, "end": "00:01:17", "is_worked_example": true, "text": "Grabbing the resource is achieved by decrementing the value of the semaphore by 1."}, {"start": "00:01:17", "is_lecture": false, "end": "00:01:21", "is_worked_example": true, "text": "Analogous to the wait(S) command, we have a signal(S) command."}, {"start": "00:01:21", "is_lecture": false, "end": "00:01:28", "is_worked_example": true, "text": "A signal of semaphore S indicates that one additional S resource has become available."}, {"start": "00:01:28", "is_lecture": false, "end": "00:01:32", "is_worked_example": true, "text": "The signal(S) command increments the value of S by 1."}, {"start": "00:01:32", "is_lecture": false, "end": "00:01:40", "is_worked_example": true, "text": "The result of this is that a process that is waiting on S will now be able to grab it and proceed with the next line of code."}, {"start": "00:01:40", "is_lecture": false, "end": "00:01:46", "is_worked_example": true, "text": "Now, lets consider two processes, P1 and P2, that run concurrently."}, {"start": "00:01:46", "is_lecture": false, "end": "00:01:51", "is_worked_example": true, "text": "P1 has two sections of code where section A is followed by section B."}, {"start": "00:01:51", "is_lecture": false, "end": "00:01:56", "is_worked_example": true, "text": "Similarly, P2 has two sections which are C followed by D."}, {"start": "00:01:56", "is_lecture": false, "end": "00:02:06", "is_worked_example": true, "text": "Within each process execution proceeds sequentially, so we are guaranteed that A will always precede B, and C will always precede D."}, {"start": "00:02:06", "is_lecture": false, "end": "00:02:12", "is_worked_example": true, "text": "Let's also assume that there is no looping and that each process runs exactly once."}, {"start": "00:02:12", "is_lecture": false, "end": "00:02:20", "is_worked_example": true, "text": "We want to consider how we can make use of different semaphores to ensure any necessary precedence constraints in the code."}, {"start": "00:02:20", "is_lecture": false, "end": "00:02:28", "is_worked_example": true, "text": "Suppose that the constraint that we need to satisfy is that the section B code completes before the section C code begins execution."}, {"start": "00:02:28", "is_lecture": false, "end": "00:02:36", "is_worked_example": true, "text": "We can achieve this using semaphore S by first initializing the semaphore to 0 in shared memory."}, {"start": "00:02:36", "is_lecture": false, "end": "00:02:46", "is_worked_example": true, "text": "Next, in order to ensure that section C code does not begin running too early, we add a wait(S) call before the section C code."}, {"start": "00:02:46", "is_lecture": false, "end": "00:02:52", "is_worked_example": true, "text": "As long as S = 0, the code in process P2 will not get to run."}, {"start": "00:02:52", "is_lecture": false, "end": "00:03:00", "is_worked_example": true, "text": "P1 on the other hand, will not be constrained in this way, so section A code can begin running right away."}, {"start": "00:03:00", "is_lecture": false, "end": "00:03:05", "is_worked_example": true, "text": "Since section B follows section A, it will be executed after section A."}, {"start": "00:03:05", "is_lecture": false, "end": "00:03:16", "is_worked_example": true, "text": "Once B completes, process P1 needs to signal our semaphore to indicate that it is now okay for process P2 to begin its execution."}, {"start": "00:03:16", "is_lecture": false, "end": "00:03:23", "is_worked_example": true, "text": "The signal(S) call will set S = 1, which will allow P2 to finally move beyond the wait(S) command."}, {"start": "00:03:23", "is_lecture": false, "end": "00:03:35", "is_worked_example": true, "text": "Next, lets consider a slightly more complicated constraint where section D precedes section A, OR section B precedes section C."}, {"start": "00:03:35", "is_lecture": false, "end": "00:03:39", "is_worked_example": true, "text": "In other words, P1 and P2 cannot overlap."}, {"start": "00:03:39", "is_lecture": false, "end": "00:03:44", "is_worked_example": true, "text": "One has to run followed by the other but either of them can be the one to run first."}, {"start": "00:03:44", "is_lecture": false, "end": "00:03:50", "is_worked_example": true, "text": "To achieve this we want to use our S semaphore as a mutual exclusion semaphore."}, {"start": "00:03:50", "is_lecture": false, "end": "00:03:59", "is_worked_example": true, "text": "A mutual exclusion semaphore, or mutex, ensures that only one complete block of code can run at a time without getting interrupted."}, {"start": "00:03:59", "is_lecture": false, "end": "00:04:08", "is_worked_example": true, "text": "This can be achieved by initializing our semaphore S to 1 and having a wait(S) statement at the top of both processes."}, {"start": "00:04:08", "is_lecture": false, "end": "00:04:14", "is_worked_example": true, "text": "Since S is initialized to 1, only one of the two processes will be able to grab the S semaphore."}, {"start": "00:04:14", "is_lecture": false, "end": "00:04:18", "is_worked_example": true, "text": "Whichever process happens to grab it first is the one that will run first."}, {"start": "00:04:18", "is_lecture": false, "end": "00:04:27", "is_worked_example": true, "text": "There is one last piece of code we need to add to complete our requirements which is that at the end of both processes' code, we must signal(S)."}, {"start": "00:04:27", "is_lecture": false, "end": "00:04:37", "is_worked_example": true, "text": "If we do not signal(S) then only the process that happened to grab the S semaphore first will get to run while the other is stuck waiting for the S semaphore."}, {"start": "00:04:37", "is_lecture": false, "end": "00:04:46", "is_worked_example": true, "text": "If at the end of the process, we signal S, then S gets incremented back to 1 thus allowing the next process to execute."}, {"start": "00:04:46", "is_lecture": false, "end": "00:04:53", "is_worked_example": true, "text": "Note that because this code does not loop, there is no concern about the first process grabbing the S semaphore again."}, {"start": "00:04:53", "is_lecture": false, "end": "00:04:57", "is_worked_example": true, "text": "Finally, lets consider one last set of constraints."}, {"start": "00:04:57", "is_lecture": false, "end": "00:05:07", "is_worked_example": true, "text": "In this case, we want to ensure that the first section of both processes P1 and P2 run before the second section of processes P1 and P2."}, {"start": "00:05:07", "is_lecture": false, "end": "00:05:12", "is_worked_example": true, "text": "In other words A must precede B and D, and C must precede B and D."}, {"start": "00:05:12", "is_lecture": false, "end": "00:05:21", "is_worked_example": true, "text": "The constraint that A must precede B, and C must precede D is satisfied by default because the code is always executed in order."}, {"start": "00:05:21", "is_lecture": false, "end": "00:05:27", "is_worked_example": true, "text": "This means that our constraint reduces to A preceding D, and C preceding B."}, {"start": "00:05:27", "is_lecture": false, "end": "00:05:35", "is_worked_example": true, "text": "To achieve this, we need to use two semaphores, say S and T and initialize them to 0."}, {"start": "00:05:35", "is_lecture": false, "end": "00:05:47", "is_worked_example": true, "text": "After the first section of a process completes, it should signal to the other process that it may begin its second section of code provided that it has already completed its first section of code."}, {"start": "00:05:47", "is_lecture": false, "end": "00:05:56", "is_worked_example": true, "text": "To ensure that it has already completed its first section of code, we place the signal calls between the two sections of code in each process."}, {"start": "00:05:56", "is_lecture": false, "end": "00:06:06", "is_worked_example": true, "text": "In addition to signaling the other process that it may proceed, each of the processes needs to wait for the semaphore that the other process is signaling."}, {"start": "00:06:06", "is_lecture": false, "end": "00:06:14", "is_worked_example": true, "text": "This combination of signal and wait ensures that sections A and C of the code will run before sections B and D."}, {"start": "00:06:14", "is_lecture": false, "end": "00:06:25", "is_worked_example": true, "text": "Since the semaphores are initialized to 0, the wait(S) will not complete until P1 calls signal(S) at which point it has already completed section A."}, {"start": "00:06:25", "is_lecture": false, "end": "00:06:35", "is_worked_example": true, "text": "Similarly, the wait(T) will not complete until P2 calls signal(T) at which point it has already completed section C."}, {"start": "00:06:35", "is_lecture": false, "end": "00:06:43", "is_worked_example": true, "text": "So once the processes can get past their wait commands, we are guaranteed that both first sections of code have already run."}, {"start": "00:06:43", "is_lecture": false, "end": "00:06:52", "is_worked_example": true, "text": "We have also not forced any additional constraints by requiring A to run before C or C to run before A, and so on."}, {"start": "00:06:52", "is_lecture": false, "end": "00:07:00", "is_worked_example": true, "text": "Of course we could have swapped our use of the S and T semaphores and ended up with exactly the same behavior."}, {"start": "00:07:00", "is_lecture": false, "end": "00:07:05", "is_worked_example": true, "text": "Note, however, that we cannot swap the signal and wait commands around."}, {"start": "00:07:05", "is_lecture": false, "end": "00:07:13", "is_worked_example": true, "text": "If we tried to call wait before signal, then both processes would get deadlocked waiting for a semaphore that never gets signaled."}, {"start": "00:07:13", "is_lecture": false, "end": "00:07:31", "is_worked_example": true, "text": "This highlights the fact that when using semaphores you must always be very careful to not only worry about satisfying the desired requirements, but also ensuring that there is no possibility of ending up in a deadlock situation where one or more processes can never run to completion."}]}, "C08S01B04-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c8/c8s1/4?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c8s1v4", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "Here's an old quiz problem we can use to test our understanding of all the factors that went into the final design of our ReadKey() SVC code."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:20", "is_worked_example": false, "text": "We're considering three different versions (R1, R2, and R3) of the ReadKey() SVC code, all variants of the various attempts from the previous section."}, {"start": "00:00:20", "is_lecture": true, "end": "00:00:25", "is_worked_example": false, "text": "And there are three types of systems (Models A, B, and C)."}, {"start": "00:00:25", "is_lecture": true, "end": "00:00:30", "is_worked_example": false, "text": "We've been asked to match the three handlers to the appropriate system."}, {"start": "00:00:30", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "Looking at R1, we see it's similar to Attempt #2 from the previous section, except it always reads from the same keyboard regardless of the process making the SVC request."}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:50", "is_worked_example": false, "text": "That wouldn't make much sense in a timesharing system since a single stream of input characters would be shared across all the processes."}, {"start": "00:00:50", "is_lecture": true, "end": "00:00:57", "is_worked_example": false, "text": "So this handler must be intended for the Model C system, which has only a single process."}, {"start": "00:00:57", "is_lecture": true, "end": "00:01:08", "is_worked_example": false, "text": "Looking at R2, we see it's similar to Attempt #1 from the previous section, which had the fatal flaw of a potentially infinite loop if attempting to read from an empty buffer."}, {"start": "00:01:08", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "So this code would only run successfully on the Model B system, which *does* allow device interrupts even when the CPU is running inside an SVC call."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:28", "is_worked_example": false, "text": "So the keyboard interrupt would interrupt the while loop in R2 and the next iteration of the loop would discover that buffer was no longer empty."}, {"start": "00:01:28", "is_lecture": true, "end": "00:01:36", "is_worked_example": false, "text": "By the process of elimination that leaves the R3 handler to be paired with the Model A system."}, {"start": "00:01:36", "is_lecture": true, "end": "00:01:45", "is_worked_example": false, "text": "R3 is Attempt #3 from the previous section and is designed for our standard system in which the kernel is uninterruptible."}, {"start": "00:01:45", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "The problem goes on to say that a fumble-fingered summer intern has jumbled up the disks containing the handlers and sent an unknown handler version to each user running one of the three model systems."}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:07", "is_worked_example": false, "text": "To atone for the mistake, he's been assigned the task of reading various user messages sent after the user has tried the new handler disk on their particular system."}, {"start": "00:02:07", "is_lecture": true, "end": "00:02:14", "is_worked_example": false, "text": "Based on the message, he's been asked to identify which handler disk and system the user is using."}, {"start": "00:02:14", "is_lecture": true, "end": "00:02:21", "is_worked_example": false, "text": "The first message says \"I get compile-time errors; Scheduler and ProcTbl are undefined!\""}, {"start": "00:02:21", "is_lecture": true, "end": "00:02:31", "is_worked_example": false, "text": "On the right of the slide we've included a table enumerating all the combinations of handlers and systems, where we've X-ed the matches from the previous slide"}, {"start": "00:02:31", "is_lecture": true, "end": "00:02:29", "is_worked_example": false, "text": "since they correspond to when the new handler would be the same as the old handler and the user wouldn't be sending a message!"}, {"start": "00:02:29", "is_lecture": true, "end": "00:02:48", "is_worked_example": false, "text": "The phrase \"Scheduler and ProcTbl are undefined\" wouldn't apply to a timesharing system, which includes both symbols."}, {"start": "00:02:48", "is_lecture": true, "end": "00:02:52", "is_worked_example": false, "text": "So we can eliminate the first two columns from consideration."}, {"start": "00:02:52", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "And we can also eliminate the second row, since handler R2 doesn't include a call to Scheduler."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:06", "is_worked_example": false, "text": "So this message came from a user trying to run handler R3 on a Model C system."}, {"start": "00:03:06", "is_lecture": true, "end": "00:03:14", "is_worked_example": false, "text": "Since Model C doesn't support timesharing, it would have neither Scheduler nor ProcTbl as part the OS code."}, {"start": "00:03:14", "is_lecture": true, "end": "00:03:22", "is_worked_example": false, "text": "Okay, here's the next message: \"Hey, now the system always reads everybody's input from keyboard 0."}, {"start": "00:03:22", "is_lecture": true, "end": "00:03:27", "is_worked_example": false, "text": "Besides that, it seems to waste a lot more CPU cycles than it used to.\""}, {"start": "00:03:27", "is_lecture": true, "end": "00:03:34", "is_worked_example": false, "text": "R1 is the only handler that always reads from keyboard 0, so we can eliminate rows 2 and 3."}, {"start": "00:03:34", "is_lecture": true, "end": "00:03:41", "is_worked_example": false, "text": "So how can we tell if R1 is being run on a Model A or a Model B system?"}, {"start": "00:03:41", "is_lecture": true, "end": "00:03:51", "is_worked_example": false, "text": "The R1 handler wastes a lot of cycles looping while waiting for a character to arrive and the implication is that was a big change for the user"}, {"start": "00:03:51", "is_lecture": true, "end": "00:03:36", "is_worked_example": false, "text": "since they're complaining that running R1 is wasting time compared to their previous handler."}, {"start": "00:03:36", "is_lecture": true, "end": "00:04:04", "is_worked_example": false, "text": "If the user had been running R2 on a model B system, they're already used to the performance hit of looping"}, {"start": "00:04:04", "is_lecture": true, "end": "00:04:12", "is_worked_example": false, "text": "and so wouldn't have noticed a performance difference switching to R1, so we can eliminate Model B from consideration."}, {"start": "00:04:12", "is_lecture": true, "end": "00:04:18", "is_worked_example": false, "text": "So this message came from a user running handler R1 on a model A system."}, {"start": "00:04:18", "is_lecture": true, "end": "00:04:23", "is_worked_example": false, "text": "The final message reads \"Neat, the new system seems to work fine."}, {"start": "00:04:23", "is_lecture": true, "end": "00:04:27", "is_worked_example": false, "text": "It even wastes less CPU time than it used to!\""}, {"start": "00:04:27", "is_lecture": true, "end": "00:04:34", "is_worked_example": false, "text": "Since the system works as expected with the new handler, we can eliminate a lot of possibilities."}, {"start": "00:04:34", "is_lecture": true, "end": "00:04:48", "is_worked_example": false, "text": "Handler R1 wouldn't work fine on a timesharing system since the user could tell that the processes were now all reading from the same keyboard buffer, so we can eliminate R1 on Models A and B."}, {"start": "00:04:48", "is_lecture": true, "end": "00:04:59", "is_worked_example": false, "text": "And handlers R2 and R3 wouldn't work on a Model C system since that doesn't include process tables or scheduling, eliminating the right-most column."}, {"start": "00:04:59", "is_lecture": true, "end": "00:05:11", "is_worked_example": false, "text": "Finally handler R2 wouldn't work on a Model A system with its uninterruptible kernel since any attempt to read from an empty buffer would cause an infinite loop."}, {"start": "00:05:11", "is_lecture": true, "end": "00:05:16", "is_worked_example": false, "text": "So, the message must have been sent by a Model B user now running R3."}, {"start": "00:05:16", "is_lecture": true, "end": "00:05:18", "is_worked_example": false, "text": "Well, that was fun!"}, {"start": "00:05:18", "is_lecture": true, "end": "00:05:23", "is_worked_example": false, "text": "Just like solving the logic puzzles you find in games magazines :)"}]}, "C11S01B03-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c11/c11s1/3?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c11s1v3", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:09", "is_worked_example": false, "text": "If we want our system to be modular and expandable, how should its design accommodate components that the user might add at a later time?"}, {"start": "00:00:09", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "For many years the approach was to provide a way to plug additional printed circuit boards into the main \"motherboard\" that holds the CPU, memory, and the initial collection of I/O components."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "The socket on the motherboard connects the circuitry on the add-in card to the signals on the motherboard that allow the CPU to communicate with the add-in card."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:36", "is_worked_example": false, "text": "These signals include power and a clock signal used to time the communication, along with the following."}, {"start": "00:00:36", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "* Address wires to select different communication end points on the add-in card."}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:47", "is_worked_example": false, "text": "The end points might include memory locations, control registers, diagnostic ports, etc."}, {"start": "00:00:47", "is_lecture": true, "end": "00:00:51", "is_worked_example": false, "text": "* Data wires for transferring data to and from the CPU."}, {"start": "00:00:51", "is_lecture": true, "end": "00:00:57", "is_worked_example": false, "text": "In older systems, there would many data wires to support byte- or word-width data transfers."}, {"start": "00:00:57", "is_lecture": true, "end": "00:01:07", "is_worked_example": false, "text": "* Some number of control wires that tell the add-in card when a particular transfer has started and that allow the add-in card to indicate when it has responded."}, {"start": "00:01:07", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "If there are multiple slots for plugging in multiple add-in cards, the same signals might be connected to all the cards and the address wires would be used to sort out which transfers were intended for which cards."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:24", "is_worked_example": false, "text": "Collectively these signals are referred to as the system bus."}, {"start": "00:01:24", "is_lecture": true, "end": "00:01:33", "is_worked_example": false, "text": "\"Bus\" is system-architect jargon for a collection of wires used to transfer data using a pre-determined communication protocol."}, {"start": "00:01:33", "is_lecture": true, "end": "00:01:37", "is_worked_example": false, "text": "Here's an example of how a bus transaction might work."}, {"start": "00:01:37", "is_lecture": true, "end": "00:01:48", "is_worked_example": false, "text": "The CLK signal is used to time when signals are placed on the bus wires (at the assertion edge of CLK) and when they're read by the recipient (at the sample edge of the CLK)."}, {"start": "00:01:48", "is_lecture": true, "end": "00:01:58", "is_worked_example": false, "text": "The timing of the clock waveform is designed to allow enough time for the signals to propagate down the bus and reach valid logic levels at all the receivers."}, {"start": "00:01:58", "is_lecture": true, "end": "00:02:04", "is_worked_example": false, "text": "The component initiating the transaction is called the bus master who is said to \"own\" the bus."}, {"start": "00:02:04", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "Most buses provide a mechanism for transferring ownership from one component to another."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:23", "is_worked_example": false, "text": "The master sets the bus lines to indicate the desired operation (read, write, block transfer, etc.), the address of the recipient, and, in the case of a write operation, the data to be sent to the recipient."}, {"start": "00:02:23", "is_lecture": true, "end": "00:02:31", "is_worked_example": false, "text": "The intended recipient, called the slave, is watching the bus lines looking for its address at each sample edge."}, {"start": "00:02:31", "is_lecture": true, "end": "00:02:40", "is_worked_example": false, "text": "When it sees a transaction for itself, the slave performs the requested operation, using a bus signal to indicate when the operation is complete."}, {"start": "00:02:40", "is_lecture": true, "end": "00:02:45", "is_worked_example": false, "text": "On completion it may use the data wires to return information to the master."}, {"start": "00:02:45", "is_lecture": true, "end": "00:02:55", "is_worked_example": false, "text": "The bus itself may include circuitry to look for transactions where the slave isn't responding and, after an appropriate interval,"}, {"start": "00:02:55", "is_lecture": true, "end": "00:02:59", "is_worked_example": false, "text": "generate an error response so the master can take the appropriate action."}, {"start": "00:02:59", "is_lecture": true, "end": "00:03:10", "is_worked_example": false, "text": "This sort of bus architecture proved to be a very workable design for accommodating add-in cards as long as the rate of transactions wasn't too fast, say less than 50 Mhz."}, {"start": "00:03:10", "is_lecture": true, "end": "00:03:22", "is_worked_example": false, "text": "But as system speeds increased, transaction rates had to increase to keep system performance at acceptable levels, so the time for each transaction got smaller."}, {"start": "00:03:22", "is_lecture": true, "end": "00:03:27", "is_worked_example": false, "text": "With less time for signaling on the bus wires, various effects began loom large."}, {"start": "00:03:27", "is_lecture": true, "end": "00:03:35", "is_worked_example": false, "text": "If the clock had too short a period, there wasn't enough time for the master to see the assertion edge, enable its drivers,"}, {"start": "00:03:35", "is_lecture": true, "end": "00:03:43", "is_worked_example": false, "text": "have the signal propagate down a long bus to the intended receiver and be stable at each receiver for long enough before the sample edge."}, {"start": "00:03:43", "is_lecture": true, "end": "00:03:49", "is_worked_example": false, "text": "Another problem was that the clock signal would arrive at different cards at different times."}, {"start": "00:03:49", "is_lecture": true, "end": "00:04:00", "is_worked_example": false, "text": "So a card with an early-arriving clock might decide it was its turn to start driving the bus signals, while a card with a late-arriving clock might still be driving the bus from the previous cycle."}, {"start": "00:04:00", "is_lecture": true, "end": "00:04:06", "is_worked_example": false, "text": "These momentary conflicts between drivers could add huge amounts of electrical noise to the system."}, {"start": "00:04:06", "is_lecture": true, "end": "00:04:14", "is_worked_example": false, "text": "Another big issue is that energy would reflect off all the small impedance discontinuities caused by the bus connectors."}, {"start": "00:04:14", "is_lecture": true, "end": "00:04:21", "is_worked_example": false, "text": "If there were many connectors, there would be many small echoes which would could corrupt the signal seen by various receivers."}, {"start": "00:04:21", "is_lecture": true, "end": "00:04:29", "is_worked_example": false, "text": "The equations in the upper right show how much of the signal energy is transmitted and how much is reflected at each discontinuity."}, {"start": "00:04:29", "is_lecture": true, "end": "00:04:35", "is_worked_example": false, "text": "The net effect was like trying to talk very fast while yelling into the Grand Canyon."}, {"start": "00:04:35", "is_lecture": true, "end": "00:04:43", "is_worked_example": false, "text": "The echoes could distort the message beyond recognition unless sufficient time was allocated between words for the echoes to die away."}, {"start": "00:04:43", "is_lecture": true, "end": "00:04:54", "is_worked_example": false, "text": "Eventually buses were relegated to relatively low-speed communication tasks and a different approach had to be developed for high-speed communication."}]}, "C03S01B09-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c3/c3s1/9?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c3s1v9", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:05", "is_worked_example": false, "text": "So here's the final version of our 5-stage pipelined data path."}, {"start": "00:00:05", "is_lecture": true, "end": "00:00:11", "is_worked_example": false, "text": "To deal with data hazards we've added stall logic to the IF and RF input registers."}, {"start": "00:00:11", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "We've also added bypass muxes on the output of the register file read ports so we can route values from later in the data path if we need to access a register value that's been computed but not yet written to the register file."}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:32", "is_worked_example": false, "text": "We also made a provision to insert NOPs into the pipeline after the RF stage if the IF and RF stages are stalled."}, {"start": "00:00:32", "is_lecture": true, "end": "00:00:39", "is_worked_example": false, "text": "To deal with control hazards, we speculate that the next instruction is at PC+4."}, {"start": "00:00:39", "is_lecture": true, "end": "00:00:47", "is_worked_example": false, "text": "But for JMPs and taken branches, that guess is wrong so we added a provision for annulling the instruction in the IF stage."}, {"start": "00:00:47", "is_lecture": true, "end": "00:00:52", "is_worked_example": false, "text": "To deal with exceptions and interrupts we added instruction muxes in all but the final pipeline stage."}, {"start": "00:00:52", "is_lecture": true, "end": "00:01:02", "is_worked_example": false, "text": "An instruction that causes an exception is replaced by our magic BNE instruction to capture its PC+4 value."}, {"start": "00:01:02", "is_lecture": true, "end": "00:01:05", "is_worked_example": false, "text": "And instructions in earlier stages are annulled."}, {"start": "00:01:05", "is_lecture": true, "end": "00:01:14", "is_worked_example": false, "text": "All this extra circuitry has been added to ensure that pipelined execution gives the same result as unpipelined execution."}, {"start": "00:01:14", "is_lecture": true, "end": "00:01:23", "is_worked_example": false, "text": "The use of bypassing and branch prediction ensures that data and control hazards have only a small negative impact on the effective CPI."}, {"start": "00:01:23", "is_lecture": true, "end": "00:01:29", "is_worked_example": false, "text": "This means that the much shorter clock period translates to a large increase in instruction throughput."}, {"start": "00:01:29", "is_lecture": true, "end": "00:01:35", "is_worked_example": false, "text": "It's worth remembering the strategies we used to deal with hazards: stalling, bypassing and speculation."}, {"start": "00:01:35", "is_lecture": true, "end": "00:01:45", "is_worked_example": false, "text": "Most execution issues can be dealt with using one of these strategies, so keep these in mind if you ever need to design a high-performance pipelined system."}, {"start": "00:01:45", "is_lecture": true, "end": "00:01:48", "is_worked_example": false, "text": "This completes our discussion of pipelining."}, {"start": "00:01:48", "is_lecture": true, "end": "00:01:55", "is_worked_example": false, "text": "We'll explore other avenues to higher processor performance in the last lecture, which discusses parallel processing."}]}, "C05S01B01-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c5/c5s1/1?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c5s1v1", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:05", "is_worked_example": false, "text": "In this lecture we return to the memory system that we last discussed in Lecture 14 of Part 2."}, {"start": "00:00:05", "is_lecture": true, "end": "00:00:15", "is_worked_example": false, "text": "There we learned about the fundamental tradeoff in current memory technologies: as the memory's capacity increases, so does it access time."}, {"start": "00:00:15", "is_lecture": true, "end": "00:00:22", "is_worked_example": false, "text": "It takes some architectural cleverness to build a memory system that has a large capacity and a small average access time."}, {"start": "00:00:22", "is_lecture": true, "end": "00:00:29", "is_worked_example": false, "text": "The cleverness is embodied in the cache, a hardware subsystem that lives between the CPU and main memory."}, {"start": "00:00:29", "is_lecture": true, "end": "00:00:43", "is_worked_example": false, "text": "Modern CPUs have several levels of cache, where the modest-capacity first level has an access time close to that of the CPU, and higher levels of cache have slower access times but larger capacities."}, {"start": "00:00:43", "is_lecture": true, "end": "00:00:57", "is_worked_example": false, "text": "Caches give fast access to a small number of memory locations, using associative addressing so that the cache has the ability to hold the contents of the memory locations the CPU is accessing most frequently."}, {"start": "00:00:57", "is_lecture": true, "end": "00:01:01", "is_worked_example": false, "text": "The current contents of the cache are managed automatically by the hardware."}, {"start": "00:01:01", "is_lecture": true, "end": "00:01:13", "is_worked_example": false, "text": "Caches work well because of the principle of locality: if the CPU accesses location X at time T, it's likely to access nearby locations in the not-too-distant future."}, {"start": "00:01:13", "is_lecture": true, "end": "00:01:26", "is_worked_example": false, "text": "The cache is organized so that nearby locations can all reside in the cache simultaneously, using a simple indexing scheme to choose which cache location should be checked for a matching address."}, {"start": "00:01:26", "is_lecture": true, "end": "00:01:32", "is_worked_example": false, "text": "If the address requested by the CPU resides in the cache, access time is quite fast."}, {"start": "00:01:32", "is_lecture": true, "end": "00:01:46", "is_worked_example": false, "text": "In order to increase the probability that requested addresses reside in the cache, we introduced the notion of \"associativity\", which increased the number of cache locations checked on each access and"}, {"start": "00:01:46", "is_lecture": true, "end": "00:01:51", "is_worked_example": false, "text": "solved the problem of having, say, instructions and data compete for the same cache locations.."}, {"start": "00:01:51", "is_lecture": true, "end": "00:01:58", "is_worked_example": false, "text": "We also discussed appropriate choices for block size (the number of words in a cache line),"}, {"start": "00:01:58", "is_lecture": true, "end": "00:02:04", "is_worked_example": false, "text": "replacement policy (how to choose which cache line to reuse on a cache miss),"}, {"start": "00:02:04", "is_lecture": true, "end": "00:02:09", "is_worked_example": false, "text": "and write policy (deciding when to write changed data back to main memory)."}, {"start": "00:02:09", "is_lecture": true, "end": "00:02:17", "is_worked_example": false, "text": "We'll see these same choices again in this lecture as we work to expand the memory hierarchy beyond main memory."}, {"start": "00:02:17", "is_lecture": true, "end": "00:02:25", "is_worked_example": false, "text": "We never discussed where the data in main memory comes from and how the process of filling main memory is managed."}, {"start": "00:02:25", "is_lecture": true, "end": "00:02:27", "is_worked_example": false, "text": "That's the topic of today's lecture.."}, {"start": "00:02:27", "is_lecture": true, "end": "00:02:41", "is_worked_example": false, "text": "Flash drives and hard disks provide storage options that have more capacity than main memory, with the added benefit of being non-volatile, i.e., they continue to store data even when turned off."}, {"start": "00:02:41", "is_lecture": true, "end": "00:02:52", "is_worked_example": false, "text": "The generic name for these new devices is \"secondary storage\", where data will reside until it's moved to \"primary storage\", i.e., main memory, for use."}, {"start": "00:02:52", "is_lecture": true, "end": "00:03:01", "is_worked_example": false, "text": "So when we first turn on a computer system, all of its data will be found in secondary storage, which we'll think of as the final level of our memory hierarchy."}, {"start": "00:03:01", "is_lecture": true, "end": "00:03:16", "is_worked_example": false, "text": "As we think about the right memory architecture, we'll build on the ideas from our previous discussion of caches, and, indeed, think of main memory as another level of cache for the permanent, high-capacity secondary storage."}, {"start": "00:03:16", "is_lecture": true, "end": "00:03:26", "is_worked_example": false, "text": "We'll be building what we call a virtual memory system, which, like caches, will automatically move data from secondary storage into main memory as needed."}, {"start": "00:03:26", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "The virtual memory system will also let us control what data can be accessed by the program, serving as a stepping stone to building a system that can securely run many programs on a single CPU."}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:41", "is_worked_example": false, "text": "Let's get started!"}, {"start": "00:03:41", "is_lecture": true, "end": "00:03:48", "is_worked_example": false, "text": "Here we see the cache and main memory, the two components of our memory system as developed in Lecture 14."}, {"start": "00:03:48", "is_lecture": true, "end": "00:03:51", "is_worked_example": false, "text": "And here's our new secondary storage layer."}, {"start": "00:03:51", "is_lecture": true, "end": "00:03:56", "is_worked_example": false, "text": "The good news: the capacity of secondary storage is huge!"}, {"start": "00:03:56", "is_lecture": true, "end": "00:04:06", "is_worked_example": false, "text": "Even the most modest modern computer system will have 100's of gigabytes of secondary storage and having a terabyte or two is not uncommon on medium-size desktop computers."}, {"start": "00:04:06", "is_lecture": true, "end": "00:04:15", "is_worked_example": false, "text": "Secondary storage for the cloud can grow to many petabytes (a petabyte is 10^15 bytes or a million gigabytes)."}, {"start": "00:04:15", "is_lecture": true, "end": "00:04:22", "is_worked_example": false, "text": "The bad news: disk access times are 100,000 times longer that those of DRAM."}, {"start": "00:04:22", "is_lecture": true, "end": "00:04:30", "is_worked_example": false, "text": "So the change in access time from DRAM to disk is much, much larger than the change from caches to DRAM."}, {"start": "00:04:30", "is_lecture": true, "end": "00:04:40", "is_worked_example": false, "text": "When looking at DRAM timing, we discovered that the additional access time for retrieving a contiguous block of words was small compared to the access time for the first word,"}, {"start": "00:04:40", "is_lecture": true, "end": "00:04:46", "is_worked_example": false, "text": "so fetching a block was the right plan assuming we'd eventually access the additional words."}, {"start": "00:04:46", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "For disks, the access time difference between the first word and successive words is even more dramatic."}, {"start": "00:04:53", "is_lecture": true, "end": "00:04:58", "is_worked_example": false, "text": "So, not surprisingly, we'll be reading fairly large blocks of data from disk."}, {"start": "00:04:58", "is_lecture": true, "end": "00:05:10", "is_worked_example": false, "text": "The consequence of the much, much larger secondary-storage access time is that it will be very time consuming to access disk if the data we need is not in main memory."}, {"start": "00:05:10", "is_lecture": true, "end": "00:05:16", "is_worked_example": false, "text": "So we need to design our virtual memory system to minimize misses when accessing main memory."}, {"start": "00:05:16", "is_lecture": true, "end": "00:05:30", "is_worked_example": false, "text": "A miss, and the subsequent disk access, will have a huge impact on the average memory access time, so the miss rate will need to be very, very small compared to, say, the rate of executing instructions."}, {"start": "00:05:30", "is_lecture": true, "end": "00:05:38", "is_worked_example": false, "text": "Given the enormous miss penalties of secondary storage, what does that tell us about how it should be used as part of our memory hierarchy?"}, {"start": "00:05:38", "is_lecture": true, "end": "00:05:47", "is_worked_example": false, "text": "We will need high associativity, i.e., we need a great deal of flexibility on how data from disk can be located in main memory."}, {"start": "00:05:47", "is_lecture": true, "end": "00:06:00", "is_worked_example": false, "text": "In other words, if our working set of memory accesses fit in main memory, our virtual memory system should make that possible, avoiding unnecessary collisions between accesses to one block of data and another."}, {"start": "00:06:00", "is_lecture": true, "end": "00:06:08", "is_worked_example": false, "text": "We'll want to use a large block size to take advantage of the low incremental cost of reading successive words from disk."}, {"start": "00:06:08", "is_lecture": true, "end": "00:06:18", "is_worked_example": false, "text": "And, given the principle of locality, we'd expect to be accessing other words of the block, thus amortizing the cost of the miss over many future hits."}, {"start": "00:06:18", "is_lecture": true, "end": "00:06:32", "is_worked_example": false, "text": "Finally, we'll want to use a write-back strategy where we'll only update the contents of disk when data that's changed in main memory needs to be replaced by data from other blocks of secondary storage."}, {"start": "00:06:32", "is_lecture": true, "end": "00:06:36", "is_worked_example": false, "text": "There is upside to misses having such long latencies."}, {"start": "00:06:36", "is_lecture": true, "end": "00:06:43", "is_worked_example": false, "text": "We can manage the organization of main memory and the accesses to secondary storage in software."}, {"start": "00:06:43", "is_lecture": true, "end": "00:06:52", "is_worked_example": false, "text": "Even it takes 1000's of instructions to deal with the consequences of a miss, executing those instructions is quick compared to the access time of a disk."}, {"start": "00:06:52", "is_lecture": true, "end": "00:06:57", "is_worked_example": false, "text": "So our strategy will be to handle hits in hardware and misses in software."}, {"start": "00:06:57", "is_lecture": true, "end": "00:07:08", "is_worked_example": false, "text": "This will lead to simple memory management hardware and the possibility of using very clever strategies implemented in software to figure out what to do on misses."}]}, "C05S01B08-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c5/c5s1/8?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c5s1v8", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:12", "is_worked_example": false, "text": "The page map provides the context for interpreting virtual addresses, i.e., it provides the information needed to correctly determine where to find a virtual address in main memory or secondary storage."}, {"start": "00:00:12", "is_lecture": true, "end": "00:00:19", "is_worked_example": false, "text": "Several programs may be simultaneously loaded into main memory, each with its own context."}, {"start": "00:00:19", "is_lecture": true, "end": "00:00:24", "is_worked_example": false, "text": "Note that the separate contexts ensure that the programs don't interfere which each other."}, {"start": "00:00:24", "is_lecture": true, "end": "00:00:35", "is_worked_example": false, "text": "For example, the physical location for virtual address 0 in one program will be different than the physical location for virtual address 0 in another program."}, {"start": "00:00:35", "is_lecture": true, "end": "00:00:41", "is_worked_example": false, "text": "Each program operates independently in its own virtual address space."}, {"start": "00:00:41", "is_lecture": true, "end": "00:00:48", "is_worked_example": false, "text": "It's the context provided by the page map that allows them to coexist and share a common physical memory."}, {"start": "00:00:48", "is_lecture": true, "end": "00:00:52", "is_worked_example": false, "text": "So we need to switch contexts when switching programs."}, {"start": "00:00:52", "is_lecture": true, "end": "00:00:55", "is_worked_example": false, "text": "This is accomplished by reloading the page map."}, {"start": "00:00:55", "is_lecture": true, "end": "00:01:08", "is_worked_example": false, "text": "In a timesharing system, the CPU will periodically switch from running one program to another, giving the illusion that multiple programs are each running on their own virtual machine."}, {"start": "00:01:08", "is_lecture": true, "end": "00:01:13", "is_worked_example": false, "text": "This is accomplished by switching contexts when switching the CPU state to the next program."}, {"start": "00:01:13", "is_lecture": true, "end": "00:01:28", "is_worked_example": false, "text": "There's a privileged set of code called the operating system (OS) that manages the sharing of one physical processor and main memory amongst many programs, each with its own CPU state and virtual address space."}, {"start": "00:01:28", "is_lecture": true, "end": "00:01:37", "is_worked_example": false, "text": "The OS is effectively creating many virtual machines and choreographing their execution using a single set of shared physical resources."}, {"start": "00:01:37", "is_lecture": true, "end": "00:01:43", "is_worked_example": false, "text": "The OS runs in a special OS context, which we call the kernel."}, {"start": "00:01:43", "is_lecture": true, "end": "00:01:48", "is_worked_example": false, "text": "The OS contains the necessary exception handlers and timesharing support."}, {"start": "00:01:48", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "Since it has to manage physical memory, it's allowed to access any physical location as it deals with page faults, etc."}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:05", "is_worked_example": false, "text": "Exceptions in running programs cause the hardware to switch to the kernel context, which we call entering \"kernel mode\"."}, {"start": "00:02:05", "is_lecture": true, "end": "00:02:12", "is_worked_example": false, "text": "After the exception handling is complete, execution of the program resumes in what we call \"user mode\"."}, {"start": "00:02:12", "is_lecture": true, "end": "00:02:20", "is_worked_example": false, "text": "Since the OS runs in kernel mode it has privileged access to many hardware registers that are inaccessible in user mode."}, {"start": "00:02:20", "is_lecture": true, "end": "00:02:24", "is_worked_example": false, "text": "These include the MMU state, I/O devices, and so on."}, {"start": "00:02:24", "is_lecture": true, "end": "00:02:38", "is_worked_example": false, "text": "User-mode programs that need to access, say, the disk, need to make a request to the OS kernel to perform the operation, giving the OS the chance to vet the request for appropriate permissions, etc."}, {"start": "00:02:38", "is_lecture": true, "end": "00:02:41", "is_worked_example": false, "text": "We'll see how all of this works in an upcoming lecture."}, {"start": "00:02:41", "is_lecture": true, "end": "00:02:51", "is_worked_example": false, "text": "User-mode programs (aka applications) are written as if they have access to the entire virtual address space."}, {"start": "00:02:51", "is_lecture": true, "end": "00:03:00", "is_worked_example": false, "text": "They often obey the same conventions such as the address of the first instruction in the program, the initial value for the stack pointer, etc."}, {"start": "00:03:00", "is_lecture": true, "end": "00:03:13", "is_worked_example": false, "text": "Since all these virtual addresses are interpreted using the current context, by controlling the contexts the OS can ensure that the programs can coexist without conflict."}, {"start": "00:03:13", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "The diagram on the right shows a standard plan for organizing the virtual address space of an application."}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:31", "is_worked_example": false, "text": "Typically the first virtual page is made inaccessible, which helps catch errors involving references to initialized (i.e., zero-valued) pointers."}, {"start": "00:03:31", "is_lecture": true, "end": "00:03:39", "is_worked_example": false, "text": "Then come some number of read-only pages that hold the application's code and perhaps the code from any shared libraries it uses."}, {"start": "00:03:39", "is_lecture": true, "end": "00:03:47", "is_worked_example": false, "text": "Marking code pages as read-only avoids hard-to-find bugs where errant data accesses inadvertently change the program!"}, {"start": "00:03:47", "is_lecture": true, "end": "00:03:53", "is_worked_example": false, "text": "Then there are read-write pages holding the application's statically allocated data structures."}, {"start": "00:03:53", "is_lecture": true, "end": "00:04:00", "is_worked_example": false, "text": "The rest of the virtual address space is divided between two data regions that can grow over time."}, {"start": "00:04:00", "is_lecture": true, "end": "00:04:06", "is_worked_example": false, "text": "The first is the application's stack, used to hold procedure activation records."}, {"start": "00:04:06", "is_lecture": true, "end": "00:04:14", "is_worked_example": false, "text": "Here we show it located at the lower end of the virtual address space since our convention is that the stack grows towards higher addresses."}, {"start": "00:04:14", "is_lecture": true, "end": "00:04:22", "is_worked_example": false, "text": "The other growable region is the heap, used when dynamically allocating storage for long-lived data structures."}, {"start": "00:04:22", "is_lecture": true, "end": "00:04:30", "is_worked_example": false, "text": "\"Dynamically\" means that the allocation and deallocation of objects is done by explicit procedure calls while the application is running."}, {"start": "00:04:30", "is_lecture": true, "end": "00:04:26", "is_worked_example": false, "text": "In other words, we don't know which objects will be created until the program actually executes."}, {"start": "00:04:26", "is_lecture": true, "end": "00:04:41", "is_worked_example": false, "text": "As shown here, as the heap expands it grows towards lower addresses."}, {"start": "00:04:41", "is_lecture": true, "end": "00:04:47", "is_worked_example": false, "text": "The page fault handler knows to allocate new pages when these regions grow."}, {"start": "00:04:47", "is_lecture": true, "end": "00:04:57", "is_worked_example": false, "text": "Of course, if they ever meet somewhere in the middle and more space is needed, the application is out of luck -- it's run out of virtual memory!"}]}, "C11S01B04-LEC.srt": {"url": "https://courses.edx.org/courses/course-v1:MITx+6.004.3x+2T2016/courseware/c11/c11s1/4?activate_block_id=block-v1%3AMITx%2B6.004.3x%2B2T2016%2Btype%40discussion%2Bblock%40c11s1v4", "items": [{"start": "00:00:00", "is_lecture": true, "end": "00:00:12", "is_worked_example": false, "text": "Network technologies were developed to connect components (in this case individual computer systems) separated by larger distances, i.e., distances measured in meters instead of centimeters."}, {"start": "00:00:12", "is_lecture": true, "end": "00:00:17", "is_worked_example": false, "text": "Communicating over these larger distances led to different design tradeoffs."}, {"start": "00:00:17", "is_lecture": true, "end": "00:00:23", "is_worked_example": false, "text": "In early networks, information was sent as a sequence of bits over the shared communication medium."}, {"start": "00:00:23", "is_lecture": true, "end": "00:00:28", "is_worked_example": false, "text": "The bits were organized into packets, each containing the address of the destination."}, {"start": "00:00:28", "is_lecture": true, "end": "00:00:38", "is_worked_example": false, "text": "Packets also included a checksum used to detect errors in transmission and the protocol supported the ability to request the retransmission of corrupted packets."}, {"start": "00:00:38", "is_lecture": true, "end": "00:00:46", "is_worked_example": false, "text": "The software controlling the network is divided into a \"stack\" of modules, each implementing a different communication abstraction."}, {"start": "00:00:46", "is_lecture": true, "end": "00:00:53", "is_worked_example": false, "text": "The lowest-level physical layer is responsible for transmitting and receiving an individual packet of bits."}, {"start": "00:00:53", "is_lecture": true, "end": "00:01:05", "is_worked_example": false, "text": "Bit errors are detected and corrected, and packets with uncorrectable errors are discarded. There are different physical-layer modules available for the different types of physical networks."}, {"start": "00:01:05", "is_lecture": true, "end": "00:01:09", "is_worked_example": false, "text": "The network layer deals with the addressing and routing of packets."}, {"start": "00:01:09", "is_lecture": true, "end": "00:01:20", "is_worked_example": false, "text": "Clever routing algorithms find the shortest communication path through the multi-hop network and deal with momentary or long-term outages on particular network links."}, {"start": "00:01:20", "is_lecture": true, "end": "00:01:29", "is_worked_example": false, "text": "The transport layer is responsible for providing the reliable communication of a stream of data, dealing with the issues of discarded or out-of-order packets."}, {"start": "00:01:29", "is_lecture": true, "end": "00:01:41", "is_worked_example": false, "text": "In an effort to optimize network usage and limit packet loses due to network congestion, the transport layer deals with flow control, i.e., the rate at which packets are sent."}, {"start": "00:01:41", "is_lecture": true, "end": "00:01:50", "is_worked_example": false, "text": "A key idea in the networking community is the notion of building a reliable communication channel on top of a \"best efforts\" packet network."}, {"start": "00:01:50", "is_lecture": true, "end": "00:01:57", "is_worked_example": false, "text": "Higher layers of the protocol are designed so that its possible to recover from errors in the lower layers."}, {"start": "00:01:57", "is_lecture": true, "end": "00:02:04", "is_worked_example": false, "text": "This has proven to be much more cost-effective and robust than trying to achieve 100% reliability at each layer."}, {"start": "00:02:04", "is_lecture": true, "end": "00:02:15", "is_worked_example": false, "text": "As we saw in the previous section, there are a lot of electrical issues when trying to communicate over a shared wire with multiple drivers and receivers."}, {"start": "00:02:15", "is_lecture": true, "end": "00:02:24", "is_worked_example": false, "text": "Slowing down the rate of communication helps to solve the problems, but \"slow\" isn't in the cards for today's high-performance systems."}, {"start": "00:02:24", "is_lecture": true, "end": "00:02:36", "is_worked_example": false, "text": "Experience in the network world has shown that the fastest and least problematic communication channels have a single driver communicating with a single receiver, what's called a point-to-point link."}, {"start": "00:02:36", "is_lecture": true, "end": "00:02:40", "is_worked_example": false, "text": "Using differential signaling is particularly robust."}, {"start": "00:02:40", "is_lecture": true, "end": "00:02:45", "is_worked_example": false, "text": "With differential signaling, the receiver measures the voltage difference across the two signaling wires."}, {"start": "00:02:45", "is_lecture": true, "end": "00:02:57", "is_worked_example": false, "text": "Electrical effects that might induce voltage noise on one signaling wire will affect the other in equal measure, so the voltage difference will be largely unaffected by most noise."}, {"start": "00:02:57", "is_lecture": true, "end": "00:03:01", "is_worked_example": false, "text": "Almost all high-performance communication links use differential signaling."}, {"start": "00:03:01", "is_lecture": true, "end": "00:03:11", "is_worked_example": false, "text": "If we're sending digital data, does that mean we also have to send a separate clock signal so the receiver knows when to sample the signal to determine the next bit?"}, {"start": "00:03:11", "is_lecture": true, "end": "00:03:20", "is_worked_example": false, "text": "With some cleverness, it turns out that we can recover the timing information from the received signal assuming we know the nominal clock period at the transmitter."}, {"start": "00:03:20", "is_lecture": true, "end": "00:03:32", "is_worked_example": false, "text": "If the transmitter changes the bit its sending at the rising edge of the transmitter's clock, then the receiver can use the transitions in the received waveform to infer the timing for some of the clock edges."}, {"start": "00:03:32", "is_lecture": true, "end": "00:03:40", "is_worked_example": false, "text": "Then the receiver can use its knowledge of the transmitter's nominal clock period to infer the location of the remaining clock edges."}, {"start": "00:03:40", "is_lecture": true, "end": "00:03:51", "is_worked_example": false, "text": "It does this by using a phase-locked loop to generate a local facsimile of the transmitter's clock, using any received transitions to correct the phase and period of the local clock."}, {"start": "00:03:51", "is_lecture": true, "end": "00:04:02", "is_worked_example": false, "text": "The transmitter adds a training sequence of bits at the front of packet to ensure that the receiver's phased-lock loop is properly synchronized before the packet data itself is transmitted."}, {"start": "00:04:02", "is_lecture": true, "end": "00:04:11", "is_worked_example": false, "text": "A unique bit sequence is used to separate the training signal from the packet data so the receiver can tell exactly where the packet starts"}, {"start": "00:04:11", "is_lecture": true, "end": "00:04:16", "is_worked_example": false, "text": "even if it missed a few training bits while the clocks were being properly synchronized."}, {"start": "00:04:16", "is_lecture": true, "end": "00:04:25", "is_worked_example": false, "text": "Once the receiver knows the timing of the clock edges, it can then sample the incoming waveform towards the end of each clock period to determine the transmitted bit."}, {"start": "00:04:25", "is_lecture": true, "end": "00:04:33", "is_worked_example": false, "text": "To keep the local clock in sync with the transmitter's clock, the incoming waveform needs to have reasonably frequent transitions."}, {"start": "00:04:33", "is_lecture": true, "end": "00:04:40", "is_worked_example": false, "text": "But if the transmitter is sending say, all zeroes, how can we guarantee frequent-enough clock edges?"}, {"start": "00:04:40", "is_lecture": true, "end": "00:04:53", "is_worked_example": false, "text": "The trick, invented by IBM, is for the transmitter to take the stream of message bits and re-encode them into a bit stream that is guaranteed to have transitions no matter what the message bits are."}, {"start": "00:04:53", "is_lecture": true, "end": "00:05:06", "is_worked_example": false, "text": "The most commonly used encoding is 8b10b, where 8 message bits are encoded into 10 transmitted bits, where the encoding guarantees a transition at least every 6 bit times."}, {"start": "00:05:06", "is_lecture": true, "end": "00:05:13", "is_worked_example": false, "text": "Of course, the receiver has to reverse the 8b10b encoding to recover the actual message bits."}, {"start": "00:05:13", "is_lecture": true, "end": "00:05:14", "is_worked_example": false, "text": "Pretty neat!"}, {"start": "00:05:14", "is_lecture": true, "end": "00:05:20", "is_worked_example": false, "text": "The benefit of this trick is that we truly only need to send a single stream of bits."}, {"start": "00:05:20", "is_lecture": true, "end": "00:05:28", "is_worked_example": false, "text": "The receiver will be able to recover both the timing information and the data without also needing to transmit a separate clock signal."}, {"start": "00:05:28", "is_lecture": true, "end": "00:05:35", "is_worked_example": false, "text": "Using these lessons, networks have evolved from using shared communication channels to using point-to-point links."}, {"start": "00:05:35", "is_lecture": true, "end": "00:05:45", "is_worked_example": false, "text": "Today local-area networks use 10, 100, or 1000 BaseT wiring which includes separate differential pairs for sending and receiving,"}, {"start": "00:05:45", "is_lecture": true, "end": "00:05:52", "is_worked_example": false, "text": "i.e., each sending or receiving channel is unidirectional with a single driver and single receiver."}, {"start": "00:05:52", "is_lecture": true, "end": "00:06:01", "is_worked_example": false, "text": "The network uses separate switches and routers to receive packets from a sender and then forward the packets over a point-to-point link to the next switch,"}, {"start": "00:06:01", "is_lecture": true, "end": "00:06:07", "is_worked_example": false, "text": "and so on, across multiple point-to-point links until the packet arrives at its destination."}, {"start": "00:06:07", "is_lecture": true, "end": "00:06:18", "is_worked_example": false, "text": "System-level connections have evolved to use the same communication strategy: point-to-point links with switches for routing packets to their intended destination."}, {"start": "00:06:18", "is_lecture": true, "end": "00:06:27", "is_worked_example": false, "text": "Note that communication along each link is independent, so a network with many links can actually support a lot of communication bandwidth."}, {"start": "00:06:27", "is_lecture": true, "end": "00:06:34", "is_worked_example": false, "text": "With a small amount of packet buffering in the switches to deal with momentary contention for a particular link,"}, {"start": "00:06:34", "is_lecture": true, "end": "00:06:41", "is_worked_example": false, "text": "this is a very effective strategy for moving massive amounts of information from one component to the next."}, {"start": "00:06:41", "is_lecture": true, "end": "00:06:46", "is_worked_example": false, "text": "In the next section, we'll look at some of the more interesting details."}]}}
