0
00:00:00,000 --> 00:00:04,000
Okay, one more cache design decision to make, then we're done!

1
00:00:04,000 --> 00:00:08,000
How should we handle memory writes in the cache?

2
00:00:08,000 --> 00:00:14,000
Ultimately we'll need update main memory with the new data, but when should that happen?

3
00:00:14,000 --> 00:00:18,000
The most obvious choice is to perform the write immediately.

4
00:00:18,000 --> 00:00:26,000
In other words, whenever the CPU sends a write request to the cache, the cache then performs the same write to main memory.

5
00:00:26,000 --> 00:00:28,000
This is called "write-through".

6
00:00:28,000 --> 00:00:33,000
That way main memory always has the most up-to-date value for all locations.

7
00:00:33,000 --> 00:00:41,000
But this can be slow if the CPU has to wait for a DRAM write access -- writes could become a real bottleneck!

8
00:00:41,000 --> 00:00:51,000
And what if the program is constantly writing a particular memory location, e.g., updating the value of a local variable in the current stack frame?

9
00:00:51,000 --> 00:00:55,000
In the end we only need to write the last value to main memory.

10
00:00:55,000 --> 00:00:59,000
Writing all the earlier values is waste of memory bandwidth.

11
00:00:59,000 --> 00:01:09,000
Suppose we let the CPU continue execution while the cache waits for the write to main memory to complete -- this is called "write-behind".

12
00:01:09,000 --> 00:01:14,000
This will overlap execution of the program with the slow writes to main memory.

13
00:01:14,000 --> 00:01:28,000
Of course, if there's another cache miss while the write is still pending, everything will have to wait at that point until both the write and subsequent refill read finish, since the CPU can't proceed until the cache miss is resolved.

14
00:01:28,000 --> 00:01:37,000
The best strategy is called "write-back" where the contents of the cache are updated and the CPU continues execution immediately.

15
00:01:37,000 --> 00:01:45,000
The updated cache value is only written to main memory when the cache line is chosen as the replacement line for a cache miss.

16
00:01:45,000 --> 00:01:53,000
This strategy minimizes the number of accesses to main memory, preserving the memory bandwidth for other operations.

17
00:01:53,000 --> 00:01:57,000
This is the strategy used by most modern processors.

18
00:01:57,000 --> 00:02:00,000
Write-back is easy to implement.

19
00:02:00,000 --> 00:02:08,000
Returning to our original cache recipe, we simply eliminate the start of the write to main memory when there's a write request to the cache.

20
00:02:08,000 --> 00:02:11,000
We just update the cache contents and leave it at that.

21
00:02:11,000 --> 00:02:25,000
However, replacing a cache line becomes a more complex operation, since we can't reuse the cache line without first writing its contents back to main memory in case they had been modified by an earlier write access.

22
00:02:25,000 --> 00:02:27,000
Hmm.

23
00:02:27,000 --> 00:02:32,000
Seems like this does a write-back of all replaced cache lines whether or not they've been written to.

24
00:02:32,000 --> 00:02:40,000
We can avoid unnecessary write-backs by adding another state bit to each cache line: the "dirty" bit.

25
00:02:40,000 --> 00:02:45,000
The dirty bit is set to 0 when a cache line is filled during a cache miss.

26
00:02:45,000 --> 00:02:57,000
If a subsequent write operation changes the data in a cache line, the dirty bit is set to 1, indicating that value in the cache now differs from the value in main memory.

27
00:02:57,000 --> 00:03:05,000
When a cache line is selected for replacement, we only need to write its data back to main memory if its dirty bit is 1.

28
00:03:05,000 --> 00:03:19,000
So a write-back strategy with a dirty bit gives an elegant solution that minimizes the number of writes to main memory and only delays the CPU on a cache miss if a dirty cache line needs to be written back to memory.

29
00:03:19,000 --> 00:03:33,000
That concludes our discussion of caches, which was motivated by our desire to minimize the average memory access time by building a hierarchical memory system that had both low latency and high capacity.

30
00:03:33,000 --> 00:03:37,000
There were a number of strategies we employed to achieve our goal.

31
00:03:37,000 --> 00:03:44,000
Increasing the number of cache lines decreases AMAT by decreasing the miss ratio.

32
00:03:44,000 --> 00:03:55,000
Increasing the block size of the cache let us take advantage of the fast column accesses in a DRAM to efficiently load a whole block of data on a cache miss.

33
00:03:55,000 --> 00:04:05,000
The expectation was that this would improve AMAT by increasing the number of hits in the future as accesses were made to nearby locations.

34
00:04:05,000 --> 00:04:13,000
Increasing the number of ways in the cache reduced the possibility of cache line conflicts, lowering the miss ratio.

35
00:04:13,000 --> 00:04:20,000
Choosing the least-recently used cache line for replacement minimized the impact of replacement on the hit ratio.

36
00:04:20,000 --> 00:04:26,000
And, finally, we chose to handle writes using a write-back strategy with dirty bits.

37
00:04:26,000 --> 00:04:30,000
How do we make the tradeoffs among all these architectural choices?

38
00:04:30,000 --> 00:04:41,000
As usual, we'll simulate different cache organizations and chose the architectural mix that provides the best performance on our benchmark programs.

