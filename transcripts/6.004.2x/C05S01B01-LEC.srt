0
00:00:00,000 --> 00:00:06,000
Today we're going to talk about how to translate high-level languages into code that computers can execute.

1
00:00:06,000 --> 00:00:14,000
So far we've seen the Beta ISA, which includes instructions that control the datapath operations performed on 32-bit data stored in the registers.

2
00:00:14,000 --> 00:00:19,000
There are also instructions for accessing main memory and changing the program counter.

3
00:00:19,000 --> 00:00:26,000
The instructions are formatted as opcode, source, and destination fields that form 32-bit values in main memory.

4
00:00:26,000 --> 00:00:33,000
To make our lives easier, we developed assembly language as a way of specifying sequences of instructions.

5
00:00:33,000 --> 00:00:38,000
Each assembly language statement corresponds to a single instruction.

6
00:00:38,000 --> 00:00:48,000
As assembly language programmers, we're responsible for managing which values are in registers and which are in main memory, and we need to figure out how to break down complicated operations,

7
00:00:48,000 --> 00:00:54,000
e.g., accessing an element of an array, into the right sequence of Beta operations.

8
00:00:54,000 --> 00:01:01,000
We can go one step further and use high-level languages to describe the computations we want to perform.

9
00:01:01,000 --> 00:01:11,000
These languages use variables and other data structures to abstract away the details of storage allocation and the movement of data to and from main memory.

10
00:01:11,000 --> 00:01:16,000
We can just refer to a data object by name and let the language processor handle the details.

11
00:01:16,000 --> 00:01:26,000
Similarly, we'll write expressions and other operators such as assignment (=) to efficiently describe what would require many statements in assembly language.

12
00:01:26,000 --> 00:01:33,000
Today we're going to dive into how to translate high-level language programs into code that will run on the Beta.

13
00:01:33,000 --> 00:01:42,000
Here we see Euclid's algorithm for determining the greatest common divisor of two numbers, in this case the algorithm is written in the C programming language.

14
00:01:42,000 --> 00:01:47,000
We'll be using a simple subset of C as our example high-level language.

15
00:01:47,000 --> 00:01:54,000
Please see the brief overview of C in the Handouts section if you'd like an introduction to C syntax and semantics.

16
00:01:54,000 --> 00:02:03,000
C was developed by Dennis Ritchie at AT&T Bell Labs in the late 60's and early 70's to use when implementing the Unix operating system.

17
00:02:03,000 --> 00:02:14,000
Since that time many new high-level languages have been introduced providing modern abstractions like object-oriented programming along with useful new data and control structures.

18
00:02:14,000 --> 00:02:25,000
Using C allows us describe a computation without referring to any of the details of the Beta ISA like registers, specific Beta instructions, and so on.

19
00:02:25,000 --> 00:02:36,000
The absence of such details means there is less work required to create the program and makes it easier for others to read and understand the algorithm implemented by the program.

20
00:02:36,000 --> 00:02:40,000
There are many advantages to using a high-level language.

21
00:02:40,000 --> 00:02:45,000
They enable programmers to be very productive since the programs are concise and readable.

22
00:02:45,000 --> 00:02:49,000
These attributes also make it easy to maintain the code.

23
00:02:49,000 --> 00:02:59,000
Often it is harder to make certain types of mistakes since the language allows us to check for silly errors like storing a string value into a numeric variable.

24
00:02:59,000 --> 00:03:07,000
And more complicated tasks like dynamically allocating and deallocating storage can be completely automated.

25
00:03:07,000 --> 00:03:15,000
The result is that it can take much less time to create a correct program in a high-level language than it would it when writing in assembly language.

26
00:03:15,000 --> 00:03:30,000
Since the high-level language has abstracted away the details of a particular ISA, the programs are portable in the sense that we can expect to run the same code on different ISAs without having to rewrite the code.

27
00:03:30,000 --> 00:03:34,000
What do we lose by using a high-level language?

28
00:03:34,000 --> 00:03:42,000
Should we worry that we'll pay a price in terms of the efficiency and performance we might get by crafting each instruction by hand?

29
00:03:42,000 --> 00:03:46,000
The answer depends on how we choose to run high-level language programs.

30
00:03:46,000 --> 00:03:52,000
The two basic execution strategies are "interpretation" and "compilation".

31
00:03:52,000 --> 00:04:00,000
To interpret a high-level language program, we'll write a special program called an "interpreter" that runs on the actual computer, M1.

32
00:04:00,000 --> 00:04:13,000
The interpreter mimics the behavior of some abstract easy-to-program machine M2 and for each M2 operation executes sequences of M1 instructions to achieve the desired result.

33
00:04:13,000 --> 00:04:27,000
We can think of the interpreter along with M1 as an implementation of M2, i.e., given a program written for M2, the interpreter will, step-by-step, emulate the effect of M2 instructions.

34
00:04:27,000 --> 00:04:33,000
We often use several layers of interpretation when tackling computation tasks.

35
00:04:33,000 --> 00:04:40,000
For example, an engineer may use her laptop with an Intel CPU to run the Python interpreter.

36
00:04:40,000 --> 00:04:49,000
In Python, she loads the SciPy toolkit, which provides a calculator-like interface for numerical analysis for matrices and data.

37
00:04:49,000 --> 00:05:02,000
For each SciPy command, e.g., "find the maximum value of a dataset", the SciPy tool kit executes many Python statements, e.g., to loop over each element of the array, remembering the largest value.

38
00:05:02,000 --> 00:05:12,000
For each Python statement, the Python interpreter executes many x86 instructions, e.g., to increment the loop index and check for loop termination.

39
00:05:12,000 --> 00:05:23,000
Executing a single SciPy command may require executing of tens of Python statements, which in turn each may require executing hundreds of x86 instructions.

40
00:05:23,000 --> 00:05:29,000
The engineer is very happy she didn't have to write each of those instructions herself!

41
00:05:29,000 --> 00:05:44,000
Interpretation is an effective implementation strategy when performing a computation once, or when exploring which computational approach is most effective before making a more substantial investment in creating a more efficient implementation.

42
00:05:44,000 --> 00:05:57,000
We'll use a compilation implementation strategy when we have computational tasks that we need to execute repeatedly and hence we are willing to invest more time up-front for more efficiency in the long-term.

43
00:05:57,000 --> 00:06:01,000
In compilation, we also start with our actual computer M1.

44
00:06:01,000 --> 00:06:09,000
Then we'll take our high-level language program P2 and translate it statement-by-statement into a program for M1.

45
00:06:09,000 --> 00:06:12,000
Note that we're not actually running the P2 program.

46
00:06:12,000 --> 00:06:19,000
Instead we're using it as a template to create an equivalent P1 program that can execute directly on M1.

47
00:06:19,000 --> 00:06:27,000
The translation process is called "compilation" and the program that does the translation is called a "compiler".

48
00:06:27,000 --> 00:06:36,000
We compile the P2 program once to get the translation P1, and then we'll run P1 on M1 whenever we want to execute P2.

49
00:06:36,000 --> 00:06:45,000
Running P1 avoids the overhead of having to process the P2 source and the costs of executing any intervening layers of interpretation.

50
00:06:45,000 --> 00:06:52,000
Instead of dynamically figuring out the necessary machine instructions for each P2 statement as it's encountered,

51
00:06:52,000 --> 00:06:59,000
in effect we've arranged to capture that stream of machine instructions and save them as a P1 program for later execution.

52
00:06:59,000 --> 00:07:06,000
If we're willing to pay the up-front costs of compilation, we'll get more efficient execution.

53
00:07:06,000 --> 00:07:15,000
And, with different compilers, we can arrange to run P2 on many different machines -- M2, M3, etc. -- without having rewrite P2.

54
00:07:15,000 --> 00:07:24,000
So we now have two ways to execute a high-level language program: interpretation and compilation.

55
00:07:24,000 --> 00:07:27,000
Both allow us to change the original source program.

56
00:07:27,000 --> 00:07:32,000
Both allow us to abstract away the details of the actual computer we'll use to run the program.

57
00:07:32,000 --> 00:07:36,000
And both strategies are widely used in modern computer systems!

58
00:07:36,000 --> 00:07:41,000
Let's summarize the differences between interpretation and compilation.

59
00:07:41,000 --> 00:07:46,000
Suppose the statement "x+2" appears in the high-level program.

60
00:07:46,000 --> 00:07:53,000
When the interpreter processes this statement it immediately fetches the value of the variable x and adds 2 to it.

61
00:07:53,000 --> 00:08:02,000
On the other hand, the compiler would generate Beta instructions that would LD the variable x into a register and then ADD 2 to that value.

62
00:08:02,000 --> 00:08:14,000
The interpreter is executing each statement as it's processed and, in fact, may process and execute the same statement many times if, e.g., it was in a loop.

63
00:08:14,000 --> 00:08:19,000
The compiler is just generating instructions to be executed at some later time.

64
00:08:19,000 --> 00:08:30,000
Interpreters have the overhead of processing the high-level source code during execution and that overhead may be incurred many times in loops.

65
00:08:30,000 --> 00:08:36,000
Compilers incur the processing overhead once, making the eventual execution more efficient.

66
00:08:36,000 --> 00:08:47,000
But during development, the programmer may have to compile and run the program many times, often incurring the cost of compilation for only a single execution of the program.

67
00:08:47,000 --> 00:08:51,000
So the compile-run-debug loop can take more time.

68
00:08:51,000 --> 00:09:02,000
The interpreter is making decisions about the data type of x and the type of operations necessary at run time, i.e., while the program is running.

69
00:09:02,000 --> 00:09:06,000
The compiler is making those decisions during the compilation process.

70
00:09:06,000 --> 00:09:08,000
Which is the better approach?

71
00:09:08,000 --> 00:09:14,000
In general, executing compiled code is much faster than running the code interpretively.

72
00:09:14,000 --> 00:09:29,000
But since the interpreter is making decisions at run time, it can change its behavior depending, say, on the type of the data in the variable X, offering considerable flexibility in handling different types of data with the same algorithm.

73
00:09:29,000 --> 00:09:35,000
Compilers take away that flexibility in exchange for fast execution.

